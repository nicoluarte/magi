* Data science program
#+STARTUP: latexpreview
** 09/07/2019, Cross validations, decision trees
*** Cross-validation
- Allows us to compare different machine learning methods and get a
sense of how well they will work in practice
- Typically this method split the data base into 75%~ for training and
25%~ for testing
- A similar principle is followed by 'leave one out cross validation',
where the model is trained in all but one sample, and that sample is
used to test model accuracy
- k-fold validation is the generalization, where data is divided in k
blocks
#+BEGIN_SRC R :results output 
              # Load pkg
  pkg <- c("tidyverse", "caret")
  lapply(pkg, library, character.only = TRUE)

              # load sample data
  data("swiss")

              # data-split
  set.seed(123)
  training.samples <- swiss$Fertility %>%
  createDataPartition(p = 0.8, list = FALSE) # 80/20 split
  train.data <- swiss[training.samples, ]
  test.data <- swiss[-training.samples, ]

              # build the model
  model <- lm(Fertility ~ ., data = train.data)

              # make predictions and compute acc
  predictions <- model %>% predict(test.data)
  data.frame(R2 = R2(predictions, test.data$Fertility),
    RMSE = RMSE(predictions, test.data$Fertility),
    MAE = MAE(predictions, test.data$Fertility))

              # repeated k-fold cross validation

        number = 5, repeats = 10)
  model.2 <- train(Fertility ~ .,
      data = swiss,
      method = "lm",
      trControl = t.control)
  predictions_2 <- model %>% predict(swiss)
  data.frame(R2 = R2(predictions_2, swiss$Fertility),
    RMSE = RMSE(predictions_2, swiss$Fertility),
    MAE = MAE(predictions_2, swiss$Fertility))
#+END_SRC
*** Confusion matrix
- The confusion matrix can be used to compute various summaries for
classification models
- Rows correspond to what it was predicted
- Columns correspond to the known truth
- Sensitivity:
\begin{equation}
\frac{T.positives}{T.positives + F.negatives}
\end{equation}

- Specificity:
\begin{equation}
\frac{T.negatives}{T.negatives + F.positives}
\end{equation}

#+BEGIN_SRC R :results output
				    # Load pkg
pkg <- c("tidyverse", "caret")
lapply(pkg, library, character.only = TRUE)

				    # Using dummy data
x <- factor(ceiling(runif(1000)-0.20)) # predictions
y <- factor(ceiling(runif(1000)-0.25)) # references
table(x, y)

                                        # confusion matrix
confusionMatrix(x, y, positive = "1")
#+END_SRC
*** Decision tree
R code to generate a decision tree
- First a tree is generated
- Later on is pruned
- In this example the difference is not very notorious
#+BEGIN_SRC R :results output :session rs
  library(rpart)
  library(tree)
  setwd("/home/nicoluarte/Downloads")
  df = read.table('cleveland.txt', h=T)
  head(df)
  summary(df)

                                          # Using rpart

  ## no data standarization needed for trees

  set.seed(123)

  ## creating the first tree

  tree.1 = rpart(df[,14] ~ .,
                 data=df[,1:13],
                 cp=0.0001,parms=list(split="information"),
                 method="class")

  ##split information means entropy/ split could be gini - cp is the complexity parameter.

  par(xpd=TRUE)
  plot(tree.1, uniform=T);
  text(tree.1, all=T, pretty=T, fancy=T, use.n=T, fwidth=0.3, fheight=0.3)

  ##pretty is for compatibility with tree package. Considers the minimum length for abbreviation of character or factor variables (4 L).

  ## Pruning

  plotcp(tree.1) ## Cross-validation results

  printcp(tree.1)

  tree.1$cptable[which.min(tree.1$cptable[,"xerror"]),"CP"]

  tree.2<-prune(tree.1,cp=0.01102941)

  par(xpd=TRUE)
  plot(tree.2, uniform=T)
  text(tree.2, all=T, pretty=T, fancy=T, use.n=T, fwidth=0.3, fheight=0.3)

  plotcp(tree.2) ## Cross-validation results

  printcp(tree.2)

#+END_SRC

#+RESULTS:
#+begin_example
  age gender     cp trestbps chol  fbs restecg thatach exang oldpeak slope ca
1  63   male angina      145  233 true     hyp     150   fal     2.3  down  0
2  67   male asympt      160  286  fal     hyp     108  true     1.5  flat  3
3  67   male asympt      120  229  fal     hyp     129  true     2.6  flat  2
4  37   male notang      130  250  fal    norm     187   fal     3.5  down  0
5  41    fem abnang      130  204  fal     hyp     172   fal     1.4    up  0
6  56   male abnang      120  236  fal    norm     178   fal     0.8    up  0
  thal diag Col15
1  fix buff     H
2 norm sick    S2
3  rev sick    S1
4 norm buff     H
5 norm buff     H
6 norm buff     H
      age         gender         cp         trestbps          chol      
 Min.   :29.00   fem : 95   abnang: 49   Min.   : 94.0   Min.   :126.0  
 1st Qu.:48.00   male:201   angina: 23   1st Qu.:120.0   1st Qu.:211.0  
 Median :56.00              asympt:141   Median :130.0   Median :242.5  
 Mean   :54.52              notang: 83   Mean   :131.6   Mean   :247.2  
 3rd Qu.:61.00                           3rd Qu.:140.0   3rd Qu.:275.2  
 Max.   :77.00                           Max.   :200.0   Max.   :564.0  
   fbs      restecg       thatach       exang        oldpeak       slope    
 fal :253   abn :  4   Min.   : 71.0   fal :199   Min.   :0.000   down: 21  
 true: 43   hyp :145   1st Qu.:133.0   true: 97   1st Qu.:0.000   flat:137  
            norm:147   Median :152.5              Median :0.800   up  :138  
                       Mean   :149.6              Mean   :1.059             
                       3rd Qu.:166.0              3rd Qu.:1.650             
                       Max.   :202.0              Max.   :6.200             
       ca           thal       diag     Col15   
 Min.   :0.0000   fix : 18   buff:160   H :160  
 1st Qu.:0.0000   norm:163   sick:136   S1: 53  
 Median :0.0000   rev :115              S2: 35  
 Mean   :0.6791                         S3: 35  
 3rd Qu.:1.0000                         S4: 13  
 Max.   :3.0000

Classification tree:
rpart(formula = df[, 14] ~ ., data = df[, 1:13], method = "class", 
    parms = list(split = "information"), cp = 1e-04)

Variables actually used in tree construction:
[1] age     ca      cp      exang   oldpeak thal    thatach

Root node error: 136/296 = 0.45946

n= 296 

         CP nsplit rel error  xerror     xstd
1 0.4926471      0   1.00000 1.00000 0.063044
2 0.0514706      1   0.50735 0.63971 0.057630
3 0.0404412      3   0.40441 0.52941 0.054276
4 0.0220588      5   0.32353 0.44853 0.051170
5 0.0110294      6   0.30147 0.44118 0.050857
6 0.0036765      8   0.27941 0.44118 0.050857
7 0.0001000     10   0.27206 0.46324 0.051780
[1] 0.01102941

Classification tree:
rpart(formula = df[, 14] ~ ., data = df[, 1:13], method = "class", 
    parms = list(split = "information"), cp = 1e-04)

Variables actually used in tree construction:
[1] age   ca    cp    exang thal 

Root node error: 136/296 = 0.45946

n= 296 

        CP nsplit rel error  xerror     xstd
1 0.492647      0   1.00000 1.00000 0.063044
2 0.051471      1   0.50735 0.63971 0.057630
3 0.040441      3   0.40441 0.52941 0.054276
4 0.022059      5   0.32353 0.44853 0.051170
5 0.011029      6   0.30147 0.44118 0.050857
6 0.011029      8   0.27941 0.44118 0.050857
#+end_example
** 11/07/2019, GLM
*** Linear regression
**** Conditions
***** Multi-Collinearity
Multi-collinearity is a phenomenon in which one predictor variable in a multiple
regression model can be linearly predicted from the other with a substantial
degree of accuracy. In this situation the coefficient estimates of the multiple
regression may change erratically in response to small changes in the model or
the data.

Collinearity: is a linear association between two explanatory variables. Two
variables are perfectly collinear if there is an exact linear relationship
between them. For example, X_1 and X_2 are perfectly collinear if there exists
parameters lambda_0 and lambda_1 such that, for all observations i, we have:

\begin{equation}
X_{2i} = \lambda_0 + \lambda_1 X_{1i}
\end{equation}

Multi-collinearity: refers to the situation in which two or more explanatory
variables in a multiple regression model are highly linearly related. We have
perfect multi-collinearity if, for example as in the equation above, the
correlation between two independent variables is equal to 1 or -1. In practice,
we rarely face perfect multi-collinearity in a data set. More commonly, the
issue of multi-collinearity arises when there is an approximate linear
relationship among two or more independent variables.

\begin{equation}
\lambda _{0}+\lambda _{1}X_{{1i}}+\lambda _{2}X_{{2i}}+\cdots +\lambda _{k}X_{{ki}}=0
\end{equation}

#+BEGIN_SRC R :results output :session linear-regression
                                          # get sample dataset

  data(swiss)

                                          # visualize correlation matrix
  library(corrplot)
  cor_mat <- cor(swiss[,c(2:6)], method = c("pearson"))
  corrplot(cor_mat, type = "upper", method = "square")
  print(as.data.frame(cor_mat))

                                          # calculate VIF
  ## variance inflation factor: the VIF's of the linear regression indicate
  ## the degree that the variances in the regression estimates are increased
  ## due to multicollinearity.
  ## VIF values higher that 10 indicate that there's a problem

  ## first we need to fit a linear regression to our data
  mdl.lnr.1 <- lm(Fertility ~ ., data = swiss)
  summary(mdl.lnr.1)

  ## calculate VIF
  library(car)
  vif(mdl.lnr.1)

  ## there's no variable that exceeds a VIF of 10.
  ## however we are going to center the data just to apply some functions

  ## centering explanatory variables
  df.cntrd <- data.frame(Fertility = swiss[, 1],
                         sapply(swiss[,2:6], function(x) x - mean(x)))

  ## scaling using r function
  df.scld <- data.frame(Fertility = swiss[, 1],
                        scale(swiss[,2:6]))

  ## re-make linear model and calculate VIF
  mdl.lnr.2 <- lm(Fertility ~ ., data = df.cntrd)
  summary(mdl.lnr.2)
  vif(mdl.lnr.2)

  ## linear model and VIF for scaled data
  mdl.lnr.3 <- lm(Fertility ~ ., data = df.scld)
  summary(mdl.lnr.3)
  vif(mdl.lnr.3)

#+END_SRC
***** Variability of independent variable should be positive
Fairly straight-forward, it must exist at least some variability in the data set
independent variables

#+BEGIN_SRC R :results output :session linear-regression
  ## LMAO
  print(sapply(swiss, function(x) var(x)))
#+END_SRC
***** Independent variables and residuals are uncorrelated
Not very important
***** Independent variables are expected to be normally distributed
#+BEGIN_SRC R :results output :session linear-regression
  ## we're going to plot one histogram per independent variable
  library(tidyr)
  library(ggplot2)
  ## we do this in order to get data in long form
  swiss %>% gather() %>% head()

  ## plot!
  ggplot(gather(swiss), aes(value)) +
    geom_histogram(bins = 10, aes(y=..density..), alpha=0.5,
                   position = "identity") +
    facet_wrap(~key, scales = 'free_x') +
    geom_density(alpha=0.2)

  ## using shapiro-wilks to test normality
  ## H_0 = data is normally distributed
  print(sapply(swiss, function(x) shapiro.test(x)))

  ## check for skewness for data correction
  library(moments)
  print(skewness(swiss))

  ## transform data based on skewness
  ## Education is positively skewed
  df_trns <- data.frame(swiss)
  df_trns[,"Education"] <- log(df_trns[,"Education"])
  ## Catholic is positively skewed
  df_trns[,"Catholic"] <- log(df_trns[,"Catholic"])
  ## Re-check for normality
  print(sapply(df_trns, function(x) shapiro.test(x)))

  ## plot to see the difference
  ggplot(gather(df_trns), aes(value)) +
    geom_histogram(bins = 10, aes(y=..density..), alpha=0.5,
                   position = "identity") +
    facet_wrap(~key, scales = 'free_x') +
    geom_density(alpha=0.2)

  ## we could also check distribution of dependent variable
  hist(swiss[,"Fertility"])


#+END_SRC

**** Multiple linear regression
*** GLM
- A function is applied to the X's, or a transformation. These are
called link functions
- Started modelling probability of event (Bernoulli, logit of 'p')
- Logit predicts 'odds' <- logistic regression
- In GLM response variable can have any distribution
Using coimbra breast cancer dataset
*CHALLENGE REMOVE OUTLIERS AND RE-RUN ANALYSIS*
- There's not much difference in removing outliers
#+BEGIN_SRC R :results output
setwd("/home/nicoluarte/Downloads/")
df <- read.csv("dataR2.csv")
head(df)
resp <- c(df$Classification - 1)

                                        # compare cancer/no-cancer by age
library(doBy)
summaryBy(Age~resp, data = df, FUN = c(mean, median))

                                        # compare by glucose
summaryBy(Glucose~resp, data = df, FUN = c(mean, median))
summaryBy(Insulin~resp, data = df, FUN = c(mean, median))

                                        # t-test to check means
t.test(Age~resp, data = df)
t.test(Glucose~resp, data = df)
t.test(BMI~resp, data = df)
                                        # re-code variable
df$Glucose2 <- ifelse(df$Glucose<=100, "<=100", ">100")
chisq.test(table(df$Glucose2, resp))

                                        # OR calculation
or <- (44*27)/(8*37)

                                        # Plots
plot(resp~Glucose, data = df)
abline(lm(resp~Glucose, data = df), col = "red")

                                        # Logistic regression
mdl.0 = glm(resp~Glucose2, data = df, family = binomial(link = "logit"))
mdl.1 = glm(resp~Glucose, data = df, family = binomial(link = "logit"))

                                        # Calculating odds
exp(1.3897) # 4 times greater the chance to get cancer

                                        # Box plot
boxplot(Glucose~resp, data = df)

                                        # Logistic regression insulin2
df$Insulin2 <- ifelse(df$Insulin<=8, "<=8", ">8")
mdl.2 = glm(resp~Insulin2, data = df, family = binomial(link = "logit"))
summary(mdl.2)

mdl.3 <- glm(resp~Glucose, data = df, family = binomial(link = "logit"))
summary(mdl.3)

                                        # Calculating odds for different values
exp(0.07867*5)
plot(0.07867*1:10)

                                        # multi-variable model
mdl.4 <- glm(resp~Glucose + Insulin, data = df, family = binomial(link = "logit"))
summary(mdl.4)
plot(Glucose~Insulin, data = df)

                                        # Adding control variables
mdl.5 <- glm(resp~Age + BMI + Glucose, data = df, family = binomial(link = "logit"))
summary(mdl.5)
exp(mdl.5$coefficients["Glucose"]) # each glucose point has a 10%~ effect on chance of cancer

                                        # ROC curve
library(pROC)
prob <- predict(mdl.5, type = c("response"))
roc(resp~prob, data = df, plot = T)

                                        # Use all variables
full.mdl <- glm(resp~., data = df[,1:9], family = binomial(link = "logit"))
summary(full.mdl)

                                        # Prune!

new_vars = setdiff(names(df[,1:9]),c(names(full.mdl$coefficients)[which.max(full.mdl$coefficients)]))
prune.mdl <- glm(resp~., data = df[new_vars], family = binomial(link = "logit"))
summary(prune.mdl)

                                        # Calculate confusion matrix
library(caret)
preds <- as.numeric(predict(full.mdl, type = c("response")) > 0.5)
confusionMatrix(as.factor(preds), as.factor(resp))

                                        # Generate same model without outliers
df_removed_outliers = df[NA,] # same df but NA rows
df_removed_outliers["Classification"] <- df["Classification"]
df_removed_outliers$Classification <- df_removed_outliers$Classification - 1 # to get same levels
for (var in names(df[,1:9]))
{
  outliers <- boxplot.stats(df[,var]) # get the outlier of this variable
  df_target <- df[,var] # get the vector of values of this variable
  idx <- which(df_target %in% outliers$out) # get the idx of outliers in vector
  df_target[idx] <- NA # put NAN's in there
  df_removed_outliers[var] <- df_target # replace with filtered values
}

clean_df = na.omit(df_removed_outliers[,1:10])
full.mdl.filtered <- glm(resp~., data = clean_df[,1:9], # cleaning na rows
                         family = binomial(link = "logit"))
summary(full.mdl.filtered)

preds_filtered <- as.numeric(predict(full.mdl.filtered, type = c("response")) > 0.5)
confusionMatrix(as.factor(preds_filtered), as.factor(clean_df$Classification))
#+END_SRC

** 23/07/2019, Bagging, Random forest, Naive Bayes
#+begin_src R :results output
  library(randomForest)
  library(gbm)
  setwd("/home/nicoluarte/Downloads")
  df <- read.table("cleveland.txt", h=T)


  set.seed(123)

  rf.1 <- randomForest(df[,14] ~ ., data = df[,1:13], mtry = 6,
                       importance = T, ntree = 100)
  print(rf.1)

  layout(matrix(c(1,2), nrow=1), width = c(4,1))
  par(mar=c(5,4,4,0))
  plot(rf.1, log="y")
  par(mar=c(5,0,4,2))
  plot(c(0,1), type?"n", axes = F, xlab = "", ylab = "")
  legend("top", colnames(rf.1$err.rate), col=c(1,2,3), cex = 0.8, fill = c(1,2,3))

  rf.pred <- predict(rf.1, df[,1:13], type = "class")

  table(rf.pred, df[,14])

  importance(rf.1, type = 2)

  varImpPlot(rf.1, type = 1)

                                          # create train and test split
  library(caret)
  trainIdx <- createDataPartition(df[,14],
                                  p = 0.8,
                                  list = FALSE,
                                  times = 1)
  df_train <- df[trainIdx, ]
  df_test <- df[-trainIdx, ]

                                          # Train new random forest
  rf.2 <- randomForest(df_train[,14] ~ ., data = df_train[,1:13], mtry = 6,
                       importance = T, ntree = 100)
  print(rf.2)
#+end_src

#+begin_src python :results output
  import pandas as pd
  import numpy as np
  import matplotlib.pyplot as plt
  import seaborn as sns
  from sklearn import tree
  from sklearn.model_selection import train_test_split, cross_val_score
  from sklearn.externals.six import StringIO
  from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier, export_graphviz
  from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, BaggingRegressor
  from sklearn.metrics import mean_squared_error, confusion_matrix, classification_report

  df = pd.read_csv("/home/nicoluarte/Downloads/heart.txt").drop('Unnamed: 0', axis=1).dropna()

  # Generate labels for categorical variables
  df.ChestPain = pd.factorize(df.ChestPain)[0]
  df.Thal = pd.factorize(df.Thal)[0]

  # Define I/O
  x = df.drop('AHD', axis=1)
  y = pd.factorize(df.AHD)[0]

  # Bagging: using all features
  clf = RandomForestClassifier(max_features=13, max_leaf_nodes=6, n_estimators=100)
  clf.fit(x, y)
  clf.score(x, y)

  # Train and test
  x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.5, random_state=0)

  # Bagging: using all features
  clf1 = RandomForestClassifier(max_features=13, max_leaf_nodes=6, n_estimators=100)
  clf1.fit(x_train, y_train)
  print(clf1.score(x_test, y_test))

  # Predicting
  pred = clf1.predict(x_test)

  # Confusion matrix
  cm = pd.DataFrame(confusion_matrix(y_test, pred).T, index=['No', 'Yes'], columns = ['No', 'Yes'])
  cm.index.name = 'Predicted'
  cm.columns.name = 'True'
  print(cm)

  # Importance
  Importance = pd.DataFrame({'Importance':clf1.feature_importances_*100}, index=x.columns)
#+end_src
** 30/07/2019, Naive Bayes, Support-vector machine
*** Naive Bayes
 #+BEGIN_SRC R :results output :session naive_bayes
   setwd("/home/nicoluarte/Downloads/")

                                           # libraries
   library(e1071)
   library(naivebayes)
   library(dummies)
   library(dplyr)

                                           # read data
   df <- data.frame(read.table('heart.txt', h=T, sep=","))
   head(df)
   summary(df)

                                           # delete NA
   df.old <- na.omit(df)
   df.Sex <- dummy(df.old$Sex)
   df.ChestPain <- dummy(df.old$ChestPain)
   df.Thal <- dummy(df.old$Thal)
   df.1 <- data.frame(df.ChestPain, df.Thal, df.Sex,
                      select(df.old, -c(Sex, ChestPain, Thal, X)))

                                           # naive bayes
   sed.seed(123)
   nb.1 <- naiveBayes(as.factor(df.1[,20]) ~ ., data = df.1[,1:19])
   print(nb.1)

                                           # predict
   pred.1 <- predict(nb.1, newdata = df.1[,1:19], type = "class")

                                           # confusion matrix
   tab.1 <- table(df.1[,20], pred.1)

                                           # accuracy
   (tab.1[1,1]+tab.1[2,2]) / sum(tab.1)

                                           # other naive bayes model
   nb.2 <- naive_bayes(as.factor(df.1[,20]) ~ .,
                       data = df.1[,1:19],
                       usekernel = T)
   pred.2 <- predict(nb.2, newdata = df.1[,1:19], type = "class")

                                           # confusion matrix
   tab.2 <- table(df.1[,20], pred.2)

                                           # accuracy
   (tab.2[1,1]+tab.2[2,2]) / sum(tab.2)
 #+END_SRC
*** Support-vector machine
- Find vectors that best divide the data-set in hyper-planes
- Linear classifier
** 01/08/2019, Time series
#+BEGIN_SRC R :results output :session time_series
  data(co2)

                                          # arrange vector as time series
  df.ts <- ts(c(co2), start=c(1959,1), frequency = 12)
  plot(df.ts)

                                          # decompose
  df.decompose <- decompose(df.ts)
  plot(df.decompose)

                                          # auto-correlation
  df.random <- acf(df.decompose$random, na.action = na.pass)

                                          # Load air database
  library(xlsx)
  df.air <- data.frame(read.xlsx("/home/nicoluarte/Downloads/aircanada.xlsx", sheetIndex = 1))

                                          # arrange air as timeseries
  df.air.ts <- ts(df.air[7], start = c(2005, 7), frequency = 12)

                                          # decompose
  df.air.ts.decompose <- decompose(df.air.ts)
  plot(df.air.ts.decompose)

                                          # auto-correlation
  df.air.random <- acf(df.air.ts.decompose$random, na.action = na.pass)

                                          # convert to non-stationary to stationary
  ## co2 example
  ## diff technique
  acf(co2, lag = 120)
  plot(diff(co2))
  acf(diff(co2), lag = 120)
  plot(diff(diff(co2), lag = 120))
  z <- diff(diff(co2), lag = 120)
  par(mfrow = c(1,2))
  acf(z[1:200])
  acf(z[200:400], na.action = na.omit)

  ## with regression
  y <- c(co2)
  x1 <- time(y)
  x2 <- as.factor(rep(1:12, 39))
  mdl.1 <- lm(y ~ x1 + x2)
  plot(y~x1, type = "l")
  lines(mdl.1$fitted.values~c(x1), col = "red")

  ## non-parametric modelling
  library(forecast)
  hw.mdl <- HoltWinters(co2)
  plot(forecast(hw.mdl, 24))

                                          # using air df
  ## complete this

                                          # ARMA
  ## AR(1): x[t] = Phi * x[t-1] + z[t], z[t] white noise
  x <- c()
  n <- 300
  z <- rnorm(n, 0, 1)
  phi <- 0.8
  x[1] <- z[1]
  for (t in 2:n)
  {
    x[t] <- phi*x[t-1]+z[t]
  }

  ## MA(1): x[t] = z[t] + Theta * z[t-1]
  ## ARMA (1, 1): x[t] - Phi * x[t-1] = theta * z[t-1] + z[t]

#+END_SRC

** 06/08/2019, Naive bayes, neural nets
#+BEGIN_SRC R :results output :session neural_net
  ##Libraries

  library(e1071)

  library(ggplot2)

  library(dummies)

  library(dplyr)

  ##Path

  setwd("/home/nicoluarte/Downloads")

  ##Simulated data

  set.seed(123)

  ##Input
  X <- matrix(rnorm(40), 20, 2)
  colnames(X) <- c("X1","X2")

  ##Output
  y <- c(rep(-1,10), rep(1,10))
  X[y == 1, ] <- X[y == 1, ] + 1

  datos <- data.frame(X,y)

  ##Plot
  ggplot(data = datos, aes(x = X1, y = X2, color = as.factor(y)))+geom_point(size = 6) +theme_bw()

  ##Support vector machine

  datos$y <- as.factor(datos$y)##Convertir variable en factor

  svm.1 <- svm(formula = y ~ X1 + X2, data = datos, kernel = "linear",cost = 10, scale = T)##kernel=SVM lineal o no lineal/scale=T implica estandarizar las variables (media 0 y varianza 1)/cost=parametro de regularizacion

  summary(svm.1)##resumen del SVM

  svm.1$index##observaciones que son vectores de soporte

  ##Classification plot

  plot(svm.1,datos)

  ##Cross-validation for C

  svm_cv <-tune("svm", y ~ X1 + X2, data = datos,kernel = 'linear',ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 20, 50, 100,150, 200)))

  summary(svm_cv)

  svm_cv$best.model

  ## Heart Data

  ##Data

  datos.full<-read.table('heart.txt',h=T, sep=",")

  head(datos.full)

  summary(datos.full)

  ##Delete NA

  datos.old<-datos.full

  datos.Sex<-dummy(datos.full$Sex)

  datos.ChestPain<-dummy(datos.old$ChestPain)

  datos.Thal<-dummy(datos.old$Thal)

  ##New frame

  datos<-data.frame(datos.ChestPain,datos.Thal,datos.Sex,select(datos.old,-c(Sex,ChestPain,Thal, X)))

  head(datos)

  summary(datos)

  ##Scaled variables

  dim(datos)

  datos.std<-datos

  datos.std[,1:19]<-scale(datos[,1:19])

  ##Support Vector Machine

  set.seed(123)

  svm.2<-svm(AHD ~ . ,data=datos.std,kernel="linear",scale=T,cost=0.01)

  print(svm.2)

  ##Prediction

  pred<-predict(svm.2,newdata = datos.std)

  caret::confusionMatrix(pred, datos.std$AHD[1:299])

  ##Classification plot

  plot(svm.2,datos,RestBP ~ MaxHR,slice=list(ChestPainasymptomatic=3,ChestPainnonanginal=4,ChestPainnontypical=5,ChestPaintypical=6,Thalfixed=7,Thalnormal=8,Thalreversable=9,Sex0=10,Sex1=11,Age=12,Chol=13,Fbs=14,RestECG=15,ExAng=16,Oldpeak=17,Slope=18,Ca=19))

  ##Cross-validation for cost

  svm_cv <-tune("svm", AHD ~ . , data = datos.std,kernel = 'linear',ranges = list(cost = c(0.001, 0.01, 0.1, 1, 10)),scale=T)

  summary(svm_cv)


  best<-svm_cv$best.model

  ##Prediction

  pred<-predict(best,newdata = datos.std[1:299,])

  tabla<-table(prediccion = pred, valor_real = datos.std$AHD)## Confusion matrix

  caret::confusionMatrix(pred, datos.std$AHD[1:296])

  ###Using train and test

  set.seed(123)

  p.train<-0.8

  n<-nrow(datos.std)
  trainIndex<-sample(1:n,size=round(p.train*n),replace=F)

  train.s<-data.frame(datos.std[trainIndex,])
  test.s<-data.frame(datos.std[-trainIndex,])

  ##Cross-validation for C

  svm_cv <-tune("svm", AHD ~ . , data = train.s,kernel = 'linear',ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 20, 50, 100,150, 200)))

  summary(svm_cv)

  best<-svm_cv$best.model

  ##Prediction

  pred<-predict(best,test.s[,-20])

  tabla<-table(prediccion = pred, valor_real = test.s$AHD)## Confusion matrix

  (tabla[1,1]+tabla[2,2])/sum(tabla)

#+END_SRC
** 08/08/2019, SARIMAX
#+BEGIN_SRC R :results output :session SARIMAX
  ###############
  ## libraries ##
  ###############
  library(forecast)

  ################
  ## ARMA model ##
  ################

                                          #AR(1)
  set.seed(123)
  mdl.1 <- arima.sim(n = 1000, model = list(order = c(1,0,0), ar = 0.7))
  ts.plot(mdl.1)
  acf(mdl.1)

                                          #MA(1)
  set.seed(111)
  mdl.2 <- arima.sim(n = 1000, model = list(order = c(0,0,1), ma = 0.8))
  ts.plot(mdl.1)
  acf(mdl.1)

                                          # ARMA
  mdl.3 <- arima.sim(n = 1000, model = list(order = c(1,0,1),
                                            ma = 0.8,
                                            ar = 0.7))

  ## plots
  par(mfrow = c(2,3))
  ts.plot(mdl.1)
  acf(mdl.1) # indicates ma model order(q)
  pacf(mdl.1) # indicate ar model order(p)
  ts.plot(mdl.2)
  acf(mdl.2)
  pacf(mdl.2)

                                          # AUTO ARIMA
  mdl.1.aa <- auto.arima(mdl.1)
  mdl.2.aa <- auto.arima(mdl.2)
  mdl.3.aa <- auto.arima(mdl.3)

                                          # differentiation
  par(mfrow = c(2,2))
  plot(co2)
  plot(diff(co2)) # removes trend
  plot(diff(diff(co2, lag = 12))) # removes stationationality
  acf(diff(diff(co2, lag = 12))) # removes stationationality

                                          # AUTO ARIMA
  mdl.aa.2 <- auto.arima(co2, d=1, D=1)

  mdl.aa.3 <- arima(co2, order = c(1,1,1),
                    seasonal = list(order = c(1,1,2),
                                    period = 12),
                    xreg = NULL, include.mean = TRUE)

  mdl.aa.4 <- arima(co2, order = c(1,1,1),
                    seasonal = list(order = c(1,1,2),
                                    period = 12),
                    xreg = NULL, include.mean = TRUE,
                    fixed = c(NA, NA,NA,0,NA))

  ts.diag(mdl.aa.4$residuals)
  plot(forecast(mdl.aa.4, 360))

  ## prediction quality
  mean(abs(mdl.aa.4$residuals)/co2)*100
  ## compare with holt winter
  hw <- HoltWinters(co2)
  err <- hw$fitted[,1] - hw$x
  mean(abs(err)/co2)*100
#+END_SRC
* PhD
** Doctorado en neurociencias UC
*** TODO postulación
    DEADLINE: <2019-09-30 Mon> SCHEDULED: <2019-09-02 Mon>
    
*** Documentos
**** DONE Formulario de postulación (según formato en linea)
    CLOSED: [2019-07-29 Mon 15:28] SCHEDULED: <2019-07-16 Tue>
[[file:formulario-postulacion-doctorado-neurociencia.doc][formulario]]
**** DONE Certificado de titulo o grado académico, original o copia legalizada ante notario
     CLOSED: [2019-07-29 Mon 15:28] SCHEDULED: <2019-07-16 Tue>
[[file:copiasimpletitulo.jpg][CopiaTitulos]]
**** DONE Concentración de notas de pre-grado y otros estudios + ranking de egreso promoción
     CLOSED: [2019-07-29 Mon 15:32] SCHEDULED: <2019-07-16 Tue>
Incluyendo estudios de perfeccionamiento y postgrado
[[/home/nicoluarte/Downloads/concentracion_notas.pdf][concentracion_notas]]
**** TODO Dos cartas de recomendación confidenciales (según formato en linea)
     SCHEDULED: <2019-07-22 Mon>
Estas debe ser enviadas directamente por las personas que
recomiendan. Es deseable que las cartas provengan de personas con
grado académico de Doctor
**** DONE Carta de intención
     CLOSED: [2019-07-29 Mon 15:27] SCHEDULED: <2019-07-17 Wed>
Presentar una declaración de propósitos, que incluya la formulación de
un tópico de interés relevante para su estudio durante el programa y
la dedicación comprometidos para el programa. El postulante debe ser
tan especifico como sea posible en cuanto a sus intereses y objetivos
de investigación a corto y largo plazo, en una extensión no mas de
tres paginas a espacio y medio.
***** Declaración de propósitos
#+AUTHOR: Luis Nicolás Luarte Rodríguez
#+OPTIONS: toc:nil date:nil
#+LATEX_HEADER: \usepackage{setspace}
#+LATEX_HEADER: \onehalfspacing
#+LATEX_HEADER: \usepackage[round]{natbib} 
#+LATEX_HEADER: \usepackage[margin=1.2in]{geometry}
#+LATEX_HEADER: \renewcommand\refname{Referencias}
****** Motivación personal para el programa de Doctorado
El cómo buscamos objetos, información, recompensas, alimentos, etc. Ha
sido lo que ha inspirado en mayor medida mi interés en la
neurociencia. A lo largo de mi vida he sentido profunda intriga en
cómo los humanos buscan en el espacio de posibilidades, para tomar una
decisión, para evocar una memoria en particular o bien simplemente
para organizar cualquier comportamiento relativamente complejo. Investigar
sobre los mecanismos subyacentes a ese fenómeno ha sido increíblemente
enriquecedor debido al fuerte componente interdisciplinar del campo.
Esto me ha llevado a generar un profundo interés en seguir
desarrollando mi carrera en neurociencia, ya que, creo, el lograr
entender ese aparentemente simple mecanismo de decisión en condiciones
de información incompleta, puede, eventualmente, ser de gran utilidad
para la comprensión tanto de procesos de memoria y aprendizaje cómo de
ciertas patologías. Con la oportunidad del programa de Doctorado
espero contribuir a la investigación del aprendizaje y memoria.

Cómo parte de mi formación en el programa de Magíster en Neurociencias
Social de la Universidad Diego Portales, investigue, cómo parte de un
artículo de revisión, las raíces evolutivas de la búsqueda semántica
(recuperación de memorias en tareas de evocación). Una de las
principales conclusiones fue que, aunque solo en grado tentativo,
parece existir un mecanismo compartido entre la búsqueda semántica y
el forrajeo ('foraging', el comportamiento de búsqueda de alimento),
teniendo este último patrones relativamente marcados, que se extienden
a lo largo de miles de años, así cómo a través de múltiples
especies. La posibilidad de que un mecanismo tan ubicuo, responsable
del comportamiento de desplazamiento en la búsqueda de alimentos, pueda estar
relacionado por exaptación a un proceso fundamental de la memoria abre una
posibilidad de establecer un mapeo evolutivo al menos a este proceso de memoria.

Deseoso de aprender más sobre este posible vínculo entre forrajeo y
memoria, me adentre en las principales áreas aledañas de conocimiento,
tales como ecología, aprendizaje por reforzamiento ('reinforcement
learning') y otros modelos computacionales. Por la alta carga de modelos
estadísticos en las áreas mencionadas, me apunté para un programa de
diplomado en ciencia de datos de la Universidad Católica de
Chile. Además de este programa he realizado aprendizaje autónomo en
cursos en línea, con el fin de contar con todas las herramientas
técnicas que son demandadas para el área.

Adicional a los programas mencionados anteriormente, desde julio del año
2018, me encuentro participando como investigador en un proyecto
FONDECYT conjunto entre la escuela de Arquitectura y Psicología de la
Universidad Diego Portales. El tema central de esta investigación es el
estudio de la percepción de peatones en diferentes ambientes
urbanos. Si bien el tema no está relacionado directamente con el área
de interés, mi rol ha consistido en utilización de técnicas de visión
de máquina ('machine vision') y procesamiento de datos tanto para
'Eye-tracker' cómo para análisis de frecuencia de objetos. Lo
anterior, adicionado a el aprendizaje de diversos lenguajes de
programación (MATLAB, Python, R, Bash), me ha permitido desarrollar
herramientas que son útiles en la investigación en general cómo
específicamente para el área de mi interés.

****** Formulación tópico de interés
Mi tópico de interés reside en el estudio de la memoria, específicamente, la
búsqueda semántica, esto es, las estrategias o patrones utilizados al momento de
recuperar un contenido de memoria, a través del estudio de patrones
evolutivamente semejantes tales como el de 'forrajeo'.

 Las memorias semánticas han sido pensadas, teóricamente, cómo elementos
pertenecientes a cierto 'espacio' constituido por la similitud en significado
citep:lundProducingHighdimensionalSemantic1996. Así, se ha propuesto una
'distancia' entre los distintos contenidos semánticos
citep:montezRoleSemanticClustering2015 y por lo tanto la posibilidad de
'navegar' entre dichos contenidos. Considerando lo anterior, es esperable que a
lo largo de la evolución se hayan generado estrategias para acceder, de manera
útil e eficiente, a dichos contenidos. Las estrategias de búsqueda para acceder
a los contenidos semánticos han sido relacionadas a aquellas del forrajeo
citep:ForagingSemanticFields,hillsAnimalForagingEvolution2006,hillsOptimalForagingSemantic2012,
en tanto las estrategias, para ambos casos, deben lidiar con el dilema de
explorar/explotar.

Se ha observado que los 'algoritmos' utilizados en el forrajeo, pueden proveer
de soluciones óptimas para dicho dilema
citep:bartumeusAnimalSearchStrategies2005a, lo cuál aplicaría, igualmente, para
estrategias en espacios semánticos citep:montezRoleSemanticClustering2015. De
esta manera se puede observar una conexión entre un mecanismo evolutivamente
antiguo (forrajeo) y la búsqueda semántica. Permitiendo un enfoque evolutivo
comprensivo al estudio de la memoria, correspondiente al tema de mi interés.

El cómo se realiza la búsqueda en espacios semánticos es de
fundamental importancia, ya que es un espacio que está en activa
búsqueda durante la comprensión y producción de lenguaje, entre otras
citep:montezRoleSemanticClustering2015, por lo mismo, el alcance de su
importancia para casi cualquier actividad cognitiva es de gran tamaño,
pudiendo afectar de manera importante el comportamiento ante múltiples y
diferentes tareas.

****** Objetivos a corto plazo
Uno de los principales tópicos de discusión en el área de búsqueda
semántica es la organización y el tipo de la relaciones que conforman
el espacio semántico citep:lundProducingHighdimensionalSemantic1996. Así, Uno de los primeros
objetivos de investigación sería poder generar configuraciones
experimentales que permitiesen determinar, principalmente, (a) efecto
del contexto en las relaciones entre contenidos semánticos
citep:schillerMemorySpaceUnderstanding2015 y (b) si el tipo de búsqueda es más
verosímil para contenidos encadenados de manera asociativa o categórica
citep:hillsOptimalForagingSemantic2012. 

Cómo segundo objetivo a corto plazo, de manera experimental, modelar el
comportamiento en tareas de evocación de memoria, a modo de sugerir posibles
mecanismos generadores del comportamiento de búsqueda semántica. Los modelos mas
relevantes son (a) aquellos basados en reglas
citep:charnovOptimalForagingMarginal1976, (b) modelos aleatorios simples
citep:thompsonWalkingWikipediaScalefree2014 y (c) modelos aleatorios complejos
cómo discutido en citep:benhamouHowManyAnimals2007
****** Objetivos a largo plazo
Los objetivos a corto plazo están relacionados, principalmente, con el
modelamiento de comportamiento en tareas de búsqueda semántica. Por otro lado,
los objetivos a largo plazo buscarían conectar dichos modelos a sus estructuras
cerebrales (u otras) subyacentes. Uno de los candidatos parece ser el 'Locus
coeruleus' citep:kaneIncreasedLocusCoeruleus2017, corteza medial pre-frontal
ventromedial citep:kollingNeuralMechanismsForaging2012, corteza cingulada
anterior citep:shenhavAnteriorCingulateEngagement2014, entre otras. Si bien la
función del 'Locus Coeruleus' es extendida en el sistema de arousal, se observa
evidencia de participación significativa en comportamiento de
exploración-explotación relacionados al forrajeo
citep:aston-jonesAdaptiveGainRole2005, además la relativa facilidad en medición,
en conjunto con otras técnicas como electro-encefalograma, ha permitido
encontrar relación entre este y el forrajeo en situaciones experimentales
citep:slanziCombiningEyeTracking2017. Esto implicaría la combinación de modelos
de comportamiento y registro de medidas fisiológicas, tales como diámetro de
pupila. El tener un modelo de comportamiento, en el marco de explorar/explotar,
permitiría eventualmente, tener una idea de que estrategia se esta ocupando en
cada decisión y por ende evaluar la contribución de las diferentes estructuras
cerebrales a esto.

Adicionalmente, se encuentra dentro de mis objetivos a largo plazo la docencia
en el área de neurociencias, principalmente buscando ser un aporte para la
promoción de la ciencia experimental en Psicología, la cual tradicionalmente
tiene un espacio muy reducido en el currículo de pre-grado, desaprovechando un
campo muy fértil en investigación.
#+Begin_Latex
\pagebreak
#+End_Latex
bibliographystyle:apa
bibliography:ref.bib
**** TODO CV
     SCHEDULED: <2019-07-17 Wed>

**** TODO Fotocopia de la cédula de identidad o pasaporte
     SCHEDULED: <2019-07-16 Tue>
**** TODO Solicitud de ingreso a la universidad (según formato)
     SCHEDULED: <2019-07-16 Tue>
** Doctorado en ingeniería de sistemas complejos
*** TODO postulacion
    SCHEDULED: <2019-09-01 Sun> DEADLINE: <2019-11-20 Wed>
*** Documentos
*SEND ALL BACKGROUND INFORMATION TO ANDREA PINTO AT EMAIL: postgrados.fic@uai.cl*
**** TODO Enter information at website [[https://ingenieria.uai.cl/phd/disc/admission/][application]]
     SCHEDULED: <2019-08-05 Mon>
**** TODO Résumé
     SCHEDULED: <2019-07-12 Fri>
**** TODO Photocopy of chilean ID
     SCHEDULED: <2019-07-10 Wed>
**** TODO Statement of interest
Format is open, to be determined by the applicant
**** TODO Letters of recommendation
     SCHEDULED: <2019-08-05 Mon>
At least two letters of recomendation from academic or direct
supervisors
**** TODO Certificates of degrees earned
     SCHEDULED: <2019-08-05 Mon>
**** TODO Grade point average
     SCHEDULED: <2019-08-05 Mon>
With ranking or relative position within the undergraduate and
graduate programs you have completed with their respective grade
scales
**** TODO English proficiency (TOEFL)
     SCHEDULED: <2019-08-05 Mon>
**** TODO Academic interview with the program director
** Doctorado en ciencias de la complejidad social (TBD)
[[https://dccs.udd.cl/es/][PHD PROGRAM]]
** Becas
   SCHEDULED: <2019-07-19 Fri>
Leer e imprimir el manual de becas chile, ver en profundidad bases para la
postulación de becas UC y UAI
** Correos
Diego Cosmelli: dcosmelli@uc.cl

Estimado profesor Diego Cosmelli,

Actualmente soy estudiante del programa de Magíster en Neurociencia Social de la
Universidad Diego Portales. Me encuentro en proceso de finalización del
Magíster, y he encontrado gran interés en el programa de Doctorado en
Neurociencias de su Universidad. 

Cómo parte de mi investigación de literatura tuve la oportunidad de leer su
artículo 'Modeling Search Behaviors during the Acquisition of Expertise in a
Sequential Decision-Making Task'. Específicamente el modelamiento de toma de
decisiones secuenciales me provocó gran interés, ya que el tema principal de mi
proyecto de tesis se centra en la raíces evolutivas de la toma de decisiones
secuenciales y los mecanismos neuronales subyacentes. He estado en búsqueda de
programas de Doctorado dónde pueda continuar el trabajo de investigación en esta
área. Mi proyecto (tentativo) se centra en el modelamiento de toma de decisiones
secuenciales tanto a nivel de comportamiento de desplazamiento en la búsqueda
(foraging) cómo a nivel de búsqueda semántica en espacios cognitivos.
Adicionalmente, recae mi interés en la formación de conciencia a través de los
procesos mencionados anteriormente en el espíritu de 'From foraging to
autonoetic consciousness: The primal self as a consequence of embodied
prospective foraging'.

Esperando no causar mayor molestia, quisiera saber si existe la posibilidad de
saber, si actualmente, está recibiendo estudiantes en calidad de
tutor/supervisor para el programa de Doctorado. Si es así, ¿estaría dispuesto a
seguir esta conversación por el medio que más guste?, sea por correo, teléfono o
una visita al campus/oficina. He investigado el sitio del programa en detalle, y
creo existe buen ajuste con mi intereses de investigación, principalmente por el
fuerte foco interdisciplinar (inherente al objeto de estudio de mi interés) y
por la posibilidad de expandir un línea de investigación relativamente novedosa.

Apreciando cualquier tiempo que me pudiese destinar, lo agradezco de antemano y
quedo atento a su respuesta.

Saludos cordiales,

Nicolás Luarte

* Projects
** FONDECYT
*** Initial inspection:
 #+BEGIN_SRC R :results output :session peatones
                                           # load packages
   pkg <- c("dplyr", "ggplot2", "tidyverse", "corrplot", "Hmisc", "psycho",
            "broom", "gvlma", "pROC", "caret", "MASS", "effects", "car",
            "ResourceSelection", "neuralnet", "lmtest")
   invisible(lapply(pkg, library, character.only = TRUE))

                                           # load database
   setwd("/home/nicoluarte/Downloads")
   df <- data.frame(read.csv("data_fondecyt.csv"))
   head(df)

                                           # models
   ## null model
   mdl.null <- glm(Valence_num ~ 1, data = df, family = binomial(link = "logit"))
   ## simple model, only noise
   mdl.1 <- glm(Valence_num ~ Noise, data = df, family = binomial(link = "logit"))
   summary(mdl.1)
   ## getting model estimate into a interpretable probability
   ## y* = ln(p/1-p) = b_0 + b_1*Noise
   ## p = exp(b_0 + b_1*Noise) / exp(b_0 + b_1*Noise) +1
   ## p = exp(y*) / exp(y*)+1

                                           # plot first model
   ilink <- family(mdl.1)$linkinv
   pd <- with(df,
              data.frame(Noise = seq(min(Noise),
                                     max(Noise),
                                     length = 100)))
   pd <- cbind(pd, predict(mdl.1, pd, type = "link", se.fit = TRUE)[1:2])
   pd <- transform(pd, Fitted = ilink(fit), Upper = ilink(fit + (2*se.fit)),
                   Lower = ilink(fit - (2*se.fit)))
   ## plot probabilities with confidence interval (Wald)
   ggplot(df, aes(x = Noise, y = as.numeric(Valence_num))) +
     geom_ribbon(data = pd, aes(ymin = Lower, ymax = Upper, x = Noise),
                 fill = "gray", alpha = 0.2, inherit.aes = FALSE) +
     geom_line(data = pd, aes(y = Fitted, x = Noise)) +
     geom_point() +
     labs(y = "Probability of positive mood", x = "Noise")

                                           # test models
   ## measures of fit:
   ## likelihood ratio test, comparing our model to the null model
   lmtest::lrtest(mdl.null, mdl.1)
   ## mdl.1 coefficient are significant
   ## The reference for this is Joseph M. Hilbe (2009) Logistic Regression Methods, CRC Press, pages 81-82
   ## Hosmer and lemeshow goodness of fit
   mdl.simple.test <- hoslem.test(mdl.1$y, fitted(mdl.1), g = 10)
   mdl.complete.test <- hoslem.test(mdl.complete$y, fitted(mdl.complete), g = 10)

   ## ROC
   mdl.1.preds <- predict(mdl.1, type = "response")
   mdl.1.roc <- roc(df$Valence_num, mdl.1.preds,
                    smoothed = TRUE,
                    ci = TRUE, ci.alpha = 0.5, stratified = FALSE,
                    plot = TRUE, auc.polygon = TRUE, max.aux.polygon = TRUE, grid = TRUE,
                    print.auc = TRUE, show.thres=TRUE)
   ## mdl.1 has poor discrimination
   ## get list of all possible thresholds and corresponding specificity and sensitivity
   mdl.1.c <- coords(roc = mdl.1.roc, x = "all", transpose = FALSE)
   ## we could calculate the threshold that maximizes the sum of specificity and sensitivity
   mdl.1.optc <- coords(mdl.1.roc, "best", "threshold")
   ## get both confusion matrices
   ## c = 0.5
   mdl.1.norm <- factor(ifelse(mdl.1.preds > 0.5, "Positive", "Negative"))
   confusionMatrix(mdl.1.norm, df$Valence)
   ## c = optimal <- 0.4396930
   mdl.1.opt <- factor(ifelse(mdl.1.preds > mdl.1.optc["threshold"], "Positive", "Negative"))
   confusionMatrix(mdl.1.opt, df$Valence)


   ## repeated k-fold cross-validation
   cntrl <- trainControl(method = "repeatedcv",
                         number = 10,
                         repeats = 10)
   mdl.1.kfold <- train(as.factor(Valence_num) ~ Noise,
                        data = na.omit(df),
                        method = "glm",
                        family = binomial(link = "logit"),
                        trControl = cntrl)

                                           # step-wise models
   ## step-wise regression
   ## socioeconomic excluded 100% colinear with neighbourhood
   mdl.1.step <- train(as.factor(Valence_num) ~ Noise + Pedestrians +
                      Cars + Neighbourhood + Sex_num,
                        data = na.omit(df),
                        method = "glmStepAIC",
                        family = binomial(link = "logit"),
                      trControl = cntrl,
                      trace = FALSE)
   mdl.1.step$finalModel
   summary(mdl.1.step$finalModel)

   ## under Akaike information criterion (goodness of fit ~ simplicity)
   ## Valence ~ Noise + Pedestrians + Neighbourhood
   ## we could test only noise model against complete one
   mdl.complete <- glm(Valence_num ~ Noise + Pedestrians + Neighbourhood,
                       data = df,
                       family = binomial(link = "logit"))
   lmtest::lrtest(mdl.1, mdl.complete)
   ## model coefficients are significant against simpler model

   ## confusion matrix, roc, and k-fold validation for complete model
   ## ROC
   mdl.complete.preds <- predict(mdl.complete, type = "response")
   mdl.complete.roc <- roc(df$Valence_num, mdl.complete.preds,
                    smoothed = TRUE,
                    ci = TRUE, ci.alpha = 0.5, stratified = FALSE,
                    plot = TRUE, auc.polygon = TRUE, max.aux.polygon = TRUE, grid = TRUE,
                    print.auc = TRUE, show.thres=TRUE)
   ## confusion matrices
   ## we could calculate the threshold that maximizes the sum of specificity and sensitivity
   mdl.complete.optc <- coords(mdl.complete.roc, "best", "threshold")
   ## get both confusion matrices
   ## c = 0.5
   mdl.complete.norm <- factor(ifelse(mdl.complete.preds > 0.5, "Positive", "Negative"))
   confusionMatrix(mdl.complete.norm, df$Valence)
   ## c = optimal <- 0.4396930
   mdl.complete.opt <- factor(ifelse(mdl.complete.preds > mdl.complete.optc["threshold"], "Positive", "Negative"))
   confusionMatrix(mdl.complete.opt, df$Valence)
   ## k-fold
   mdl.complete.kfold <- train(as.factor(Valence_num) ~ Noise + Pedestrians + Neighbourhood,
                               data = na.omit(df),
                        method = "glm",
                        family = binomial(link = "logit"),
                        trControl = cntrl)
   ## accuracy of simple a complete model are almost the same


                                           # coefficients interpretation
   plot(allEffects(mdl.complete))

                                           # what's left
   ## checking for outlier influence

                                           # neural-net
   nn.control <- trainControl(method = "repeatedcv",
                         number = 10,
                         repeats = 10)
   nn.grid <- expand.grid(size = seq(from = 1, to = 15, by = 1),
                         decay = seq(from = 0.1, to = 0.5, by = 0.1))
   nn.kfold <- train(as.factor(Valence_num) ~ (.)^2,
                        data = select_if(df, is.numeric),
                        method = "nnet",
                     trControl = nn.control,
                     preProcess = c('center', 'scale'),
                     tuneGrid =  nn.grid)
 #+END_SRC

 #+RESULTS:
 #+begin_example

   Subjects Valence_num  Valence       Noise Pedestrians      Cars       Pupil
 1        1           0 Negative 0.010767789  0.00000000 0.0000000         2.3
 2        1           0 Negative 0.012513303  0.46391753 0.1546392          []
 3        1           1 Positive 0.010372872  0.41237113 0.0000000  2.52325703
 4        1           1 Positive 0.009794006  0.09278351 0.0000000 2.079852955
 5        1           1 Positive 0.013058803  0.48453608 0.0000000 2.505264791
 6        1           1 Positive 0.024941132  0.16494845 0.0000000 2.590279936
   Neighbourhood_num Neighbourhood Socioeconomic_num Socioeconomic Sex_num
 1                 1          Cumm                 1        Middle       0
 2                 1          Cumm                 1        Middle       0
 3                 1          Cumm                 1        Middle       0
 4                 1          Cumm                 1        Middle       0
 5                 1          Cumm                 1        Middle       0
 6                 1          Cumm                 1        Middle       0

                   Subjects Valence_num   Noise Pedestrians    Cars
 Subjects                                                          
 Valence_num       -0.19***                                        
 Noise              0.36***    -0.17***                            
 Pedestrians             0       -0.09  0.13**                     
 Cars               0.19***      -0.05    0.1*       -0.07         
 Neighbourhood_num  0.99***    -0.21*** 0.36***      -0.01  0.18***
 Socioeconomic_num  0.95***    -0.19*** 0.36***       0.01  0.13** 
 Sex_num              0.03        0.01  0.14**         0.1  0.14** 
                   Neighbourhood_num Socioeconomic_num
 Subjects                                             
 Valence_num                                          
 Noise                                                
 Pedestrians                                          
 Cars                                                 
 Neighbourhood_num                                    
 Socioeconomic_num           0.96***                  
 Sex_num                      -0.03             -0.06

 Call:
 glm(formula = Valence_num ~ Noise, family = binomial(link = "logit"), 
     data = df)

 Deviance Residuals: 
     Min       1Q   Median       3Q      Max  
 -1.2618  -1.0714  -0.8653   1.2181   2.0739  

 Coefficients:
             Estimate Std. Error z value Pr(>|z|)    
 (Intercept)   0.2316     0.1372   1.689   0.0912 .  
 Noise       -26.2153     5.5992  -4.682 2.84e-06 ***
 ---
 Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

 (Dispersion parameter for binomial family taken to be 1)

     Null deviance: 1025.4  on 753  degrees of freedom
 Residual deviance: 1000.9  on 752  degrees of freedom
 AIC: 1004.9

 Number of Fisher Scoring iterations: 4

 Confusion Matrix and Statistics

           Reference
 Prediction   0   1
          0 391 244
          1  47  72

                Accuracy : 0.6141         
                  95% CI : (0.5783, 0.649)
     No Information Rate : 0.5809         
     P-Value [Acc
 NIR] : 0.03489        

                   Kappa : 0.132          

  Mcnemar's Test P-Value : < 2e-16        

             Sensitivity : 0.22785        
             Specificity : 0.89269        
          Pos Pred Value : 0.60504        
          Neg Pred Value : 0.61575        
              Prevalence : 0.41910        
          Detection Rate : 0.09549        
    Detection Prevalence : 0.15782        
       Balanced Accuracy : 0.56027        

        'Positive' Class : 1

 Setting levels: control = 0, case = 1
 Setting direction: controls < cases

 Confusion Matrix and Statistics

           Reference
 Prediction  0  1
          0 75 42
          1 17 16

                Accuracy : 0.6067          
                  95% CI : (0.5237, 0.6853)
     No Information Rate : 0.6133          
     P-Value [Acc
 NIR] : 0.601529        

                   Kappa : 0.099           

  Mcnemar's Test P-Value : 0.001781        

             Sensitivity : 0.2759          
             Specificity : 0.8152          
          Pos Pred Value : 0.4848          
          Neg Pred Value : 0.6410          
              Prevalence : 0.3867          
          Detection Rate : 0.1067          
    Detection Prevalence : 0.2200          
       Balanced Accuracy : 0.5455          

        'Positive' Class : 1

 Warning message:
 NAs introduced by coercion

 Generalized Linear Model 

 490 samples
   4 predictor
   2 classes: '0', '1' 

 Pre-processing: centered (4), scaled (4) 
 Resampling: Cross-Validated (5 fold, repeated 10 times) 
 Summary of sample sizes: 392, 392, 393, 391, 392, 392, ... 
 Resampling results:

   Accuracy   Kappa    
   0.5879787  0.1654871

 Warning message:
 In coords.roc(roc.curve, "best") :
   An upcoming version of pROC will set the 'transpose' argument to FALSE by default. Set transpose = TRUE explicitly to keep the current behavior, or transpose = FALSE to adopt the new one and silence this warning. Type help(coords_transpose) for additional information.

               default   optimal
 Sensitivity 0.2758621 0.7586207
 Specificity 0.8152174 0.5000000

 Setting levels: control = 0, case = 1
 Setting direction: controls < cases

 Warning message:
 In coords.roc(roc.curve.1, "best") :
   An upcoming version of pROC will set the 'transpose' argument to FALSE by default. Set transpose = TRUE explicitly to keep the current behavior, or transpose = FALSE to adopt the new one and silence this warning. Type help(coords_transpose) for additional information.

               default   optimal
 Sensitivity 0.4655172 0.8103448
 Specificity 0.7065217 0.4673913
 #+end_example
*** Model check
Complete model with interactions
#+BEGIN_SRC R :results output :session peatones
                                          # complete model with interactions
  log.mdl.full <- train(as.factor(Valence_num) ~ (Noise + Pedestrians +
                                                  Cars + Neighbourhood + Sex_num)^2,
                        data = na.omit(df),
                        method = "glmStepAIC",
                        family = binomial(link = "logit"),
                        trControl = cntrl,
                        trace = FALSE)
  ## save model
  saveRDS(log.mdl.full, "full_log_mdl.rds")

  ## confusion matrix of complete model with interactions
  log.mdl.full.preds <- predict(log.mdl.full$finalModel, type = "response")
  log.mdl.preds <- factor(ifelse(log.mdl.full.preds > 0.5, "Positive", "Negative"))
  confusionMatrix(log.mdl.preds, df$Valence)
#+END_SRC

#+RESULTS:
#+begin_example
Error in train(as.factor(Valence_num) ~ (Noise
Pedestrians
Cars
 : 
  could not find function "train"
Error in saveRDS(log.mdl.full, "full_log_mdl.rds") : 
  object 'log.mdl.full' not found
Error in predict(log.mdl.full$finalModel, type = "response") : 
  object 'log.mdl.full' not found
Error in ifelse(log.mdl.full.preds
0.5, "Positive", "Negative") : 
  object 'log.mdl.full.preds' not found
Error in confusionMatrix(log.mdl.preds, df$Valence) : 
  could not find function "confusionMatrix"
#+end_example

Overall model evaluation
#+BEGIN_SRC R :results output :session peatones
  ## build all simple models
  log.mdl.1 <- glm(Valence_num ~ Noise, data = df, family = binomial(link = "logit"))
  log.mdl.2 <- glm(Valence_num ~ Pedestrians, data = df, family = binomial(link = "logit"))
  log.mdl.3 <- glm(Valence_num ~ Cars, data = df, family = binomial(link = "logit"))
  log.mdl.4 <- glm(Valence_num ~ Neighbourhood, data = df, family = binomial(link = "logit"))
  log.mdl.5 <- glm(Valence_num ~ Socioeconomic, data = df, family = binomial(link = "logit"))
  log.mdl.6 <- glm(Valence_num ~ Sex_num, data = df, family = binomial(link = "logit"))

  ## build all 2 predictor models
  log.mdl.7 <- glm(Valence_num ~ Noise + Pedestrians, data = df, family = binomial(link = "logit"))
  log.mdl.8 <- glm(Valence_num ~ Noise + Cars, data = df, family = binomial(link = "logit"))
  log.mdl.9 <- glm(Valence_num ~ Noise + Neighbourhood, data = df, family = binomial(link = "logit"))
  log.mdl.10 <- glm(Valence_num ~ Noise + Socioeconomic, data = df, family = binomial(link = "logit"))
  log.mdl.11 <- glm(Valence_num ~ Noise + Sex_num, data = df, family = binomial(link = "logit"))

  ## Build all 3 predictor models
  log.mdl.12 <- glm(Valence_num ~ Noise + Pedestrians + Cars, data = df, family = binomial(link = "logit"))
  log.mdl.13 <- glm(Valence_num ~ Noise + Pedestrians + Neighbourhood, data = df, family = binomial(link = "logit"))
  log.mdl.14 <- glm(Valence_num ~ Noise + Pedestrians + Socioeconomic, data = df, family = binomial(link = "logit"))
  log.mdl.15 <- glm(Valence_num ~ Noise + Pedestrians + Sex_num, data = df, family = binomial(link = "logit"))
  log.mdl.16 <- glm(Valence_num ~ Noise + Cars + Neighbourhood, data = df, family = binomial(link = "logit"))
  log.mdl.17 <- glm(Valence_num ~ Noise + Cars + Socioeconomic, data = df, family = binomial(link = "logit"))
  log.mdl.18 <- glm(Valence_num ~ Noise + Cars + Sex_num, data = df, family = binomial(link = "logit"))

  ## list all models
  mdl.list <- list(log.mdl.1, log.mdl.2, log.mdl.3, log.mdl.3, log.mdl.4, log.mdl.5,
                   log.mdl.6, log.mdl.7, log.mdl.8, log.mdl.9, log.mdl.10,
                   log.mdl.11, log.mdl.12, log.mdl.13, log.mdl.14, log.mdl.15, log.mdl.16,
                   log.mdl.17, log.mdl.18)

  ## likelihood ratio test
  lrt <- sapply(mdl.list, function(x) c(lrtest(mdl.null, x)))
  ## Wald test
  walds <- sapply(mdl.list, function(x) data.frame(summary.glm(x)$coefficients))


                                          # comparison with intercept only model
  ## likelihood ratio model 1
  likelihood.raio.test <- lrtest(mdl.null


#+END_SRC
*** Complete models
#+BEGIN_SRC R :results output :session peatones
  ## fit simple model
  glm.mdl.null <- glm(Valence_num ~ 1,
                     data = df,
                     family = binomial(link = "logit"))
  glm.mdl.1 <- glm(Valence_num ~ Noise,
                   data = df,
                   family = binomial(link = "logit"))
  glm.mdl.2 <- glm(Valence_num ~ Noise + Pedestrians,
                   data = df,
                   family = binomial(link = "logit"))
  glm.mdl.3 <- glm(Valence_num ~ Noise + Cars,
                   data = df,
                   family = binomial(link = "logit"))
  glm.mdl.4 <- glm(Valence_num ~ Noise + Neighbourhood,
                   data = df,
                   family = binomial(link = "logit"))
  glm.mdl.5 <- glm(Valence_num ~ Noise + Socioeconomic,
                   data = df,
                   family = binomial(link = "logit"))
  glm.mdl.6 <- glm(Valence_num ~ Noise + Pedestrians + Cars,
                   data = df,
                   family = binomial(link = "logit"))
  glm.mdl.7 <- glm(Valence_num ~ Noise + Pedestrians + Neighbourhood,
                   data = df,
                   family = binomial(link = "logit"))
  glm.mdl.8 <- glm(Valence_num ~ Noise + Pedestrians + Socioeconomic,
                   data = df,
                   family = binomial(link = "logit"))
  glm.list <- list(glm.mdl.1, glm.mdl.2, glm.mdl.3, glm.mdl.4, glm.mdl.5,
                   glm.mdl.6, glm.mdl.7, glm.mdl.8)

  ## walds-test
  glm.wald <- sapply(glm.list, function(x) Anova(x, test.statistic = c("Wald")),
                 simplify = FALSE)
  ## likelihood ratio test
  glm.lr <- sapply(glm.list, function(x) anova(glm.mdl.null, x, test = "LRT"),
                   simplify = FALSE)
  ## score test
  glm.score <- sapply(glm.list, function(x) anova(glm.mdl.null, x, test = "Rao"),
                      simplify = FALSE)
  ## hosmer & lemeshow, g = covariates + 1
  glm.hl <- sapply(glm.list, function(x) hoslem.test(x$y, fitted(x), g= 3))
  ## AIC for every model
  glm.aic <- sapply(glm.list, function(x) AIC(x))
  ## VIF for all models
  glm.vif <- sapply(glm.list[2:8], function(x) vif(x))

  ## repeated k-fold cross validated models
  cntrl <- trainControl(method = "repeatedcv",                      number = 10,
                        repeats = 10,
                        savePredictions = TRUE)
  mdl.1.kfold <- train(as.factor(Valence_num) ~ Noise,
                       data = na.omit(df),
                       method = "glm",
                       family = binomial(link = "logit"),
                       trControl = cntrl)
  mdl.2.kfold <- train(as.factor(Valence_num) ~ Noise + Pedestrians,
                       data = na.omit(df),
                       method = "glm",
                       family = binomial(link = "logit"),
                       trControl = cntrl)
  mdl.3.kfold <- train(as.factor(Valence_num) ~ Noise + Cars,
                       data = na.omit(df),
                       method = "glm",
                       family = binomial(link = "logit"),
                       trControl = cntrl)
  mdl.4.kfold <- train(as.factor(Valence_num) ~ Noise + Neighbourhood,
                       data = na.omit(df),
                       method = "glm",
                       family = binomial(link = "logit"),
                       trControl = cntrl)
  mdl.5.kfold <- train(as.factor(Valence_num) ~ Noise + Socioeconomic,
                       data = na.omit(df),
                       method = "glm",
                       family = binomial(link = "logit"),
                       trControl = cntrl)
  mdl.6.kfold <- train(as.factor(Valence_num) ~ Noise + Pedestrians + Cars,
                       data = na.omit(df),
                       method = "glm",
                       family = binomial(link = "logit"),
                       trControl = cntrl)
  mdl.7.kfold <- train(as.factor(Valence_num) ~ Noise + Pedestrians + Neighbourhood,
                       data = na.omit(df),
                       method = "glm",
                       family = binomial(link = "logit"),
                       trControl = cntrl)
  mdl.8.kfold <- train(as.factor(Valence_num) ~ Noise + Pedestrians + Socioeconomic,
                       data = na.omit(df),
                       method = "glm",
                       family = binomial(link = "logit"),
                       trControl = cntrl)


  mdl.1.pred <- predict(mdl.1.kfold)
  mdl.2.pred <- predict(mdl.2.kfold)
  mdl.3.pred <- predict(mdl.3.kfold)
  mdl.4.pred <- predict(mdl.4.kfold)
  mdl.5.pred <- predict(mdl.4.kfold)
  mdl.6.pred <- predict(mdl.4.kfold)
  mdl.7.pred <- predict(mdl.4.kfold)
  mdl.8.pred <- predict(mdl.4.kfold)

  mdl.1.cm <- caret::confusionMatrix(mdl.1.pred, as.factor(df$Valence_num))
  mdl.2.cm <- caret::confusionMatrix(mdl.2.pred, as.factor(df$Valence_num))
  mdl.3.cm <- caret::confusionMatrix(mdl.3.pred, as.factor(df$Valence_num))
  mdl.4.cm <- caret::confusionMatrix(mdl.4.pred, as.factor(df$Valence_num))
  mdl.5.cm <- caret::confusionMatrix(mdl.1.pred, as.factor(df$Valence_num))
  mdl.6.cm <- caret::confusionMatrix(mdl.2.pred, as.factor(df$Valence_num))
  mdl.7.cm <- caret::confusionMatrix(mdl.3.pred, as.factor(df$Valence_num))
  mdl.8.cm <- caret::confusionMatrix(mdl.4.pred, as.factor(df$Valence_num))


  par(mfrow = c(4,2))
  fourfoldplot(mdl.1.cm$table, main = "Noise")
  fourfoldplot(mdl.2.cm$table, main = "Noise + Pedestrians")
  fourfoldplot(mdl.3.cm$table, main = "Noise + Cars")
  fourfoldplot(mdl.4.cm$table, main = "Noise + Neighborhood")
  fourfoldplot(mdl.4.cm$table, main = "Noise + Socioeconomic")
  fourfoldplot(mdl.4.cm$table, main = "Noise + Pedestrians + Cars")
  fourfoldplot(mdl.4.cm$table, main = "Noise + Pedestrians + Neighborhood")
  fourfoldplot(mdl.4.cm$table, main = "Noise + Pedestrians + Socioeconomic")

  library(MASS)
  odd.ratio <- sapply(glm.list, function(x) exp(cbind(coef(x), confint(x))))

  ## plot linear odd-ratio
##  sapply(glm.list, function(x) plot(allEffects(x)))
#+END_SRC

#+RESULTS:
: Waiting for profiling to be done...
: Waiting for profiling to be done...
: Waiting for profiling to be done...
: Waiting for profiling to be done...
: Waiting for profiling to be done...
: Waiting for profiling to be done...
: Waiting for profiling to be done...
: Waiting for profiling to be done...

* Defensa de Tesis
** Gráficos
#+begin_src python :results output
  import numpy as np
  import scipy
  from scipy.stats import uniform
  from scipy.stats import levy
  from scipy.stats import powerlaw
  import matplotlib.pyplot as plt

  def levy_walk(n):
      # uniform distribution for turning angles
      angle = uniform.rvs(size=(n,), loc=0, scale=2.*np.pi)

      # levy dist step length
      r = powerlaw.rvs(0.05, size=n)

      # coordinates
      x = np.cumsum(r*np.cos(angle))
      y = np.cumsum(r*np.sin(angle))

      return np.array((x, y, r, angle))

  # steps to be simulated
  n = 500

  # levy flight
  foo = levy_walk(n)
  foo2 = levy_walk(200)

  fig = plt.figure(figsize=(14, 6))

  # plot the walk
  ax1 = fig.add_subplot(2,1,2)
  ax1.plot(foo[0,:], foo[1,:], 'g--')
  ax1.set_title('Simulated foraging pattern')
  ax1.set_xticks([])
  ax1.set_yticks([])

  # plot the histogram
  ax2 = fig.add_subplot(2,2,1)
  ax2.set_yscale('log')
  ax2.hist(foo[2,:], bins=int(n/10), color = 'green')
  ax2.set_title('Foraging step-size histogram')
  ax2.set_xticks([])
  ax2.set_yticks([])

  # IRI histogram
  ax3 = fig.add_subplot(2,2,2)
  ax3.set_yscale('log')
  ax3.hist(foo2[2,:], bins=int(n/10), color = 'gray')
  ax3.set_title('Inter-response interval histogram')
  ax3.set_xticks([])
  ax3.set_yticks([])

  plt.show()

  def truncated_power_law(a,m):
      x = np.arange(1, m+1, dtype='float')
      pmf = 1/x**a
      pmf /= pmf.sum()
      return scipy.stats.rv_discrete(values=(range(1, m+1), pmf))

  a, m = 2, 10
  d = truncated_power_law(a=a, m=m)

  N = 1000
  sample = d.rvs(size=N)
#+end_src


