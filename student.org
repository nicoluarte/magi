* Diplomado data science
** 09/07/2019, decision tree
*** Cross-validation
**** Leave one out
**** k fold cross validation
*** Decision tree
R code to generate a decision tree
- First a tree is generated
- Later on is pruned
- In this example the difference is not very notorious
#+BEGIN_SRC R :results output :session rs
library(rpart)
library(tree)
setwd("/home/nicoluarte/Downloads")
df = read.table('cleveland.txt', h=T)
head(df)
summary(df)

                                        # Using rpart

## no data standarization needed for trees

set.seed(123)

## creating the first tree

tree.1 = rpart(df[,14] ~ .,
               data=df[,1:13],
               cp=0.0001,parms=list(split="information"),
               method="class")

##split information means entropy/ split could be gini - cp is the complexity parameter.

par(xpd=TRUE)
plot(tree.1, uniform=T);
text(tree.1, all=T, pretty=T, fancy=T, use.n=T, fwidth=0.3, fheight=0.3)

##pretty is for compatibility with tree package. Considers the minimum length for abbreviation of character or factor variables (4 L).

## Pruning

plotcp(tree.1) ## Cross-validation results

printcp(tree.1)

tree.1$cptable[which.min(tree.1$cptable[,"xerror"]),"CP"]

tree.2<-prune(tree.1,cp=0.01102941)

par(xpd=TRUE)
plot(tree.2, uniform=T);
text(tree.2, all=T, pretty=T, fancy=T, use.n=T, fwidth=0.3, fheight=0.3)

plotcp(tree.2) ## Cross-validation results

printcp(tree.2)

#+END_SRC

#+RESULTS:
#+begin_example
  age gender     cp trestbps chol  fbs restecg thatach exang oldpeak slope ca
1  63   male angina      145  233 true     hyp     150   fal     2.3  down  0
2  67   male asympt      160  286  fal     hyp     108  true     1.5  flat  3
3  67   male asympt      120  229  fal     hyp     129  true     2.6  flat  2
4  37   male notang      130  250  fal    norm     187   fal     3.5  down  0
5  41    fem abnang      130  204  fal     hyp     172   fal     1.4    up  0
6  56   male abnang      120  236  fal    norm     178   fal     0.8    up  0
  thal diag Col15
1  fix buff     H
2 norm sick    S2
3  rev sick    S1
4 norm buff     H
5 norm buff     H
6 norm buff     H
      age         gender         cp         trestbps          chol      
 Min.   :29.00   fem : 95   abnang: 49   Min.   : 94.0   Min.   :126.0  
 1st Qu.:48.00   male:201   angina: 23   1st Qu.:120.0   1st Qu.:211.0  
 Median :56.00              asympt:141   Median :130.0   Median :242.5  
 Mean   :54.52              notang: 83   Mean   :131.6   Mean   :247.2  
 3rd Qu.:61.00                           3rd Qu.:140.0   3rd Qu.:275.2  
 Max.   :77.00                           Max.   :200.0   Max.   :564.0  
   fbs      restecg       thatach       exang        oldpeak       slope    
 fal :253   abn :  4   Min.   : 71.0   fal :199   Min.   :0.000   down: 21  
 true: 43   hyp :145   1st Qu.:133.0   true: 97   1st Qu.:0.000   flat:137  
            norm:147   Median :152.5              Median :0.800   up  :138  
                       Mean   :149.6              Mean   :1.059             
                       3rd Qu.:166.0              3rd Qu.:1.650             
                       Max.   :202.0              Max.   :6.200             
       ca           thal       diag     Col15   
 Min.   :0.0000   fix : 18   buff:160   H :160  
 1st Qu.:0.0000   norm:163   sick:136   S1: 53  
 Median :0.0000   rev :115              S2: 35  
 Mean   :0.6791                         S3: 35  
 3rd Qu.:1.0000                         S4: 13  
 Max.   :3.0000

Classification tree:
rpart(formula = df[, 14] ~ ., data = df[, 1:13], method = "class", 
    parms = list(split = "information"), cp = 1e-04)

Variables actually used in tree construction:
[1] age     ca      cp      exang   oldpeak thal    thatach

Root node error: 136/296 = 0.45946

n= 296 

         CP nsplit rel error  xerror     xstd
1 0.4926471      0   1.00000 1.00000 0.063044
2 0.0514706      1   0.50735 0.63971 0.057630
3 0.0404412      3   0.40441 0.52941 0.054276
4 0.0220588      5   0.32353 0.44853 0.051170
5 0.0110294      6   0.30147 0.44118 0.050857
6 0.0036765      8   0.27941 0.44118 0.050857
7 0.0001000     10   0.27206 0.46324 0.051780
[1] 0.01102941

Classification tree:
rpart(formula = df[, 14] ~ ., data = df[, 1:13], method = "class", 
    parms = list(split = "information"), cp = 1e-04)

Variables actually used in tree construction:
[1] age   ca    cp    exang thal 

Root node error: 136/296 = 0.45946

n= 296 

        CP nsplit rel error  xerror     xstd
1 0.492647      0   1.00000 1.00000 0.063044
2 0.051471      1   0.50735 0.63971 0.057630
3 0.040441      3   0.40441 0.52941 0.054276
4 0.022059      5   0.32353 0.44853 0.051170
5 0.011029      6   0.30147 0.44118 0.050857
6 0.011029      8   0.27941 0.44118 0.050857
#+end_example
** 11/07/2019, GLM
*** Linear regression
**** Conditions
**** Multiple linear regression
*** GLM
- A function is applied to the X's, or a transformation. These are
called link functions
- Started modelling probability of event (Bernoulli, logit of 'p')
- Logit predicts 'odds' <- logistic regression
- In GLM response variable can have any distribution
Using coimbra breast cancer dataset
*CHALLENGE REMOVE OUTLIERS AND RE-RUN ANALYSIS*
- There's not much difference in removing outliers
#+BEGIN_SRC R :results output
setwd("/home/nicoluarte/Downloads/")
df <- read.csv("dataR2.csv")
head(df)
resp <- c(df$Classification - 1)

                                        # compare cancer/no-cancer by age
library(doBy)
summaryBy(Age~resp, data = df, FUN = c(mean, median))

                                        # compare by glucose
summaryBy(Glucose~resp, data = df, FUN = c(mean, median))
summaryBy(Insulin~resp, data = df, FUN = c(mean, median))

                                        # t-test to check means
t.test(Age~resp, data = df)
t.test(Glucose~resp, data = df)
t.test(BMI~resp, data = df)
                                        # re-code variable
df$Glucose2 <- ifelse(df$Glucose<=100, "<=100", ">100")
chisq.test(table(df$Glucose2, resp))

                                        # OR calculation
or <- (44*27)/(8*37)

                                        # Plots
plot(resp~Glucose, data = df)
abline(lm(resp~Glucose, data = df), col = "red")

                                        # Logistic regression
mdl.0 = glm(resp~Glucose2, data = df, family = binomial(link = "logit"))
mdl.1 = glm(resp~Glucose, data = df, family = binomial(link = "logit"))

                                        # Calculating odds
exp(1.3897) # 4 times greater the chance to get cancer

                                        # Box plot
boxplot(Glucose~resp, data = df)

                                        # Logistic regression insulin2
df$Insulin2 <- ifelse(df$Insulin<=8, "<=8", ">8")
mdl.2 = glm(resp~Insulin2, data = df, family = binomial(link = "logit"))
summary(mdl.2)

mdl.3 <- glm(resp~Glucose, data = df, family = binomial(link = "logit"))
summary(mdl.3)

                                        # Calculating odds for different values
exp(0.07867*5)
plot(0.07867*1:10)

                                        # multi-variable model
mdl.4 <- glm(resp~Glucose + Insulin, data = df, family = binomial(link = "logit"))
summary(mdl.4)
plot(Glucose~Insulin, data = df)

                                        # Adding control variables
mdl.5 <- glm(resp~Age + BMI + Glucose, data = df, family = binomial(link = "logit"))
summary(mdl.5)
exp(mdl.5$coefficients["Glucose"]) # each glucose point has a 10%~ effect on chance of cancer

                                        # ROC curve
library(pROC)
prob <- predict(mdl.5, type = c("response"))
roc(resp~prob, data = df, plot = T)

                                        # Use all variables
full.mdl <- glm(resp~., data = df[,1:9], family = binomial(link = "logit"))
summary(full.mdl)

                                        # Prune!

new_vars = setdiff(names(df[,1:9]),c(names(full.mdl$coefficients)[which.max(full.mdl$coefficients)]))
prune.mdl <- glm(resp~., data = df[new_vars], family = binomial(link = "logit"))
summary(prune.mdl)

                                        # Calculate confusion matrix
library(caret)
preds <- as.numeric(predict(full.mdl, type = c("response")) > 0.5)
confusionMatrix(as.factor(preds), as.factor(resp))

                                        # Generate same model without outliers
df_removed_outliers = df[NA,] # same df but NA rows
df_removed_outliers["Classification"] <- df["Classification"]
df_removed_outliers$Classification <- df_removed_outliers$Classification - 1 # to get same levels
for (var in names(df[,1:9]))
{
  outliers <- boxplot.stats(df[,var]) # get the outlier of this variable
  df_target <- df[,var] # get the vector of values of this variable
  idx <- which(df_target %in% outliers$out) # get the idx of outliers in vector
  df_target[idx] <- NA # put NAN's in there
  df_removed_outliers[var] <- df_target # replace with filtered values
}

clean_df = na.omit(df_removed_outliers[,1:10])
full.mdl.filtered <- glm(resp~., data = clean_df[,1:9], # cleaning na rows
                         family = binomial(link = "logit"))
summary(full.mdl.filtered)

preds_filtered <- as.numeric(predict(full.mdl.filtered, type = c("response")) > 0.5)
confusionMatrix(as.factor(preds_filtered), as.factor(clean_df$Classification))
#+END_SRC




* PhD
** Doctorado en neurociencias UC
*** TODO postulacion
    DEADLINE: <2019-09-30 Mon> SCHEDULED: <2019-09-02 Mon>
    
*** Documentos
**** TODO Formulario de postulacion (segun formato en linea)
    SCHEDULED: <2019-08-19 Mon>
**** TODO Certificado de titulo o grado academico, original o copia legalizada ante notario
     SCHEDULED: <2019-08-05 Mon>
**** TODO Concentracion de notas de pregrado y otros estudios
     SCHEDULED: <2019-08-05 Mon>
Incluyendo estudios de perfeccionamiento y postgrado
**** TODO Dos cartas de recomendacion confidenciales (segun formato en linea)
     SCHEDULED: <2019-08-05 Mon>
Estas debe ser enviadas directamente por las personas que
recomiendan. Es deseable que las cartas provengan de personas con
grado academico de Doctor
**** TODO Carta de intencion
     SCHEDULED: <2019-07-10 Wed>
Presentar una declaracion de propositos, que incluya la formulacion de
un topico de interes relevante para su estudio durante el programa y
la dedicacion comprometidos para el programa. El postulante debe ser
tan especifico como sea posible en cuanto a sus intereses y objetivos
de investigacion a corto y largo plazo, en una extension no mas de
tres paginas a espacio y medio.
***** Declaración de propósitos
****** Motivación I
El cómo buscamos objetos, información, recompensas, alimentos, etc. Ha
sido lo que ha inspirado en mayor medida mi interés en la
neurociencia. A lo largo de mi vida he sentido profunda intriga en
cómo los humanos buscan en el espacio de posibilidades, para tomar una
decisión, para evocar una memoria en partícular o bien simplemente
para organizar cualquier compartamiento relativamente complejo, esto
es, sin tener de antemano consideradas todas las posibilidades y aún
pese a eso tener un buen desempeño en multiples tareas. Investigar
sobre los mecanismo subyacentes a ese fenómeno ha sido increíblemente
enriquecedor debido a la fuerte interdisciplinareidad que subyace al
campo. Esto me ha llevado a generar un profunda interés en seguir
desarrollando carrera en neurociencia, ya que, creo, el lograr
entender ese aparentemente simple mecainsmo de decisión en condiciones
de información incompleta, puede eventualmente, ser de gran utilidad
para la comprensión tanto de procesos de memoria y aprendizaje cómo de
ciertas patologías. Con la oportunidad del programa de Doctorado
espero contribuir a la investigación del aprendizaje y memoria.
****** Background
Cómo parte de mi formación en el programa de Magíster en Neurociencias
Social de la Universidad Diego Portales, investigue, cómo parte de un
artículo de revisión, las raíces evolutivas de la búsqueda semántica
(recuperación de memorias en tareas de evocación). Una de las
principales conclusiones fue que, aunque solo en grado tentativo,
parace existir un mecanismo compartido entre la búsqueda semántica y
el forrajeo ('foraging', el comportamiento de búsqueda de alimento),
teniendo este último patrones relativamente marcados que se extienden
a lo largo de miles de años, así cómo a través de multiples
especies. La posibilidad de que un mecanismo tan ubiquo, responsable
del comportamiento motil en la búsqueda de alimentos, pueda estar
relacionado por exaptación a un proceso fundamental de la memoria. Lo
que abre una posibilidad de establecer un mapeo evolutivo al menos a
este proceso de memoria.
****** Motivación II
Deseoso de aprender más sobre este posible vínculo entre forrajeo y
memoria, me adentre en las principales áreas aledañas de conocimiento,
tales como ecología, aprendizaje por reforzamiento ('reinforcement
learning') y modelos computacionales. Por la alta carga de modelos
estadísticos de las áreas mencionadas, me apunté para un programa de
diplomado en ciencia de datos de la Universidad Católica de
Chile. Además de este programa he realizado aprendizaje autonomo en
cursos en línea, con el fin de contar con todas las herramientas
técnicas que son demandadas para el área.
****** Motivación III (investigación) 
Adicional los programas mencionados anteriormente, desde julio del año
2018, me encuentro participando como investigador en un proyecto
FONDECYT conjunto entre la escuela de Arquitectura y Psicología de la
Univerdad Diego Portales. El tema central de esta investigación es el
estudio de la percepción de peatones en diferentes ámbientes
urbanos. Si bien, el tema no está relacionado directamente con el área
de interés, mi rol ha consistido en utilización de técnicas de visión
de máquina ('machine vision') y procesamiento de datos tanto para
'Eye-tracker' cómo para análisis de frecuencia de objetos. Lo
anterior, adicionado, a el aprendizaje de diversos lenguajes de
programación (MATLAB, Python, R, Bash) me ha permitido desarrollar
herramientas que son útiles en la investigación en general cómo
especificamente para el área de mi interés.
****** Formulación tópico de interés
******* Introducción
******* Estado del arte
******* Trabajo realizado
****** Objetivos a corto plazo
****** Objetivos a largo plazo
****** Compromiso
**** TODO Fotocopia de la cedula de identidad o pasaporte
     SCHEDULED: <2019-07-10 Wed>
**** TODO Solicitud de ingreso a la universidad (segun formato)
     SCHEDULED: <2019-07-10 Wed>
** Doctorado en ingenieria de sistemas complejos
*** TODO postulacion
    SCHEDULED: <2019-09-01 Sun> DEADLINE: <2019-11-20 Wed>
*** Documentos
*SEND ALL BACKGROUND INFORMATION TO ANDREA PINTO AT EMAIL: postgrados.fic@uai.cl*
**** TODO Enter information at website [[https://ingenieria.uai.cl/phd/disc/admission/][application]]
     SCHEDULED: <2019-08-05 Mon>
**** TODO Résumé
     SCHEDULED: <2019-07-12 Fri>
**** TODO Photocopy of chilean ID
     SCHEDULED: <2019-07-10 Wed>
**** TODO Statement of interest
Format is open, to be determined by the applicant
**** TODO Letters of recommendation
     SCHEDULED: <2019-08-05 Mon>
At least two letters of recomendation from academic or direct
supervisors
**** TODO Certificates of degrees earned
     SCHEDULED: <2019-08-05 Mon>
**** TODO Grade point average
     SCHEDULED: <2019-08-05 Mon>
With ranking or relative position within the undergraduate and
graduate programs you have completed with their respective grade
scales
**** TODO English proficiency (TOEFL)
     SCHEDULED: <2019-08-05 Mon>
**** TODO Academic interview with the program director
** Doctorado en ciencias de la complejidad social (TBD)
[[https://dccs.udd.cl/es/][PHD PROGRAM]]
** Becas
   SCHEDULED: <2019-07-12 Fri>
*REVISAR*
