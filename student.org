* Data science program
#+STARTUP: latexpreview
** 09/07/2019, Cross validations, decision trees
*** Cross-validation
- Allows us to compare different machine learning methods and get a
sense of how well they will work in practice
- Typically this method split the data base into 75%~ for training and
25%~ for testing
- A similar principle is followed by 'leave one out cross validation',
where the model is trained in all but one sample, and that sample is
used to test model accuracy
- k-fold validation is the generalization, where data is divided in k
blocks
#+BEGIN_SRC R :results output 
              # Load pkg
  pkg <- c("tidyverse", "caret")
  lapply(pkg, library, character.only = TRUE)

              # load sample data
  data("swiss")

              # data-split
  set.seed(123)
  training.samples <- swiss$Fertility %>%
  createDataPartition(p = 0.8, list = FALSE) # 80/20 split
  train.data <- swiss[training.samples, ]
  test.data <- swiss[-training.samples, ]

              # build the model
  model <- lm(Fertility ~ ., data = train.data)

              # make predictions and compute acc
  predictions <- model %>% predict(test.data)
  data.frame(R2 = R2(predictions, test.data$Fertility),
    RMSE = RMSE(predictions, test.data$Fertility),
    MAE = MAE(predictions, test.data$Fertility))

              # repeated k-fold cross validation

        number = 5, repeats = 10)
  model.2 <- train(Fertility ~ .,
      data = swiss,
      method = "lm",
      trControl = t.control)
  predictions_2 <- model %>% predict(swiss)
  data.frame(R2 = R2(predictions_2, swiss$Fertility),
    RMSE = RMSE(predictions_2, swiss$Fertility),
    MAE = MAE(predictions_2, swiss$Fertility))
#+END_SRC
*** Confusion matrix
- The confusion matrix can be used to compute various summaries for
classification models
- Rows correspond to what it was predicted
- Columns correspond to the known truth
- Sensitivity:
\begin{equation}
\frac{T.positives}{T.positives + F.negatives}
\end{equation}

- Specificity:
\begin{equation}
\frac{T.negatives}{T.negatives + F.positives}
\end{equation}

#+BEGIN_SRC R :results output
				    # Load pkg
pkg <- c("tidyverse", "caret")
lapply(pkg, library, character.only = TRUE)

				    # Using dummy data
x <- factor(ceiling(runif(1000)-0.20)) # predictions
y <- factor(ceiling(runif(1000)-0.25)) # references
table(x, y)

                                        # confusion matrix
confusionMatrix(x, y, positive = "1")
#+END_SRC
*** Decision tree
R code to generate a decision tree
- First a tree is generated
- Later on is pruned
- In this example the difference is not very notorious
#+BEGIN_SRC R :results output :session rs
  library(rpart)
  library(tree)
  setwd("/home/nicoluarte/Downloads")
  df = read.table('cleveland.txt', h=T)
  head(df)
  summary(df)

                                          # Using rpart

  ## no data standarization needed for trees

  set.seed(123)

  ## creating the first tree

  tree.1 = rpart(df[,14] ~ .,
                 data=df[,1:13],
                 cp=0.0001,parms=list(split="information"),
                 method="class")

  ##split information means entropy/ split could be gini - cp is the complexity parameter.

  par(xpd=TRUE)
  plot(tree.1, uniform=T);
  text(tree.1, all=T, pretty=T, fancy=T, use.n=T, fwidth=0.3, fheight=0.3)

  ##pretty is for compatibility with tree package. Considers the minimum length for abbreviation of character or factor variables (4 L).

  ## Pruning

  plotcp(tree.1) ## Cross-validation results

  printcp(tree.1)

  tree.1$cptable[which.min(tree.1$cptable[,"xerror"]),"CP"]

  tree.2<-prune(tree.1,cp=0.01102941)

  par(xpd=TRUE)
  plot(tree.2, uniform=T)
  text(tree.2, all=T, pretty=T, fancy=T, use.n=T, fwidth=0.3, fheight=0.3)

  plotcp(tree.2) ## Cross-validation results

  printcp(tree.2)

#+END_SRC

#+RESULTS:
#+begin_example
  age gender     cp trestbps chol  fbs restecg thatach exang oldpeak slope ca
1  63   male angina      145  233 true     hyp     150   fal     2.3  down  0
2  67   male asympt      160  286  fal     hyp     108  true     1.5  flat  3
3  67   male asympt      120  229  fal     hyp     129  true     2.6  flat  2
4  37   male notang      130  250  fal    norm     187   fal     3.5  down  0
5  41    fem abnang      130  204  fal     hyp     172   fal     1.4    up  0
6  56   male abnang      120  236  fal    norm     178   fal     0.8    up  0
  thal diag Col15
1  fix buff     H
2 norm sick    S2
3  rev sick    S1
4 norm buff     H
5 norm buff     H
6 norm buff     H
      age         gender         cp         trestbps          chol      
 Min.   :29.00   fem : 95   abnang: 49   Min.   : 94.0   Min.   :126.0  
 1st Qu.:48.00   male:201   angina: 23   1st Qu.:120.0   1st Qu.:211.0  
 Median :56.00              asympt:141   Median :130.0   Median :242.5  
 Mean   :54.52              notang: 83   Mean   :131.6   Mean   :247.2  
 3rd Qu.:61.00                           3rd Qu.:140.0   3rd Qu.:275.2  
 Max.   :77.00                           Max.   :200.0   Max.   :564.0  
   fbs      restecg       thatach       exang        oldpeak       slope    
 fal :253   abn :  4   Min.   : 71.0   fal :199   Min.   :0.000   down: 21  
 true: 43   hyp :145   1st Qu.:133.0   true: 97   1st Qu.:0.000   flat:137  
            norm:147   Median :152.5              Median :0.800   up  :138  
                       Mean   :149.6              Mean   :1.059             
                       3rd Qu.:166.0              3rd Qu.:1.650             
                       Max.   :202.0              Max.   :6.200             
       ca           thal       diag     Col15   
 Min.   :0.0000   fix : 18   buff:160   H :160  
 1st Qu.:0.0000   norm:163   sick:136   S1: 53  
 Median :0.0000   rev :115              S2: 35  
 Mean   :0.6791                         S3: 35  
 3rd Qu.:1.0000                         S4: 13  
 Max.   :3.0000

Classification tree:
rpart(formula = df[, 14] ~ ., data = df[, 1:13], method = "class", 
    parms = list(split = "information"), cp = 1e-04)

Variables actually used in tree construction:
[1] age     ca      cp      exang   oldpeak thal    thatach

Root node error: 136/296 = 0.45946

n= 296 

         CP nsplit rel error  xerror     xstd
1 0.4926471      0   1.00000 1.00000 0.063044
2 0.0514706      1   0.50735 0.63971 0.057630
3 0.0404412      3   0.40441 0.52941 0.054276
4 0.0220588      5   0.32353 0.44853 0.051170
5 0.0110294      6   0.30147 0.44118 0.050857
6 0.0036765      8   0.27941 0.44118 0.050857
7 0.0001000     10   0.27206 0.46324 0.051780
[1] 0.01102941

Classification tree:
rpart(formula = df[, 14] ~ ., data = df[, 1:13], method = "class", 
    parms = list(split = "information"), cp = 1e-04)

Variables actually used in tree construction:
[1] age   ca    cp    exang thal 

Root node error: 136/296 = 0.45946

n= 296 

        CP nsplit rel error  xerror     xstd
1 0.492647      0   1.00000 1.00000 0.063044
2 0.051471      1   0.50735 0.63971 0.057630
3 0.040441      3   0.40441 0.52941 0.054276
4 0.022059      5   0.32353 0.44853 0.051170
5 0.011029      6   0.30147 0.44118 0.050857
6 0.011029      8   0.27941 0.44118 0.050857
#+end_example
** 11/07/2019, GLM
*** Linear regression
**** Conditions
***** Multi-Collinearity
Multi-collinearity is a phenomenon in which one predictor variable in a multiple
regression model can be linearly predicted from the other with a substantial
degree of accuracy. In this situation the coefficient estimates of the multiple
regression may change erratically in response to small changes in the model or
the data.

Collinearity: is a linear association between two explanatory variables. Two
variables are perfectly collinear if there is an exact linear relationship
between them. For example, X_1 and X_2 are perfectly collinear if there exists
parameters lambda_0 and lambda_1 such that, for all observations i, we have:

\begin{equation}
X_{2i} = \lambda_0 + \lambda_1 X_{1i}
\end{equation}

Multi-collinearity: refers to the situation in which two or more explanatory
variables in a multiple regression model are highly linearly related. We have
perfect multi-collinearity if, for example as in the equation above, the
correlation between two independent variables is equal to 1 or -1. In practice,
we rarely face perfect multi-collinearity in a data set. More commonly, the
issue of multi-collinearity arises when there is an approximate linear
relationship among two or more independent variables.

\begin{equation}
\lambda _{0}+\lambda _{1}X_{{1i}}+\lambda _{2}X_{{2i}}+\cdots +\lambda _{k}X_{{ki}}=0
\end{equation}

#+BEGIN_SRC R :results output :session linear-regression
                                          # get sample dataset

  data(swiss)

                                          # visualize correlation matrix
  library(corrplot)
  cor_mat <- cor(swiss[,c(2:6)], method = c("pearson"))
  corrplot(cor_mat, type = "upper", method = "square")
  print(as.data.frame(cor_mat))

                                          # calculate VIF
  ## variance inflation factor: the VIF's of the linear regression indicate
  ## the degree that the variances in the regression estimates are increased
  ## due to multicollinearity.
  ## VIF values higher that 10 indicate that there's a problem

  ## first we need to fit a linear regression to our data
  mdl.lnr.1 <- lm(Fertility ~ ., data = swiss)
  summary(mdl.lnr.1)

  ## calculate VIF
  library(car)
  vif(mdl.lnr.1)

  ## there's no variable that exceeds a VIF of 10.
  ## however we are going to center the data just to apply some functions

  ## centering explanatory variables
  df.cntrd <- data.frame(Fertility = swiss[, 1],
                         sapply(swiss[,2:6], function(x) x - mean(x)))

  ## scaling using r function
  df.scld <- data.frame(Fertility = swiss[, 1],
                        scale(swiss[,2:6]))

  ## re-make linear model and calculate VIF
  mdl.lnr.2 <- lm(Fertility ~ ., data = df.cntrd)
  summary(mdl.lnr.2)
  vif(mdl.lnr.2)

  ## linear model and VIF for scaled data
  mdl.lnr.3 <- lm(Fertility ~ ., data = df.scld)
  summary(mdl.lnr.3)
  vif(mdl.lnr.3)

#+END_SRC
***** Variability of independent variable should be positive
Fairly straight-forward, it must exist at least some variability in the data set
independent variables

#+BEGIN_SRC R :results output :session linear-regression
  ## LMAO
  print(sapply(swiss, function(x) var(x)))
#+END_SRC
***** Independent variables and residuals are uncorrelated
Not very important
***** Independent variables are expected to be normally distributed
#+BEGIN_SRC R :results output :session linear-regression
  ## we're going to plot one histogram per independent variable
  library(tidyr)
  library(ggplot2)
  ## we do this in order to get data in long form
  swiss %>% gather() %>% head()

  ## plot!
  ggplot(gather(swiss), aes(value)) +
    geom_histogram(bins = 10, aes(y=..density..), alpha=0.5,
                   position = "identity") +
    facet_wrap(~key, scales = 'free_x') +
    geom_density(alpha=0.2)

  ## using shapiro-wilks to test normality
  ## H_0 = data is normally distributed
  print(sapply(swiss, function(x) shapiro.test(x)))

  ## check for skewness for data correction
  library(moments)
  print(skewness(swiss))

  ## transform data based on skewness
  ## Education is positively skewed
  df_trns <- data.frame(swiss)
  df_trns[,"Education"] <- log(df_trns[,"Education"])
  ## Catholic is positively skewed
  df_trns[,"Catholic"] <- log(df_trns[,"Catholic"])
  ## Re-check for normality
  print(sapply(df_trns, function(x) shapiro.test(x)))

  ## plot to see the difference
  ggplot(gather(df_trns), aes(value)) +
    geom_histogram(bins = 10, aes(y=..density..), alpha=0.5,
                   position = "identity") +
    facet_wrap(~key, scales = 'free_x') +
    geom_density(alpha=0.2)

  ## we could also check distribution of dependent variable
  hist(swiss[,"Fertility"])


#+END_SRC

**** Multiple linear regression
*** GLM
- A function is applied to the X's, or a transformation. These are
called link functions
- Started modelling probability of event (Bernoulli, logit of 'p')
- Logit predicts 'odds' <- logistic regression
- In GLM response variable can have any distribution
Using coimbra breast cancer dataset
*CHALLENGE REMOVE OUTLIERS AND RE-RUN ANALYSIS*
- There's not much difference in removing outliers
#+BEGIN_SRC R :results output
setwd("/home/nicoluarte/Downloads/")
df <- read.csv("dataR2.csv")
head(df)
resp <- c(df$Classification - 1)

                                        # compare cancer/no-cancer by age
library(doBy)
summaryBy(Age~resp, data = df, FUN = c(mean, median))

                                        # compare by glucose
summaryBy(Glucose~resp, data = df, FUN = c(mean, median))
summaryBy(Insulin~resp, data = df, FUN = c(mean, median))

                                        # t-test to check means
t.test(Age~resp, data = df)
t.test(Glucose~resp, data = df)
t.test(BMI~resp, data = df)
                                        # re-code variable
df$Glucose2 <- ifelse(df$Glucose<=100, "<=100", ">100")
chisq.test(table(df$Glucose2, resp))

                                        # OR calculation
or <- (44*27)/(8*37)

                                        # Plots
plot(resp~Glucose, data = df)
abline(lm(resp~Glucose, data = df), col = "red")

                                        # Logistic regression
mdl.0 = glm(resp~Glucose2, data = df, family = binomial(link = "logit"))
mdl.1 = glm(resp~Glucose, data = df, family = binomial(link = "logit"))

                                        # Calculating odds
exp(1.3897) # 4 times greater the chance to get cancer

                                        # Box plot
boxplot(Glucose~resp, data = df)

                                        # Logistic regression insulin2
df$Insulin2 <- ifelse(df$Insulin<=8, "<=8", ">8")
mdl.2 = glm(resp~Insulin2, data = df, family = binomial(link = "logit"))
summary(mdl.2)

mdl.3 <- glm(resp~Glucose, data = df, family = binomial(link = "logit"))
summary(mdl.3)

                                        # Calculating odds for different values
exp(0.07867*5)
plot(0.07867*1:10)

                                        # multi-variable model
mdl.4 <- glm(resp~Glucose + Insulin, data = df, family = binomial(link = "logit"))
summary(mdl.4)
plot(Glucose~Insulin, data = df)

                                        # Adding control variables
mdl.5 <- glm(resp~Age + BMI + Glucose, data = df, family = binomial(link = "logit"))
summary(mdl.5)
exp(mdl.5$coefficients["Glucose"]) # each glucose point has a 10%~ effect on chance of cancer

                                        # ROC curve
library(pROC)
prob <- predict(mdl.5, type = c("response"))
roc(resp~prob, data = df, plot = T)

                                        # Use all variables
full.mdl <- glm(resp~., data = df[,1:9], family = binomial(link = "logit"))
summary(full.mdl)

                                        # Prune!

new_vars = setdiff(names(df[,1:9]),c(names(full.mdl$coefficients)[which.max(full.mdl$coefficients)]))
prune.mdl <- glm(resp~., data = df[new_vars], family = binomial(link = "logit"))
summary(prune.mdl)

                                        # Calculate confusion matrix
library(caret)
preds <- as.numeric(predict(full.mdl, type = c("response")) > 0.5)
confusionMatrix(as.factor(preds), as.factor(resp))

                                        # Generate same model without outliers
df_removed_outliers = df[NA,] # same df but NA rows
df_removed_outliers["Classification"] <- df["Classification"]
df_removed_outliers$Classification <- df_removed_outliers$Classification - 1 # to get same levels
for (var in names(df[,1:9]))
{
  outliers <- boxplot.stats(df[,var]) # get the outlier of this variable
  df_target <- df[,var] # get the vector of values of this variable
  idx <- which(df_target %in% outliers$out) # get the idx of outliers in vector
  df_target[idx] <- NA # put NAN's in there
  df_removed_outliers[var] <- df_target # replace with filtered values
}

clean_df = na.omit(df_removed_outliers[,1:10])
full.mdl.filtered <- glm(resp~., data = clean_df[,1:9], # cleaning na rows
                         family = binomial(link = "logit"))
summary(full.mdl.filtered)

preds_filtered <- as.numeric(predict(full.mdl.filtered, type = c("response")) > 0.5)
confusionMatrix(as.factor(preds_filtered), as.factor(clean_df$Classification))
#+END_SRC

** 23/07/2019, Bagging, Random forest, Naive Bayes
#+begin_src R :results output
  library(randomForest)
  library(gbm)
  setwd("/home/nicoluarte/Downloads")
  df <- read.table("cleveland.txt", h=T)


  set.seed(123)

  rf.1 <- randomForest(df[,14] ~ ., data = df[,1:13], mtry = 6,
                       importance = T, ntree = 100)
  print(rf.1)

  layout(matrix(c(1,2), nrow=1), width = c(4,1))
  par(mar=c(5,4,4,0))
  plot(rf.1, log="y")
  par(mar=c(5,0,4,2))
  plot(c(0,1), type?"n", axes = F, xlab = "", ylab = "")
  legend("top", colnames(rf.1$err.rate), col=c(1,2,3), cex = 0.8, fill = c(1,2,3))

  rf.pred <- predict(rf.1, df[,1:13], type = "class")

  table(rf.pred, df[,14])

  importance(rf.1, type = 2)

  varImpPlot(rf.1, type = 1)

                                          # create train and test split
  library(caret)
  trainIdx <- createDataPartition(df[,14],
                                  p = 0.8,
                                  list = FALSE,
                                  times = 1)
  df_train <- df[trainIdx, ]
  df_test <- df[-trainIdx, ]

                                          # Train new random forest
  rf.2 <- randomForest(df_train[,14] ~ ., data = df_train[,1:13], mtry = 6,
                       importance = T, ntree = 100)
  print(rf.2)
#+end_src

#+begin_src python :results output
  import pandas as pd
  import numpy as np
  import matplotlib.pyplot as plt
  import seaborn as sns
  from sklearn import tree
  from sklearn.model_selection import train_test_split, cross_val_score
  from sklearn.externals.six import StringIO
  from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier, export_graphviz
  from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, BaggingRegressor
  from sklearn.metrics import mean_squared_error, confusion_matrix, classification_report

  df = pd.read_csv("/home/nicoluarte/Downloads/heart.txt").drop('Unnamed: 0', axis=1).dropna()

  # Generate labels for categorical variables
  df.ChestPain = pd.factorize(df.ChestPain)[0]
  df.Thal = pd.factorize(df.Thal)[0]

  # Define I/O
  x = df.drop('AHD', axis=1)
  y = pd.factorize(df.AHD)[0]

  # Bagging: using all features
  clf = RandomForestClassifier(max_features=13, max_leaf_nodes=6, n_estimators=100)
  clf.fit(x, y)
  clf.score(x, y)

  # Train and test
  x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.5, random_state=0)

  # Bagging: using all features
  clf1 = RandomForestClassifier(max_features=13, max_leaf_nodes=6, n_estimators=100)
  clf1.fit(x_train, y_train)
  print(clf1.score(x_test, y_test))

  # Predicting
  pred = clf1.predict(x_test)

  # Confusion matrix
  cm = pd.DataFrame(confusion_matrix(y_test, pred).T, index=['No', 'Yes'], columns = ['No', 'Yes'])
  cm.index.name = 'Predicted'
  cm.columns.name = 'True'
  print(cm)

  # Importance
  Importance = pd.DataFrame({'Importance':clf1.feature_importances_*100}, index=x.columns)
#+end_src
** 30/07/2019, Naive Bayes, Support-vector machine
*** Naive Bayes
 #+BEGIN_SRC R :results output :session naive_bayes
   setwd("/home/nicoluarte/Downloads/")

                                           # libraries
   library(e1071)
   library(naivebayes)
   library(dummies)
   library(dplyr)

                                           # read data
   df <- data.frame(read.table('heart.txt', h=T, sep=","))
   head(df)
   summary(df)

                                           # delete NA
   df.old <- na.omit(df)
   df.Sex <- dummy(df.old$Sex)
   df.ChestPain <- dummy(df.old$ChestPain)
   df.Thal <- dummy(df.old$Thal)
   df.1 <- data.frame(df.ChestPain, df.Thal, df.Sex,
                      select(df.old, -c(Sex, ChestPain, Thal, X)))

                                           # naive bayes
   sed.seed(123)
   nb.1 <- naiveBayes(as.factor(df.1[,20]) ~ ., data = df.1[,1:19])
   print(nb.1)

                                           # predict
   pred.1 <- predict(nb.1, newdata = df.1[,1:19], type = "class")

                                           # confusion matrix
   tab.1 <- table(df.1[,20], pred.1)

                                           # accuracy
   (tab.1[1,1]+tab.1[2,2]) / sum(tab.1)

                                           # other naive bayes model
   nb.2 <- naive_bayes(as.factor(df.1[,20]) ~ .,
                       data = df.1[,1:19],
                       usekernel = T)
   pred.2 <- predict(nb.2, newdata = df.1[,1:19], type = "class")

                                           # confusion matrix
   tab.2 <- table(df.1[,20], pred.2)

                                           # accuracy
   (tab.2[1,1]+tab.2[2,2]) / sum(tab.2)
 #+END_SRC
*** Support-vector machine
- Find vectors that best divide the data-set in hyper-planes
- Linear classifier
** 01/08/2019, Time series
#+BEGIN_SRC R :results output :session time_series
  data(co2)

                                          # arrange vector as time series
  df.ts <- ts(c(co2), start=c(1959,1), frequency = 12)
  plot(df.ts)

                                          # decompose
  df.decompose <- decompose(df.ts)
  plot(df.decompose)

                                          # auto-correlation
  df.random <- acf(df.decompose$random, na.action = na.pass)

                                          # Load air database
  library(xlsx)
  df.air <- data.frame(read.xlsx("/home/nicoluarte/Downloads/aircanada.xlsx", sheetIndex = 1))

                                          # arrange air as timeseries
  df.air.ts <- ts(df.air[7], start = c(2005, 7), frequency = 12)

                                          # decompose
  df.air.ts.decompose <- decompose(df.air.ts)
  plot(df.air.ts.decompose)

                                          # auto-correlation
  df.air.random <- acf(df.air.ts.decompose$random, na.action = na.pass)

                                          # convert to non-stationary to stationary
  ## co2 example
  ## diff technique
  acf(co2, lag = 120)
  plot(diff(co2))
  acf(diff(co2), lag = 120)
  plot(diff(diff(co2), lag = 120))
  z <- diff(diff(co2), lag = 120)
  par(mfrow = c(1,2))
  acf(z[1:200])
  acf(z[200:400], na.action = na.omit)

  ## with regression
  y <- c(co2)
  x1 <- time(y)
  x2 <- as.factor(rep(1:12, 39))
  mdl.1 <- lm(y ~ x1 + x2)
  plot(y~x1, type = "l")
  lines(mdl.1$fitted.values~c(x1), col = "red")

  ## non-parametric modelling
  library(forecast)
  hw.mdl <- HoltWinters(co2)
  plot(forecast(hw.mdl, 24))

                                          # using air df
  ## complete this

                                          # ARMA
  ## AR(1): x[t] = Phi * x[t-1] + z[t], z[t] white noise
  x <- c()
  n <- 300
  z <- rnorm(n, 0, 1)
  phi <- 0.8
  x[1] <- z[1]
  for (t in 2:n)
  {
    x[t] <- phi*x[t-1]+z[t]
  }

  ## MA(1): x[t] = z[t] + Theta * z[t-1]
  ## ARMA (1, 1): x[t] - Phi * x[t-1] = theta * z[t-1] + z[t]

#+END_SRC

** 06/08/2019, Naive bayes, neural nets
#+BEGIN_SRC R :results output :session neural_net
  ##Libraries

  library(e1071)

  library(ggplot2)

  library(dummies)

  library(dplyr)

  ##Path

  setwd("/home/nicoluarte/Downloads")

  ##Simulated data

  set.seed(123)

  ##Input
  X <- matrix(rnorm(40), 20, 2)
  colnames(X) <- c("X1","X2")

  ##Output
  y <- c(rep(-1,10), rep(1,10))
  X[y == 1, ] <- X[y == 1, ] + 1

  datos <- data.frame(X,y)

  ##Plot
  ggplot(data = datos, aes(x = X1, y = X2, color = as.factor(y)))+geom_point(size = 6) +theme_bw()

  ##Support vector machine

  datos$y <- as.factor(datos$y)##Convertir variable en factor

  svm.1 <- svm(formula = y ~ X1 + X2, data = datos, kernel = "linear",cost = 10, scale = T)##kernel=SVM lineal o no lineal/scale=T implica estandarizar las variables (media 0 y varianza 1)/cost=parametro de regularizacion

  summary(svm.1)##resumen del SVM

  svm.1$index##observaciones que son vectores de soporte

  ##Classification plot

  plot(svm.1,datos)

  ##Cross-validation for C

  svm_cv <-tune("svm", y ~ X1 + X2, data = datos,kernel = 'linear',ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 20, 50, 100,150, 200)))

  summary(svm_cv)

  svm_cv$best.model

  ## Heart Data

  ##Data

  datos.full<-read.table('heart.txt',h=T, sep=",")

  head(datos.full)

  summary(datos.full)

  ##Delete NA

  datos.old<-datos.full

  datos.Sex<-dummy(datos.full$Sex)

  datos.ChestPain<-dummy(datos.old$ChestPain)

  datos.Thal<-dummy(datos.old$Thal)

  ##New frame

  datos<-data.frame(datos.ChestPain,datos.Thal,datos.Sex,select(datos.old,-c(Sex,ChestPain,Thal, X)))

  head(datos)

  summary(datos)

  ##Scaled variables

  dim(datos)

  datos.std<-datos

  datos.std[,1:19]<-scale(datos[,1:19])

  ##Support Vector Machine

  set.seed(123)

  svm.2<-svm(AHD ~ . ,data=datos.std,kernel="linear",scale=T,cost=0.01)

  print(svm.2)

  ##Prediction

  pred<-predict(svm.2,newdata = datos.std)

  caret::confusionMatrix(pred, datos.std$AHD[1:299])

  ##Classification plot

  plot(svm.2,datos,RestBP ~ MaxHR,slice=list(ChestPainasymptomatic=3,ChestPainnonanginal=4,ChestPainnontypical=5,ChestPaintypical=6,Thalfixed=7,Thalnormal=8,Thalreversable=9,Sex0=10,Sex1=11,Age=12,Chol=13,Fbs=14,RestECG=15,ExAng=16,Oldpeak=17,Slope=18,Ca=19))

  ##Cross-validation for cost

  svm_cv <-tune("svm", AHD ~ . , data = datos.std,kernel = 'linear',ranges = list(cost = c(0.001, 0.01, 0.1, 1, 10)),scale=T)

  summary(svm_cv)


  best<-svm_cv$best.model

  ##Prediction

  pred<-predict(best,newdata = datos.std[1:299,])

  tabla<-table(prediccion = pred, valor_real = datos.std$AHD)## Confusion matrix

  caret::confusionMatrix(pred, datos.std$AHD[1:296])

  ###Using train and test

  set.seed(123)

  p.train<-0.8

  n<-nrow(datos.std)
  trainIndex<-sample(1:n,size=round(p.train*n),replace=F)

  train.s<-data.frame(datos.std[trainIndex,])
  test.s<-data.frame(datos.std[-trainIndex,])

  ##Cross-validation for C

  svm_cv <-tune("svm", AHD ~ . , data = train.s,kernel = 'linear',ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 20, 50, 100,150, 200)))

  summary(svm_cv)

  best<-svm_cv$best.model

  ##Prediction

  pred<-predict(best,test.s[,-20])

  tabla<-table(prediccion = pred, valor_real = test.s$AHD)## Confusion matrix

  (tabla[1,1]+tabla[2,2])/sum(tabla)

#+END_SRC
** 08/08/2019, SARIMAX
#+BEGIN_SRC R :results output :session SARIMAX
  ###############
  ## libraries ##
  ###############
  library(forecast)

  ################
  ## ARMA model ##
  ################

                                          #AR(1)
  set.seed(123)
  mdl.1 <- arima.sim(n = 1000, model = list(order = c(1,0,0), ar = 0.7))
  ts.plot(mdl.1)
  acf(mdl.1)

                                          #MA(1)
  set.seed(111)
  mdl.2 <- arima.sim(n = 1000, model = list(order = c(0,0,1), ma = 0.8))
  ts.plot(mdl.1)
  acf(mdl.1)

                                          # ARMA
  mdl.3 <- arima.sim(n = 1000, model = list(order = c(1,0,1),
                                            ma = 0.8,
                                            ar = 0.7))

  ## plots
  par(mfrow = c(2,3))
  ts.plot(mdl.1)
  acf(mdl.1) # indicates ma model order(q)
  pacf(mdl.1) # indicate ar model order(p)
  ts.plot(mdl.2)
  acf(mdl.2)
  pacf(mdl.2)

                                          # AUTO ARIMA
  mdl.1.aa <- auto.arima(mdl.1)
  mdl.2.aa <- auto.arima(mdl.2)
  mdl.3.aa <- auto.arima(mdl.3)

                                          # differentiation
  par(mfrow = c(2,2))
  plot(co2)
  plot(diff(co2)) # removes trend
  plot(diff(diff(co2, lag = 12))) # removes stationationality
  acf(diff(diff(co2, lag = 12))) # removes stationationality

                                          # AUTO ARIMA
  mdl.aa.2 <- auto.arima(co2, d=1, D=1)

  mdl.aa.3 <- arima(co2, order = c(1,1,1),
                    seasonal = list(order = c(1,1,2),
                                    period = 12),
                    xreg = NULL, include.mean = TRUE)

  mdl.aa.4 <- arima(co2, order = c(1,1,1),
                    seasonal = list(order = c(1,1,2),
                                    period = 12),
                    xreg = NULL, include.mean = TRUE,
                    fixed = c(NA, NA,NA,0,NA))

  ts.diag(mdl.aa.4$residuals)
  plot(forecast(mdl.aa.4, 360))

  ## prediction quality
  mean(abs(mdl.aa.4$residuals)/co2)*100
  ## compare with holt winter
  hw <- HoltWinters(co2)
  err <- hw$fitted[,1] - hw$x
  mean(abs(err)/co2)*100
#+END_SRC
** 13/08/2019, Supervised algorithms
#+BEGIN_SRC R :results output :session supervised
  #########
  ## KNN ##
  #########

  ## libs
  pkg <- c("class", "cluster", "dummies", "dplyr", "caret")
  invisible(lapply(pkg, library, character.only = TRUE))
  setwd("/home/nicoluarte/Downloads")

  ## load data
  df <- read.table("heart.txt", sep = ",", header = TRUE)
  df <- df[, -1]
  head(df)

  ## clean data
  df.old <- na.omit(df)
  df.ChestPain <- dummy(df.old$ChestPain)
  df.Thal <- dummy(df.old$Thal)
  df.Sex <- dummy(df.old$Sex)
  df.clean <- data.frame(df.ChestPain,
                         df.Thal,
                         df.Sex,
                         select(df.old, -c(Sex, ChestPain, Thal)))
  ## scale
  dim(df.clean)
  df.std <- df.clean
  df.std[,1:19] <- scale(df.clean[,1:19])

  ## cross-validation
  set.seed(123)
  control <- trainControl(method = "repeatedcv",
                          number = 10,
                          repeats = 3,
                          savePredictions = TRUE)
  knn.mdl.0 <- train(AHD ~ .,
                     data = df.std,
                     method = "knn",
                     trControl = control,
                     tuneGrid = expand.grid(k = 1:297))
  confusionMatrix(knn.mdl.0)

  ########################
  ## K-means clustering ##
  ########################
  kmeans.mdl <- kmeans(x = df.std[, 1:19], centers = 2, nstart = 20)

  ##################################
  ## Principal component analysis ##
  ##################################
#+END_SRC

** Classification models
- Load Boston database
- Generate binary variable out of criminality
#+BEGIN_SRC R :results output :session cls_mdl :exports both
  library(MASS)
  library(dplyr)
  ## load database
  df <- Boston

  ## transform crim to binary
  df$crim <- ifelse(df$crim >= mean(df$crim), 1, 0)
  print(head(df))
#+END_SRC

#+RESULTS:
:   crim zn indus chas   nox    rm  age    dis rad tax ptratio  black lstat medv
: 1    0 18  2.31    0 0.538 6.575 65.2 4.0900   1 296    15.3 396.90  4.98 24.0
: 2    0  0  7.07    0 0.469 6.421 78.9 4.9671   2 242    17.8 396.90  9.14 21.6
: 3    0  0  7.07    0 0.469 7.185 61.1 4.9671   2 242    17.8 392.83  4.03 34.7
: 4    0  0  2.18    0 0.458 6.998 45.8 6.0622   3 222    18.7 394.63  2.94 33.4
: 5    0  0  2.18    0 0.458 7.147 54.2 6.0622   3 222    18.7 396.90  5.33 36.2
: 6    0  0  2.18    0 0.458 6.430 58.7 6.0622   3 222    18.7 394.12  5.21 28.7

- Generate train and test partition (0.7, 0.3)
#+BEGIN_SRC R :results output :session cls_mdl :exports both
  ## option 1 self-made
  df_partition <- function(data, train_size)
  {
    set.seed(101)
    # get indices
    sample <- sample.int(n = nrow(data),
                         size = floor(train_size*nrow(data)),
                         replace = F)
    # generate train data
    train <- data[sample, ]
    # and test data
    test  <- data[-sample, ]
    # list of train and test
    return(list(train, test))
                                          # return it
  }

  ## example
  fun_list <- df_partition(df, 0.7)
  df_train <- fun_list[[1]]
  df_test <- fun_list[[2]]

  ## option 2 using caret
  library(caret)
  ## LGOCV <- leave group out
  ## p <- train percentage
  ## number <- iterations
  ## same as 1 train/test split
  train_control <- trainControl(method = "LGOCV",
                                p = 0.7,
                                number = 1,
                                savePredictions = TRUE)

  ## the first function will be used since it's easier to deal with
  ## native R objects
#+END_SRC

#+RESULTS:

- Train following models
- KNN
- SVM
- Decision tree
- Neural net
#+BEGIN_SRC R :results output :session cls_mdl :exports both
  ##########################
  ## K-nearest-neighbours ##
  ##########################
  ## knn parameter is number neighbours 'k'
  library(class)
  knn_mdl <- knn(as.factor(crim) ~ .,
                 data = df,
                 k = 3)
  ## confusion matrix
  tbl <- table(knn_mdl, df_test$crim)
  accuracy <- function(x){sum(diag(x)/(sum(rowSums(x)))) * 100}
  accuracy(tbl)

  ## optimize with grid search
  grid <- seq(from = 1, to = 200, by = 1)
  k_opt <- sapply(grid, function(x) accuracy(table(knn(df_train,
                               df_test,
                               cl = as.factor(df_train$crim),
                               k = x), df_test$crim)))

  ## using caret
  knn_caret <- train(as.factor(crim) ~ .,
                     data = df,
                     method = "knn",
                     trControl = train_control,
                     tuneGrid = expand.grid(k = 1:200))
  ## we can get the optimal parameter back a train using that
  knn_mdl_opt <- knn(df_train,
                 df_test,
                 cl = as.factor(df_train$crim),
                 k = knn_caret$bestTune$k)
  ## confusion matrix optimal model
  tbl_2 <- table(knn_mdl, df_test$crim)
  accuracy(tbl_2)

  ############################
  ## Support vector machine ##
  ############################
  ## here on out I'll be using caret because is way much faster
  ## assumming a linear kernel there's only one parameter to optimize
  ## the cost o C
  ## parallel so it goes faster
  library(doParallel)
  registerDoParallel(cores=8)
  svm_caret <- train(as.factor(crim) ~ .,
                     data = df, method = "svmLinear",
                     tuneGrid = expand.grid(C = 1:100),
                     preProcess = c("scale"),
                     metric = "Accuracy")

  ## we do the same as before
  library(e1017)
  svm_mdl_opt <- svm(as.factor(crim) ~ .,
                     data = df_train,
                     kernel.type="linear",
                     cost = svm_caret$bestTune$C)
  svm_pred <- predict(svm_mdl_opt, newdata = df_test)
  ## confusion matrix optimal model
  tbl_3 <- table(svm_pred, df_test$crim)
  accuracy(tbl_3)

  ###################
  ## Decision tree ##
  ###################
  ## trained in all data
  library(tree)
  tree_boston <- tree(as.factor(crim) ~ .,
                   data = df)
  par(mfrow = c(1,3))
  plot(tree_boston)
  text(tree_boston, pretty = 0)
  ## using train test
  tree_boston_cv = tree(as.factor(crim) ~ .,
                        data = df_train)
  plot(tree_boston_cv)
  text(tree_boston_cv, pretty=0)
  ## check accuracy
  tree_pred = predict(tree_boston_cv, newdata = df_train, type="class")
  tbl_4 <- with(df_train, table(tree_pred, crim))
  accuracy(tbl_4)

  ## prune the tree
  tree_boston_prune <- cv.tree(tree_boston_cv, FUN = prune.misclass)
  plot(tree_boston_prune)

  ####################
  ## neural network ##
  ####################
  library(nnet)
  nnet_boston <- train(as.factor(crim) ~ .,
                       data = df,
                       method = "nnet",
                       trControl = train_control,
                       tuneGrid = expand.grid(size = 1:20, decay = 0.1))
#+END_SRC

#+RESULTS:
#+begin_example
Error in knn(as.factor(crim_bin) ~ ., data = df, k = 3) : 
  unused argument (data = df)
Error in table(knn_mdl, df_test$crim_bin) : 
  all arguments must have the same length
[1] 98.02632
Error in knn(df_train, df_test, cl = as.factor(df_train$crim_bin), k = x) : 
  'train' and 'class' have different lengths
Error in is.factor(x) : object 'crim_bin' not found
Error in knn(df_train, df_test, cl = as.factor(df_train$crim_bin), k = knn_caret$bestTune$k) : 
  'train' and 'class' have different lengths
Error in table(knn_mdl, df_test$crim_bin) : 
  all arguments must have the same length
[1] 98.02632
Error in is.factor(x) : object 'crim_bin' not found
Error in library(e1017) : there is no package called ‘e1017’
Error in is.factor(x) : object 'crim_bin' not found
Error in table(svm_pred, df_test$crim_bin) : 
  all arguments must have the same length
[1] 98.68421
Error in is.factor(x) : object 'crim_bin' not found
#+end_example

- Compare models
#+BEGIN_SRC R :results output :session cls_mdl :exports both
  df_cmp <- Boston
  compare_control <- trainControl(method = "repeatedcv",
                                  number = 10,
                                  repeats = 3,
                                savePredictions = TRUE,
                                classProbs = TRUE)

  library(doParallel)
  cl = makeCluster(8)
  registerDoParallel(cl)

  mdl_nn <- train(make.names(crim) ~ .,
                  data = df_cmp,
                  method = "nnet",
                  trControl = compare_control,
                  tuneGrid = expand.grid(size = 1:20, decay = 0.1),
                  metric = "Kappa")

  dtree <- train(make.names(crim) ~ .,
                 data = df_cmp,
                 method = "rpart",
                 trControl = compare_control,
                 tuneLength = 30,
                 metric = "Kappa")

  k_near <- train(make.names(crim) ~ .,
                  data = df_cmp,
                  method = "rpart",
                  trControl = compare_control,
                  tuneLength = 40,
                  metric = "Kappa")

  stopCluster(cl)
  remove(cl)
  registerDoSEQ()

  results <- resamples(list(NeuralNetwork=mdl_nn,
                            DecisionTree=dtree,
                            KNN=k_near))
  results$values

                                          # summarize the distributions
  summary(results)

                                          # boxplot of results
  bwplot(results)
#+END_SRC

- Export database to python
#+BEGIN_SRC R :results output :session cls_mdl :exports both
  xx <- Boston
  head(xx)
  setwd("/home/nicoluarte/magi")
  write.csv(xx, "boston.csv")
#+END_SRC

*** Python code
#+BEGIN_SRC python :results output :session py_mdl

#+END_SRC

** 27/08/2019, Particle swarm optimization
- citep:kennedyParticleSwarmOptimization1995 
- meta-heuristic, little no assumptions
- cant be optimized, non differentiable
- optimal solution is not guaranteed
- actualization: inertia + personal influence + social influence 
#+BEGIN_SRC R :results output :session swarm
  #################################
  ## particle swarm optimization ##
  #################################

  cost <- function(x)
  {
    return(sqrt(x[1]^2+x[2]^2))
  }

  cost2 <- function(x, y)
  {
    return(sqrt(x^2+y^2))
  }

  x <- seq(-10, 10, len = 1000)
  y <- seq(-10, 10, len = 1000)
  z <- outer(x, y, function(x,y){cost2(x,y)})

  library(GA)
  persp3D(x = x, y = y, z = z)

  ## algorithm
  pso <- function(n.part = 100, iterations = 1000, omega = 0.5,
                  phip = 0.4, phig = 1-phip, lower, upper)
  {
    particles <- matrix(0, nrow = n.part, ncol = 2)
    for(i in 1:n.part){particles[i,] <- runif(2, lower, upper)}
    ss <- particles
    velocity <- matrix(rnorm(2*n.part), nrow = n.part, ncol = 2)

    localbest <- particles
    costlocalbest <- apply(localbest, 1, cost)

    globalbest <- particles[which.min(costlocalbest),]
    costglobalbest <- cost(globalbest)

    for(i in 1:iterations)
    {
      cat("Iteration: ", i, "\n")

      for(j in 1:n.part)
      {
        rp <- runif(2)
        rg <- runif(2)
        velocity[j,] <- omega*velocity[j,] +
          phip*rp*(localbest[j,]-particles[j,]) +
          phig*rg*(globalbest-particles[j,])
        particles[j,] <- particles[j,] + velocity[j,]

        if(cost(particles[j,] < costlocalbest[j]))
        {
          localbest[j,] <- particles[j,]
          costlocalbest[j] <- cost(localbest[j,])
        }
      }
    }

    position <- which.min(costlocalbest)
    if(costlocalbest[position] < costglobalbest)
    {
      globalbest <- localbest[position,]
      costglobalbest <- cost(globalbest)
    }
    return(globalbest)
  }
#+END_SRC

* PhD
** Doctorado en neurociencias UC
*** TODO postulación
    DEADLINE: <2019-09-30 Mon> SCHEDULED: <2019-09-02 Mon>
    
*** Documentos
**** DONE Formulario de postulación (según formato en linea)
    CLOSED: [2019-07-29 Mon 15:28] SCHEDULED: <2019-07-16 Tue>
[[file:formulario-postulacion-doctorado-neurociencia.doc][formulario]]
**** DONE Certificado de titulo o grado académico, original o copia legalizada ante notario
     CLOSED: [2019-07-29 Mon 15:28] SCHEDULED: <2019-07-16 Tue>
[[file:copiasimpletitulo.jpg][CopiaTitulos]]
**** DONE Concentración de notas de pre-grado y otros estudios + ranking de egreso promoción
     CLOSED: [2019-07-29 Mon 15:32] SCHEDULED: <2019-07-16 Tue>
Incluyendo estudios de perfeccionamiento y postgrado
[[/home/nicoluarte/Downloads/concentracion_notas.pdf][concentracion_notas]]
**** TODO Dos cartas de recomendación confidenciales (según formato en linea)
     SCHEDULED: <2019-07-22 Mon>
Estas debe ser enviadas directamente por las personas que
recomiendan. Es deseable que las cartas provengan de personas con
grado académico de Doctor
**** DONE Carta de intención
     CLOSED: [2019-07-29 Mon 15:27] SCHEDULED: <2019-07-17 Wed>
Presentar una declaración de propósitos, que incluya la formulación de
un tópico de interés relevante para su estudio durante el programa y
la dedicación comprometidos para el programa. El postulante debe ser
tan especifico como sea posible en cuanto a sus intereses y objetivos
de investigación a corto y largo plazo, en una extensión no mas de
tres paginas a espacio y medio.
***** Declaración de propósitos
#+AUTHOR: Luis Nicolás Luarte Rodríguez
#+OPTIONS: toc:nil date:nil
#+LATEX_HEADER: \usepackage{setspace}
#+LATEX_HEADER: \onehalfspacing
#+LATEX_HEADER: \usepackage[round]{natbib} 
#+LATEX_HEADER: \usepackage[margin=1.2in]{geometry}
#+LATEX_HEADER: \renewcommand\refname{Referencias}
****** Motivación personal para el programa de Doctorado
El cómo buscamos objetos, información, recompensas, alimentos, etc. Ha sido lo
que ha inspirado en mayor medida mi interés en la neurociencia. A lo largo de mi
vida he sentido profunda intriga en cómo los humanos buscan en el espacio de
posibilidades, para tomar una decisión, para evocar una memoria en particular o
bien simplemente para organizar cualquier comportamiento relativamente complejo.
Investigar sobre los mecanismos subyacentes a ese fenómeno ha sido
increíblemente enriquecedor debido al fuerte componente interdisciplinar del
campo. Esto me ha llevado a generar un profundo interés en seguir desarrollando
mi carrera en neurociencia, ya que, creo, el realizar avances en la comprensión
de los mecanismos de decisión y búsqueda en condiciones de información
incompleta, puede, eventualmente, ser de gran utilidad para comprender los
procesos de memoria y aprendizaje, en tanto estos últimos se pueden comprender
como un proceso de búsqueda de contenidos en un espacio mental. Con la
oportunidad del programa de Doctorado espero contribuir a la investigación del
aprendizaje y memoria.

En el programa de Magíster en Neurociencias Social de la Universidad Diego
Portales, investigue, como parte de un artículo de revisión, las raíces
evolutivas de la búsqueda semántica (recuperación de memorias en tareas de
evocación). Una de las principales conclusiones fue que, aunque sólo en grado
tentativo, parece existir un mecanismo compartido entre la búsqueda semántica y
el forrajeo ('foraging', el comportamiento de búsqueda de alimento), teniendo
este último patrones relativamente marcados, que se extienden a lo largo de
miles de años, así como a través de múltiples especies. La posibilidad de que un
mecanismo tan ubicuo, responsable del comportamiento de desplazamiento en la
búsqueda de alimentos, pueda estar relacionado por exaptación a un proceso
fundamental de la memoria abre una posibilidad de establecer un mapeo evolutivo
al menos a este proceso de memoria.

En la investigación mencionada anteriormente, hallé interesantes vínculos
interdisciplinares, por ejemplo, entre neurociencia de la toma de decisiones y
etología citep:mobbsForagingFoundationsDecision2018a, lo que me permitió
postular la toma de decisiones bajo modelos de forrajeo, tales como los
propuestos por el 'Marginal Value Theorem'
citep:charnovOptimalForagingMarginal1976. Además, me adentre en modelos
computacionales de la toma de decisiones
citep:aston-jonesINTEGRATIVETHEORYLOCUS2005a, y finalmente relacionar los
diversos modelos de toma de decisiones secuenciales con la búsqueda de
contenidos semánticos citep:hillsForagingSemanticFields2015. Siendo lo
anteriores tópicos de mi particular interés.

****** Formulación del tópico de interés
Mi tópico de interés principal reside en el estudio de la memoria,
específicamente, la búsqueda semántica, esto es, las estrategias o patrones
utilizados al momento de recuperar un contenido de memoria.

 Las memorias semánticas han sido pensadas, teóricamente, como elementos
pertenecientes a cierto 'espacio' constituido por la similitud en significado
citep:lundProducingHighdimensionalSemantic1996. Así, se ha propuesto una
'distancia' entre los distintos contenidos semánticos
citep:montezRoleSemanticClustering2015, y por lo tanto, la posibilidad de
'navegar' entre dichos contenidos. Considerando lo anterior, es esperable que a
lo largo de la evolución se hayan generado estrategias para acceder, de manera
útil e eficiente, a dichos contenidos. Las estrategias de búsqueda para acceder
a los contenidos semánticos han sido relacionadas a aquellas del forrajeo
citep:ForagingSemanticFields,hillsAnimalForagingEvolution2006,hillsOptimalForagingSemantic2012,
en tanto las estrategias, para ambos casos, deben lidiar con el dilema de
explorar/explotar citep:berger-talExplorationExploitationDilemmaMultidisciplinary2014.

Se ha observado que los 'algoritmos' utilizados en el forrajeo, pueden proveer
de soluciones óptimas para dicho dilema
citep:bartumeusAnimalSearchStrategies2005a, lo cual aplicaría, igualmente, para
estrategias en espacios semánticos citep:montezRoleSemanticClustering2015. De
esta manera se puede observar una conexión entre un mecanismo evolutivamente
antiguo (forrajeo) y la búsqueda semántica. Permitiendo un enfoque evolutivo
comprensivo al estudio de la memoria, correspondiente al tema de mi interés.

El cómo se realiza la búsqueda en espacios semánticos es de
fundamental importancia, ya que es un espacio que está en activa
búsqueda durante la comprensión y producción de lenguaje, entre otras
citep:montezRoleSemanticClustering2015, por lo mismo, el alcance de su
importancia para casi cualquier actividad cognitiva es de gran tamaño,
pudiendo afectar de manera importante el comportamiento ante múltiples y
diferentes tareas.
****** Objetivos a corto plazo
Uno de los principales tópicos de discusión en el área de búsqueda
semántica es la organización y el tipo de la relaciones que conforman
el espacio semántico citep:lundProducingHighdimensionalSemantic1996. Así, Uno de los primeros
objetivos de investigación sería poder generar tareas
experimentales que permitiesen determinar, principalmente, (a) efecto
del contexto en las relaciones entre contenidos semánticos
citep:schillerMemorySpaceUnderstanding2015 y (b) si el tipo de búsqueda es más
verosímil para contenidos encadenados de manera asociativa o categórica
citep:hillsOptimalForagingSemantic2012. 

Cómo segundo objetivo a corto plazo, de manera experimental, modelar el
comportamiento en tareas de evocación de memoria, a modo de sugerir posibles
mecanismos generadores del comportamiento de búsqueda semántica. Los modelos más
relevantes son (a) aquellos basados en reglas
citep:charnovOptimalForagingMarginal1976, (b) modelos aleatorios simples
citep:thompsonWalkingWikipediaScalefree2014 y (c) modelos aleatorios complejos
cómo discutido en citep:benhamouHowManyAnimals2007
****** Objetivos a largo plazo
Los objetivos a corto plazo están relacionados, principalmente, con el
modelamiento de comportamiento en tareas de búsqueda semántica. Por otro lado,
los objetivos a largo plazo buscarían conectar dichos modelos a sus estructuras
cerebrales (u otras) subyacentes. Uno de los candidatos parece ser el 'Locus
coeruleus' citep:kaneIncreasedLocusCoeruleus2017, corteza medial pre-frontal
ventromedial citep:kollingNeuralMechanismsForaging2012, corteza cingulada
anterior citep:shenhavAnteriorCingulateEngagement2014, entre otras. Si bien la
función del 'Locus Coeruleus' es extendida en el sistema de arousal, se observa
evidencia de participación significativa en comportamiento de
exploración-explotación relacionados al forrajeo
citep:aston-jonesAdaptiveGainRole2005, además la relativa facilidad en medición,
en conjunto con otras técnicas como electro-encefalograma, ha permitido
encontrar relación entre este y el forrajeo en situaciones experimentales
citep:slanziCombiningEyeTracking2017. Esto implicaría la combinación de modelos
de comportamiento y registro de medidas fisiológicas, tales como diámetro de
pupila. El tener un modelo de comportamiento, en el marco de explorar/explotar,
permitiría eventualmente, tener una idea de qué estrategia se está ocupando en
cada decisión y por ende evaluar la contribución de las diferentes estructuras
cerebrales a esto.

Adicionalmente, se encuentra dentro de mis objetivos a largo plazo la docencia
en el área de neurociencias, principalmente buscando ser un aporte para la
promoción de la ciencia experimental en Psicología, la cual tradicionalmente
tiene un espacio muy reducido en el currículo de pre-grado, desaprovechando un
campo muy fértil en investigación.
#+Begin_Latex
\pagebreak
#+End_Latex
bibliographystyle:apa
bibliography:ref.bib
****** Conocimiento relevante
  Por la alta carga de modelos estadísticos en las áreas de interés mencionadas,
  me apunté para un programa de diplomado en ciencia de datos de la Universidad
  Católica de Chile, dónde he aprendido fundamentos computacionales usados en
  teorías de toma de decisiones, además de herramientas estadísticas necesarias.
  Adicionalmente, he aprendido teoría de aprendizaje por refuerzo
  ('reinforcement learning') citep:suttonReinforcementLearningDirect1992, lo que
  aporta una base para la comprensión de muchos de los modelos mencionados con
  anterioridad.

  Adicional a los programas mencionados anteriormente, desde julio del año 2018,
  me encuentro participando como investigador en un proyecto FONDECYT conjunto
  entre la escuela de Arquitectura y Psicología de la Universidad Diego
  Portales. El tema central de esta investigación es el estudio de la percepción
  de peatones en diferentes ambientes urbanos. Si bien el tema no está
  relacionado directamente con el área de interés, mi rol ha consistido en
  ajuste de modelos estadísticos, utilización de técnicas de visión de máquina
  ('machine vision') y procesamiento de datos tanto para 'Eye-tracker' cómo para
  análisis de frecuencia de objetos. Lo anterior, adicionado a el aprendizaje de
  diversos lenguajes de programación (MATLAB, Python, R, Bash), me ha permitido
  desarrollar herramientas que son útiles en la investigación en general cómo
  específicamente para el área de mi interés.
**** DONE CV
     CLOSED: [2019-08-15 Thu 16:07] SCHEDULED: <2019-07-17 Wed>
***** Curriculum Vitae
#+OPTIONS: num:nil
\vspace{-13mm}
\begin{center}
18.126.883-2

Santiago, San José de Maipo, Los Acasios 2785

+569 421 626 99
\end{center}
****** Antecedentes académicos
******* Formación de pre-grado:
- 2010 - 2014 Título Psicólogo Universidad Adolfo Ibáñez
******* Formación de post-grado:
- 2014 - 2015 Magíster en Psicología de las Organizaciones Universidad Adolfo Ibáñez
- 2017 - 2019 Nagíster en Neurociencia Social Universidad Diego Portales (En proceso de titulación)
****** Otros antecedentes académicos
- 2018 - Presente Tesista e investigador en proyecto FONDECYT regular 
- 2017 - Presente Ayudante de investigación Laboratorio Neurociencia Cognitiva y Social UDP
- 2019 - Presente Diplomado en Data Science Universidad Católica de Chile
- 2019 - Presente conversatorios regulares en estudio de la pupila UDP
- 2018 Workshop: Brain Connectomics Hackaton I
- 2018 Workshop: Brain Connectomics Hackaton II, Machine Learning
- 2018 Workshop: Brain Connectomics Hackaton III, Modelamiento lineal de la señal de EEG
****** Experiencia laboral
- 2015 - 2017 Consultor en desarrollo organizacional Partners & Success
****** Investigaciones
- Análisis de la experiencia y carga perceptual en primera persona durante una caminata programada por el Gran Santiago (FONDECYT No1170292)
****** Publicaciones
- Rossi, A., Grasso-Cladera, A., Luarte, N., Riillo, A., & Parada, F. J. (2019). The brain/body-in-the-world system is cognitive science’s study object for the twenty-first century / El sistema cerebro/cuerpo-en-el-mundo es el objeto de estudio de la ciencia cognitiva en el siglo XXI. Estudios de Psicología, 40(2), 363–395. https://doi.org/10.1080/02109395.2019.1596704
****** Becas
- Beca excelencia académica 2010 - 2014

**** DONE Fotocopia de la cédula de identidad o pasaporte
     CLOSED: [2019-08-15 Thu 16:24] SCHEDULED: <2019-07-16 Tue>
**** DONE Solicitud de ingreso a la universidad (según formato)
     CLOSED: [2019-08-15 Thu 16:14] SCHEDULED: <2019-07-16 Tue>
** Doctorado en ingeniería de sistemas complejos
*** TODO postulacion
    SCHEDULED: <2019-09-01 Sun> DEADLINE: <2019-11-20 Wed>
*** Documentos
*SEND ALL BACKGROUND INFORMATION TO ANDREA PINTO AT EMAIL: postgrados.fic@uai.cl*
**** TODO Enter information at website [[https://ingenieria.uai.cl/phd/disc/admission/][application]]
     SCHEDULED: <2019-08-05 Mon>
**** TODO Résumé
     SCHEDULED: <2019-07-12 Fri>
**** TODO Photocopy of chilean ID
     SCHEDULED: <2019-07-10 Wed>
**** TODO Statement of interest
Format is open, to be determined by the applicant
**** TODO Letters of recommendation
     SCHEDULED: <2019-08-05 Mon>
At least two letters of recomendation from academic or direct
supervisors
**** TODO Certificates of degrees earned
     SCHEDULED: <2019-08-05 Mon>
**** TODO Grade point average
     SCHEDULED: <2019-08-05 Mon>
With ranking or relative position within the undergraduate and
graduate programs you have completed with their respective grade
scales
**** TODO English proficiency (TOEFL)
     SCHEDULED: <2019-08-05 Mon>
**** TODO Academic interview with the program director
** Doctorado en ciencias de la complejidad social (TBD)
[[https://dccs.udd.cl/es/][PHD PROGRAM]]
** Becas
   SCHEDULED: <2019-07-19 Fri>
Leer e imprimir el manual de becas chile, ver en profundidad bases para la
postulación de becas UC y UAI
** Correos
Diego Cosmelli: dcosmelli@uc.cl

Estimado profesor Diego Cosmelli,

Actualmente soy estudiante del programa de Magíster en Neurociencia Social de la
Universidad Diego Portales. Me encuentro en proceso de finalización del
Magíster, y he encontrado gran interés en el programa de Doctorado en
Neurociencias de su Universidad. 

Cómo parte de mi investigación de literatura tuve la oportunidad de leer su
artículo 'Modeling Search Behaviors during the Acquisition of Expertise in a
Sequential Decision-Making Task'. Específicamente el modelamiento de toma de
decisiones secuenciales me provocó gran interés, ya que el tema principal de mi
proyecto de tesis se centra en la raíces evolutivas de la toma de decisiones
secuenciales y los mecanismos neuronales subyacentes. He estado en búsqueda de
programas de Doctorado dónde pueda continuar el trabajo de investigación en esta
área. Mi proyecto (tentativo) se centra en el modelamiento de toma de decisiones
secuenciales tanto a nivel de comportamiento de desplazamiento en la búsqueda
(foraging) cómo a nivel de búsqueda semántica en espacios cognitivos.
Adicionalmente, recae mi interés en la formación de conciencia a través de los
procesos mencionados anteriormente en el espíritu de 'From foraging to
autonoetic consciousness: The primal self as a consequence of embodied
prospective foraging'.

Esperando no causar mayor molestia, quisiera saber si existe la posibilidad de
saber, si actualmente, está recibiendo estudiantes en calidad de
tutor/supervisor para el programa de Doctorado. Si es así, ¿estaría dispuesto a
seguir esta conversación por el medio que más guste?, sea por correo, teléfono o
una visita al campus/oficina. He investigado el sitio del programa en detalle, y
creo existe buen ajuste con mi intereses de investigación, principalmente por el
fuerte foco interdisciplinar (inherente al objeto de estudio de mi interés) y
por la posibilidad de expandir un línea de investigación relativamente novedosa.

Apreciando cualquier tiempo que me pudiese destinar, lo agradezco de antemano y
quedo atento a su respuesta.

Saludos cordiales,

Nicolás Luarte

* Projects
** FONDECYT
*** Initial inspection:
 #+BEGIN_SRC R :results output :session peatones
                                           # load packages
   pkg <- c("dplyr", "ggplot2", "tidyverse", "corrplot", "Hmisc", "psycho",
            "broom", "gvlma", "pROC", "caret", "MASS", "effects", "car",
            "ResourceSelection", "neuralnet", "lmtest")
   invisible(lapply(pkg, library, character.only = TRUE))

                                           # load database
   setwd("/home/nicoluarte/Downloads")
   df <- data.frame(read.csv("data_fondecyt.csv"))
   head(df)

   ## metricas con el valence ~ socioeconimic + socioeconimic:noise
   ## permutacion valence ~ Socioeconomic

                                           # models
   ## null model
   mdl.null <- glm(Valence_num ~ 1, data = df, family = binomial(link = "logit"))
   ## simple model, only noise
   mdl.1 <- glm(Valence_num ~ Noise, data = df, family = binomial(link = "logit"))
   summary(mdl.1)
   ## getting model estimate into a interpretable probability
   ## y* = ln(p/1-p) = b_0 + b_1*Noise
   ## p = exp(b_0 + b_1*Noise) / exp(b_0 + b_1*Noise) +1
   ## p = exp(y*) / exp(y*)+1

                                           # plot first model
   ilink <- family(mdl.1)$linkinv
   pd <- with(df,
              data.frame(Noise = seq(min(Noise),
                                     max(Noise),
                                     length = 100)))
   pd <- cbind(pd, predict(mdl.1, pd, type = "link", se.fit = TRUE)[1:2])
   pd <- transform(pd, Fitted = ilink(fit), Upper = ilink(fit + (2*se.fit)),
                   Lower = ilink(fit - (2*se.fit)))
   ## plot probabilities with confidence interval (Wald)
   ggplot(df, aes(x = Noise, y = as.numeric(Valence_num))) +
     geom_ribbon(data = pd, aes(ymin = Lower, ymax = Upper, x = Noise),
                 fill = "gray", alpha = 0.2, inherit.aes = FALSE) +
     geom_line(data = pd, aes(y = Fitted, x = Noise)) +
     geom_point() +
     labs(y = "Probability of positive mood", x = "Noise")

                                           # test models
   ## measures of fit:
   ## likelihood ratio test, comparing our model to the null model
   lmtest::lrtest(mdl.null, mdl.1)
   ## mdl.1 coefficient are significant
   ## The reference for this is Joseph M. Hilbe (2009) Logistic Regression Methods, CRC Press, pages 81-82
   ## Hosmer and lemeshow goodness of fit
   mdl.simple.test <- hoslem.test(mdl.1$y, fitted(mdl.1), g = 10)
   mdl.complete.test <- hoslem.test(mdl.complete$y, fitted(mdl.complete), g = 10)

   ## ROC
   mdl.1.preds <- predict(mdl.1, type = "response")
   mdl.1.roc <- roc(df$Valence_num, mdl.1.preds,
                    smoothed = TRUE,
                    ci = TRUE, ci.alpha = 0.5, stratified = FALSE,
                    plot = TRUE, auc.polygon = TRUE, max.aux.polygon = TRUE, grid = TRUE,
                    print.auc = TRUE, show.thres=TRUE)
   ## mdl.1 has poor discrimination
   ## get list of all possible thresholds and corresponding specificity and sensitivity
   mdl.1.c <- coords(roc = mdl.1.roc, x = "all", transpose = FALSE)
   ## we could calculate the threshold that maximizes the sum of specificity and sensitivity
   mdl.1.optc <- coords(mdl.1.roc, "best", "threshold")
   ## get both confusion matrices
   ## c = 0.5
   mdl.1.norm <- factor(ifelse(mdl.1.preds > 0.5, "Positive", "Negative"))
   confusionMatrix(mdl.1.norm, df$Valence)
   ## c = optimal <- 0.4396930
   mdl.1.opt <- factor(ifelse(mdl.1.preds > mdl.1.optc["threshold"], "Positive", "Negative"))
   confusionMatrix(mdl.1.opt, df$Valence)


   ## repeated k-fold cross-validation
   cntrl <- trainControl(method = "repeatedcv",
                         number = 10,
                         repeats = 10)
   mdl.1.kfold <- train(as.factor(Valence_num) ~ Noise,
                        data = na.omit(df),
                        method = "glm",
                        family = binomial(link = "logit"),
                        trControl = cntrl)

                                           # step-wise models
   ## step-wise regression
   ## socioeconomic excluded 100% colinear with neighbourhood
   mdl.1.step <- train(as.factor(Valence_num) ~ Noise + Pedestrians +
                      Cars + Neighbourhood + Sex_num,
                        data = na.omit(df),
                        method = "glmStepAIC",
                        family = binomial(link = "logit"),
                      trControl = cntrl,
                      trace = FALSE)
   mdl.1.step$finalModel
   summary(mdl.1.step$finalModel)

   ## under Akaike information criterion (goodness of fit ~ simplicity)
   ## Valence ~ Noise + Pedestrians + Neighbourhood
   ## we could test only noise model against complete one
   mdl.complete <- glm(Valence_num ~ Noise + Pedestrians + Neighbourhood,
                       data = df,
                       family = binomial(link = "logit"))
   lmtest::lrtest(mdl.1, mdl.complete)
   ## model coefficients are significant against simpler model

   ## confusion matrix, roc, and k-fold validation for complete model
   ## ROC
   mdl.complete.preds <- predict(mdl.complete, type = "response")
   mdl.complete.roc <- roc(df$Valence_num, mdl.complete.preds,
                    smoothed = TRUE,
                    ci = TRUE, ci.alpha = 0.5, stratified = FALSE,
                    plot = TRUE, auc.polygon = TRUE, max.aux.polygon = TRUE, grid = TRUE,
                    print.auc = TRUE, show.thres=TRUE)
   ## confusion matrices
   ## we could calculate the threshold that maximizes the sum of specificity and sensitivity
   mdl.complete.optc <- coords(mdl.complete.roc, "best", "threshold")
   ## get both confusion matrices
   ## c = 0.5
   mdl.complete.norm <- factor(ifelse(mdl.complete.preds > 0.5, "Positive", "Negative"))
   confusionMatrix(mdl.complete.norm, df$Valence)
   ## c = optimal <- 0.4396930
   mdl.complete.opt <- factor(ifelse(mdl.complete.preds > mdl.complete.optc["threshold"], "Positive", "Negative"))
   confusionMatrix(mdl.complete.opt, df$Valence)
   ## k-fold
   mdl.complete.kfold <- train(as.factor(Valence_num) ~ Noise + Pedestrians + Neighbourhood,
                               data = na.omit(df),
                        method = "glm",
                        family = binomial(link = "logit"),
                        trControl = cntrl)
   ## accuracy of simple a complete model are almost the same


                                           # coefficients interpretation
   plot(allEffects(mdl.complete))

                                           # what's left
   ## checking for outlier influence

                                           # neural-net
   nn.control <- trainControl(method = "repeatedcv",
                         number = 10,
                         repeats = 10)
   nn.grid <- expand.grid(size = seq(from = 1, to = 15, by = 1),
                         decay = seq(from = 0.1, to = 0.5, by = 0.1))
   nn.kfold <- train(as.factor(Valence_num) ~ (.)^2,
                        data = select_if(df, is.numeric),
                        method = "nnet",
                     trControl = nn.control,
                     preProcess = c('center', 'scale'),
                     tuneGrid =  nn.grid)
 #+END_SRC

 #+RESULTS:
 #+begin_example

   Subjects Valence_num  Valence       Noise Pedestrians      Cars       Pupil
 1        1           0 Negative 0.010767789  0.00000000 0.0000000         2.3
 2        1           0 Negative 0.012513303  0.46391753 0.1546392          []
 3        1           1 Positive 0.010372872  0.41237113 0.0000000  2.52325703
 4        1           1 Positive 0.009794006  0.09278351 0.0000000 2.079852955
 5        1           1 Positive 0.013058803  0.48453608 0.0000000 2.505264791
 6        1           1 Positive 0.024941132  0.16494845 0.0000000 2.590279936
   Neighbourhood_num Neighbourhood Socioeconomic_num Socioeconomic Sex_num
 1                 1          Cumm                 1        Middle       0
 2                 1          Cumm                 1        Middle       0
 3                 1          Cumm                 1        Middle       0
 4                 1          Cumm                 1        Middle       0
 5                 1          Cumm                 1        Middle       0
 6                 1          Cumm                 1        Middle       0

                   Subjects Valence_num   Noise Pedestrians    Cars
 Subjects                                                          
 Valence_num       -0.19***                                        
 Noise              0.36***    -0.17***                            
 Pedestrians             0       -0.09  0.13**                     
 Cars               0.19***      -0.05    0.1*       -0.07         
 Neighbourhood_num  0.99***    -0.21*** 0.36***      -0.01  0.18***
 Socioeconomic_num  0.95***    -0.19*** 0.36***       0.01  0.13** 
 Sex_num              0.03        0.01  0.14**         0.1  0.14** 
                   Neighbourhood_num Socioeconomic_num
 Subjects                                             
 Valence_num                                          
 Noise                                                
 Pedestrians                                          
 Cars                                                 
 Neighbourhood_num                                    
 Socioeconomic_num           0.96***                  
 Sex_num                      -0.03             -0.06

 Call:
 glm(formula = Valence_num ~ Noise, family = binomial(link = "logit"), 
     data = df)

 Deviance Residuals: 
     Min       1Q   Median       3Q      Max  
 -1.2618  -1.0714  -0.8653   1.2181   2.0739  

 Coefficients:
             Estimate Std. Error z value Pr(>|z|)    
 (Intercept)   0.2316     0.1372   1.689   0.0912 .  
 Noise       -26.2153     5.5992  -4.682 2.84e-06 ***
 ---
 Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

 (Dispersion parameter for binomial family taken to be 1)

     Null deviance: 1025.4  on 753  degrees of freedom
 Residual deviance: 1000.9  on 752  degrees of freedom
 AIC: 1004.9

 Number of Fisher Scoring iterations: 4

 Confusion Matrix and Statistics

           Reference
 Prediction   0   1
          0 391 244
          1  47  72

                Accuracy : 0.6141         
                  95% CI : (0.5783, 0.649)
     No Information Rate : 0.5809         
     P-Value [Acc
 NIR] : 0.03489        

                   Kappa : 0.132          

  Mcnemar's Test P-Value : < 2e-16        

             Sensitivity : 0.22785        
             Specificity : 0.89269        
          Pos Pred Value : 0.60504        
          Neg Pred Value : 0.61575        
              Prevalence : 0.41910        
          Detection Rate : 0.09549        
    Detection Prevalence : 0.15782        
       Balanced Accuracy : 0.56027        

        'Positive' Class : 1

 Setting levels: control = 0, case = 1
 Setting direction: controls < cases

 Confusion Matrix and Statistics

           Reference
 Prediction  0  1
          0 75 42
          1 17 16

                Accuracy : 0.6067          
                  95% CI : (0.5237, 0.6853)
     No Information Rate : 0.6133          
     P-Value [Acc
 NIR] : 0.601529        

                   Kappa : 0.099           

  Mcnemar's Test P-Value : 0.001781        

             Sensitivity : 0.2759          
             Specificity : 0.8152          
          Pos Pred Value : 0.4848          
          Neg Pred Value : 0.6410          
              Prevalence : 0.3867          
          Detection Rate : 0.1067          
    Detection Prevalence : 0.2200          
       Balanced Accuracy : 0.5455          

        'Positive' Class : 1

 Warning message:
 NAs introduced by coercion

 Generalized Linear Model 

 490 samples
   4 predictor
   2 classes: '0', '1' 

 Pre-processing: centered (4), scaled (4) 
 Resampling: Cross-Validated (5 fold, repeated 10 times) 
 Summary of sample sizes: 392, 392, 393, 391, 392, 392, ... 
 Resampling results:

   Accuracy   Kappa    
   0.5879787  0.1654871

 Warning message:
 In coords.roc(roc.curve, "best") :
   An upcoming version of pROC will set the 'transpose' argument to FALSE by default. Set transpose = TRUE explicitly to keep the current behavior, or transpose = FALSE to adopt the new one and silence this warning. Type help(coords_transpose) for additional information.

               default   optimal
 Sensitivity 0.2758621 0.7586207
 Specificity 0.8152174 0.5000000

 Setting levels: control = 0, case = 1
 Setting direction: controls < cases

 Warning message:
 In coords.roc(roc.curve.1, "best") :
   An upcoming version of pROC will set the 'transpose' argument to FALSE by default. Set transpose = TRUE explicitly to keep the current behavior, or transpose = FALSE to adopt the new one and silence this warning. Type help(coords_transpose) for additional information.

               default   optimal
 Sensitivity 0.4655172 0.8103448
 Specificity 0.7065217 0.4673913
 #+end_example
*** Model check
Complete model with interactions
#+BEGIN_SRC R :results output :session peatones
                                          # complete model with interactions
  log.mdl.full <- train(as.factor(Valence_num) ~ (Noise + Pedestrians +
                                                  Cars + Neighbourhood + Sex_num)^2,
                        data = na.omit(df),
                        method = "glmStepAIC",
                        family = binomial(link = "logit"),
                        trControl = cntrl,
                        trace = FALSE)
  ## save model
  saveRDS(log.mdl.full, "full_log_mdl.rds")

  ## confusion matrix of complete model with interactions
  log.mdl.full.preds <- predict(log.mdl.full$finalModel, type = "response")
  log.mdl.preds <- factor(ifelse(log.mdl.full.preds > 0.5, "Positive", "Negative"))
  confusionMatrix(log.mdl.preds, df$Valence)
#+END_SRC

#+RESULTS:
#+begin_example
Error in train(as.factor(Valence_num) ~ (Noise
Pedestrians
Cars
 : 
  could not find function "train"
Error in saveRDS(log.mdl.full, "full_log_mdl.rds") : 
  object 'log.mdl.full' not found
Error in predict(log.mdl.full$finalModel, type = "response") : 
  object 'log.mdl.full' not found
Error in ifelse(log.mdl.full.preds
0.5, "Positive", "Negative") : 
  object 'log.mdl.full.preds' not found
Error in confusionMatrix(log.mdl.preds, df$Valence) : 
  could not find function "confusionMatrix"
#+end_example

Overall model evaluation
#+BEGIN_SRC R :results output :session peatones
  ## build all simple models
  log.mdl.1 <- glm(Valence_num ~ Noise, data = df, family = binomial(link = "logit"))
  log.mdl.2 <- glm(Valence_num ~ Pedestrians, data = df, family = binomial(link = "logit"))
  log.mdl.3 <- glm(Valence_num ~ Cars, data = df, family = binomial(link = "logit"))
  log.mdl.4 <- glm(Valence_num ~ Neighbourhood, data = df, family = binomial(link = "logit"))
  log.mdl.5 <- glm(Valence_num ~ Socioeconomic, data = df, family = binomial(link = "logit"))
  log.mdl.6 <- glm(Valence_num ~ Sex_num, data = df, family = binomial(link = "logit"))

  ## build all 2 predictor models
  log.mdl.7 <- glm(Valence_num ~ Noise + Pedestrians, data = df, family = binomial(link = "logit"))
  log.mdl.8 <- glm(Valence_num ~ Noise + Cars, data = df, family = binomial(link = "logit"))
  log.mdl.9 <- glm(Valence_num ~ Noise + Neighbourhood, data = df, family = binomial(link = "logit"))
  log.mdl.10 <- glm(Valence_num ~ Noise + Socioeconomic, data = df, family = binomial(link = "logit"))
  log.mdl.11 <- glm(Valence_num ~ Noise + Sex_num, data = df, family = binomial(link = "logit"))

  ## Build all 3 predictor models
  log.mdl.12 <- glm(Valence_num ~ Noise + Pedestrians + Cars, data = df, family = binomial(link = "logit"))
  log.mdl.13 <- glm(Valence_num ~ Noise + Pedestrians + Neighbourhood, data = df, family = binomial(link = "logit"))
  log.mdl.14 <- glm(Valence_num ~ Noise + Pedestrians + Socioeconomic, data = df, family = binomial(link = "logit"))
  log.mdl.15 <- glm(Valence_num ~ Noise + Pedestrians + Sex_num, data = df, family = binomial(link = "logit"))
  log.mdl.16 <- glm(Valence_num ~ Noise + Cars + Neighbourhood, data = df, family = binomial(link = "logit"))
  log.mdl.17 <- glm(Valence_num ~ Noise + Cars + Socioeconomic, data = df, family = binomial(link = "logit"))
  log.mdl.18 <- glm(Valence_num ~ Noise + Cars + Sex_num, data = df, family = binomial(link = "logit"))

  ## list all models
  mdl.list <- list(log.mdl.1, log.mdl.2, log.mdl.3, log.mdl.3, log.mdl.4, log.mdl.5,
                   log.mdl.6, log.mdl.7, log.mdl.8, log.mdl.9, log.mdl.10,
                   log.mdl.11, log.mdl.12, log.mdl.13, log.mdl.14, log.mdl.15, log.mdl.16,
                   log.mdl.17, log.mdl.18)

  ## likelihood ratio test
  lrt <- sapply(mdl.list, function(x) c(lrtest(mdl.null, x)))
  ## Wald test
  walds <- sapply(mdl.list, function(x) data.frame(summary.glm(x)$coefficients))


                                          # comparison with intercept only model
  ## likelihood ratio model 1
  likelihood.raio.test <- lrtest(mdl.null


#+END_SRC
*** Complete models
#+BEGIN_SRC R :results output :session peatones
  ## fit simple model
  glm.mdl.null <- glm(Valence_num ~ 1,
                     data = df,
                     family = binomial(link = "logit"))
  glm.mdl.1 <- glm(Valence_num ~ Noise,
                   data = df,
                   family = binomial(link = "logit"))
  glm.mdl.2 <- glm(Valence_num ~ Noise + Pedestrians,
                   data = df,
                   family = binomial(link = "logit"))
  glm.mdl.3 <- glm(Valence_num ~ Noise + Cars,
                   data = df,
                   family = binomial(link = "logit"))
  glm.mdl.4 <- glm(Valence_num ~ Noise + Neighbourhood,
                   data = df,
                   family = binomial(link = "logit"))
  glm.mdl.5 <- glm(Valence_num ~ Noise + Socioeconomic,
                   data = df,
                   family = binomial(link = "logit"))
  glm.mdl.6 <- glm(Valence_num ~ Noise + Pedestrians + Cars,
                   data = df,
                   family = binomial(link = "logit"))
  glm.mdl.7 <- glm(Valence_num ~ Noise + Pedestrians + Neighbourhood,
                   data = df,
                   family = binomial(link = "logit"))
  glm.mdl.8 <- glm(Valence_num ~ Noise + Pedestrians + Socioeconomic,
                   data = df,
                   family = binomial(link = "logit"))
  glm.list <- list(glm.mdl.1, glm.mdl.2, glm.mdl.3, glm.mdl.4, glm.mdl.5,
                   glm.mdl.6, glm.mdl.7, glm.mdl.8)

  ## walds-test
  glm.wald <- sapply(glm.list, function(x) Anova(x, test.statistic = c("Wald")),
                 simplify = FALSE)
  ## likelihood ratio test
  glm.lr <- sapply(glm.list, function(x) anova(glm.mdl.null, x, test = "LRT"),
                   simplify = FALSE)
  ## score test
  glm.score <- sapply(glm.list, function(x) anova(glm.mdl.null, x, test = "Rao"),
                      simplify = FALSE)
  ## hosmer & lemeshow, g = covariates + 1
  glm.hl <- sapply(glm.list, function(x) hoslem.test(x$y, fitted(x), g= 3))
  ## AIC for every model
  glm.aic <- sapply(glm.list, function(x) AIC(x))
  ## VIF for all models
  glm.vif <- sapply(glm.list[2:8], function(x) vif(x))

  ## repeated k-fold cross validated models
  cntrl <- trainControl(method = "repeatedcv",                      number = 10,
                        repeats = 10,
                        savePredictions = TRUE)
  mdl.1.kfold <- train(as.factor(Valence_num) ~ Noise,
                       data = na.omit(df),
                       method = "glm",
                       family = binomial(link = "logit"),
                       trControl = cntrl)
  mdl.2.kfold <- train(as.factor(Valence_num) ~ Noise + Pedestrians,
                       data = na.omit(df),
                       method = "glm",
                       family = binomial(link = "logit"),
                       trControl = cntrl)
  mdl.3.kfold <- train(as.factor(Valence_num) ~ Noise + Cars,
                       data = na.omit(df),
                       method = "glm",
                       family = binomial(link = "logit"),
                       trControl = cntrl)
  mdl.4.kfold <- train(as.factor(Valence_num) ~ Noise + Neighbourhood,
                       data = na.omit(df),
                       method = "glm",
                       family = binomial(link = "logit"),
                       trControl = cntrl)
  mdl.5.kfold <- train(as.factor(Valence_num) ~ Noise + Socioeconomic,
                       data = na.omit(df),
                       method = "glm",
                       family = binomial(link = "logit"),
                       trControl = cntrl)
  mdl.6.kfold <- train(as.factor(Valence_num) ~ Noise + Pedestrians + Cars,
                       data = na.omit(df),
                       method = "glm",
                       family = binomial(link = "logit"),
                       trControl = cntrl)
  mdl.7.kfold <- train(as.factor(Valence_num) ~ Noise + Pedestrians + Neighbourhood,
                       data = na.omit(df),
                       method = "glm",
                       family = binomial(link = "logit"),
                       trControl = cntrl)
  mdl.8.kfold <- train(as.factor(Valence_num) ~ Noise + Pedestrians + Socioeconomic,
                       data = na.omit(df),
                       method = "glm",
                       family = binomial(link = "logit"),
                       trControl = cntrl)


  mdl.1.pred <- predict(mdl.1.kfold)
  mdl.2.pred <- predict(mdl.2.kfold)
  mdl.3.pred <- predict(mdl.3.kfold)
  mdl.4.pred <- predict(mdl.4.kfold)
  mdl.5.pred <- predict(mdl.4.kfold)
  mdl.6.pred <- predict(mdl.4.kfold)
  mdl.7.pred <- predict(mdl.4.kfold)
  mdl.8.pred <- predict(mdl.4.kfold)

  mdl.1.cm <- caret::confusionMatrix(mdl.1.pred, as.factor(df$Valence_num))
  mdl.2.cm <- caret::confusionMatrix(mdl.2.pred, as.factor(df$Valence_num))
  mdl.3.cm <- caret::confusionMatrix(mdl.3.pred, as.factor(df$Valence_num))
  mdl.4.cm <- caret::confusionMatrix(mdl.4.pred, as.factor(df$Valence_num))
  mdl.5.cm <- caret::confusionMatrix(mdl.1.pred, as.factor(df$Valence_num))
  mdl.6.cm <- caret::confusionMatrix(mdl.2.pred, as.factor(df$Valence_num))
  mdl.7.cm <- caret::confusionMatrix(mdl.3.pred, as.factor(df$Valence_num))
  mdl.8.cm <- caret::confusionMatrix(mdl.4.pred, as.factor(df$Valence_num))


  par(mfrow = c(4,2))
  fourfoldplot(mdl.1.cm$table, main = "Noise")
  fourfoldplot(mdl.2.cm$table, main = "Noise + Pedestrians")
  fourfoldplot(mdl.3.cm$table, main = "Noise + Cars")
  fourfoldplot(mdl.4.cm$table, main = "Noise + Neighborhood")
  fourfoldplot(mdl.4.cm$table, main = "Noise + Socioeconomic")
  fourfoldplot(mdl.4.cm$table, main = "Noise + Pedestrians + Cars")
  fourfoldplot(mdl.4.cm$table, main = "Noise + Pedestrians + Neighborhood")
  fourfoldplot(mdl.4.cm$table, main = "Noise + Pedestrians + Socioeconomic")

  library(MASS)
  odd.ratio <- sapply(glm.list, function(x) exp(cbind(coef(x), confint(x))))

  ## plot linear odd-ratio
##  sapply(glm.list, function(x) plot(allEffects(x)))
#+END_SRC

#+RESULTS:
: Waiting for profiling to be done...
: Waiting for profiling to be done...
: Waiting for profiling to be done...
: Waiting for profiling to be done...
: Waiting for profiling to be done...
: Waiting for profiling to be done...
: Waiting for profiling to be done...
: Waiting for profiling to be done...

*** Simple models
#+BEGIN_SRC R :results both :session 
                                            # load packages
      pkg <- c("dplyr", "ggplot2", "tidyverse", "corrplot", "Hmisc", "psycho",
              "broom", "gvlma", "pROC", "caret", "MASS", "effects", "car",
              "ResourceSelection", "neuralnet", "lmtest", "pROC", "latexpdf",
              "export", "InformationValue", "MuMIn")
      invisible(lapply(pkg, library, character.only = TRUE))
                                              # load database
      setwd("/home/nicoluarte/Downloads")
      df <- data.frame(read.csv("data_fondecyt.csv"))

      ##########################
      ## k-fold cv parameters ##
      ##########################
      control <- trainControl(method = "repeatedcv",
                              number = 10,
                              repeats = 2,
                              savePredictions = TRUE)

      #################
      ## Valence ~ 1 ##
      #################
      mdl.null <- glm(Valence ~ 1,
                      family = binomial(link = "logit"),
                      data = df)

      #####################
      ## Valence ~ Noise ##
      #####################
      glm.1 <- glm(Valence ~ Noise,
                   data = df,
                   family = binomial(link = "logit"))
      mdl.1 <- train(Valence ~ Noise,
                     data = df,
                     method = "glm",
                     family = binomial(link = "logit"),
                     trControl = control)
                                              # Cross validation results
      ## Accuracy: 0.6130058
      ## Kappa: 0.1291081
      ## Confusion matrix
      mdl.1.cm <- caret::confusionMatrix(mdl.1,
                                         norm = "overall",
                                         dnn = c("Prediction", "Reference"))
      print(mdl.1.cm) # this is a percentage
      ## Confusion matrix with "optimized" cutoff
      probsTrain <- predict(mdl.1, type = "prob")
      rocCurve   <- roc(response = df$Valence,
                        predictor = probsTrain[,"Positive"])
      plot(rocCurve, print.thres = "best")
      ## highest combined (sensitivity and specificity)
      mdl.1.cutoff <- rocCurve$thresholds[which(
                                 rocCurve$sensitivities +
                                 rocCurve$specificities ==
                                 max(rocCurve$sensitivities +
                                     rocCurve$specificities))]
      mdl.1.pred <- factor(ifelse(probsTrain[, "Positive"] > mdl.1.cutoff,
                                  "Positive", "Negative") )
      mdl.1.cm.opt <- caret::confusionMatrix(mdl.1.pred, df$Valence)
      ## with this cutoff we obtain a way better kappa, accuracy might be
      ## misleading because of class unbalance

                                              # Model evaluation
      ## Wald-test
      mdl.1.wald <- Anova(mdl.1$finalModel, test.statistic = c("Wald"))
      print(mdl.1.wald)
      ## Score-test
      mdl.1.score <- anova(mdl.1$finalModel, test = "Rao")
      print(mdl.1.score)
      ## likelihood ratio test
      mdl.1.lrt <- lrtest(mdl.null, mdl.1$finalModel)
      print(mdl.1.lrt)
                                              # Fit
      ## Hosmer & Lemeshow g = covariate + 1
      mdl.1.hoslem <- hoslem.test(mdl.1$finalModel$y, fitted(mdl.1), g = 3)
      print(mdl.1.hoslem)

                                              # Odd ratios
      print(exp(mdl.1$finalModel$coefficients))

                                              # Plots
      mdl.1.plt <- plot(allEffects(glm.1))
      mdl.1.cm.plt <- fourfoldplot(round(mdl.1.cm$table, 2), conf.level = 0, main = "Noise")

      #########################################
      ## Valence ~ Noise + Cars + Pedestrians ##
      #########################################
      glm.2 <- glm(Valence ~ Noise + Cars + Pedestrians,
                   data = df,
                   family = binomial(link = "logit"))
      mdl.2 <- train(Valence ~ Noise + Cars + Pedestrians,
                     data = df,
                     method = "glm",
                     family = binomial(link = "logit"),
                     trControl = control)
                                              # Cross validation results
      ## Confusion matrix
      mdl.2.cm <- caret::confusionMatrix(mdl.2,
                                         norm = "overall",
                                         dnn = c("Prediction", "Reference"))
      print(mdl.2.cm) # this is a percentage
      ## Confusion matrix with "optimized" cutoff
      probsTrain.2 <- predict(mdl.2, type = "prob")
      rocCurve.2   <- roc(response = df$Valence,
                        predictor = probsTrain.2[,"Positive"])
      plot(rocCurve.2, print.thres = "best")

      ## highest combined (sensitivity and specificity)
      mdl.2.cutoff <- rocCurve.2$thresholds[which(
                                 rocCurve.2$sensitivities +
                                 rocCurve.2$specificities ==
                                 max(rocCurve.2$sensitivities +
                                     rocCurve.2$specificities))]
      mdl.2.pred <- factor(ifelse(probsTrain.2[, "Positive"] > mdl.2.cutoff,
                                  "Positive", "Negative") )
      mdl.2.cm.opt <- caret::confusionMatrix(mdl.2.pred, df$Valence)

                                              # Model evaluation
      ## Wald-test
      mdl.2.wald <- Anova(mdl.2$finalModel, test.statistic = c("Wald"))
      print(mdl.2.wald)
      ## Score-test
      mdl.2.score <- anova(mdl.2$finalModel, test = "Rao")
      print(mdl.2.score)
      ## likelihood ratio test
      mdl.2.lrt <- lrtest(mdl.null, mdl.2$finalModel)
      print(mdl.2.lrt)
                                              # Fit
      ## Hosmer & Lemeshow g = covariate + 1
      mdl.2.hoslem <- hoslem.test(mdl.2$finalModel$y, fitted(mdl.2), g = 4)
      print(mdl.2.hoslem)

                                              # Odd ratios
      print(exp(mdl.2$finalModel$coefficients))

                                              # Plots
      mdl.2.plt <- plot(allEffects(glm.2))
      mdl.2.cm.plt <- fourfoldplot(round(mdl.2.cm$table, 2), conf.level = 0, main = "Noise + Cars + Pedestrians")

      #####################################
      ## Valence ~ Noise + Neighbourhood ##
      #####################################

      glm.3 <- glm(Valence ~ Noise + Neighbourhood,
                   data = df,
                   family = binomial(link = "logit"))
      mdl.3 <- train(Valence ~ Noise + Neighbourhood,
                     data = df,
                     method = "glm",
                     family = binomial(link = "logit"),
                     trControl = control)
                                              # Cross validation results
      ## Confusion matrix
      mdl.3.cm <- caret::confusionMatrix(mdl.3,
                                         norm = "overall",
                                         dnn = c("Prediction", "Reference"))
      print(mdl.3.cm) # this is a percentage
      ## Confusion matrix with "optimized" cutoff
      probsTrain.3 <- predict(mdl.3, type = "prob")
      rocCurve.3   <- roc(response = df$Valence,
                        predictor = probsTrain.3[,"Positive"])
      plot(rocCurve.3, print.thres = "best")

      ## highest combined (sensitivity and specificity)
      mdl.3.cutoff <- rocCurve.3$thresholds[which(
                                 rocCurve.3$sensitivities +
                                 rocCurve.3$specificities ==
                                 max(rocCurve.3$sensitivities +
                                     rocCurve.3$specificities))]
      mdl.3.pred <- factor(ifelse(probsTrain.3[, "Positive"] > mdl.3.cutoff,
                                  "Positive", "Negative") )
      mdl.3.cm.opt <- caret::confusionMatrix(mdl.3.pred, df$Valence)

                                              # Model evaluation
      ## Wald-test
      mdl.3.wald <- Anova(mdl.3$finalModel, test.statistic = c("Wald"))
      print(mdl.3.wald)
      ## Score-test
      mdl.3.score <- anova(mdl.3$finalModel, test = "Rao")
      print(mdl.3.score)
      ## likelihood ratio test
      mdl.3.lrt <- lrtest(mdl.null, mdl.3$finalModel)
      print(mdl.3.lrt)
                                              # Fit
      ## Hosmer & Lemeshow g = covariate + 1
      mdl.3.hoslem <- hoslem.test(mdl.3$finalModel$y, fitted(mdl.3), g = 4)
      print(mdl.3.hoslem)

                                              # Odd ratios
      print(exp(mdl.3$finalModel$coefficients))

                                              # Plots
      mdl.3.plt <- plot(allEffects(glm.3))
      mdl.3.cm.plt <- fourfoldplot(round(mdl.3.cm$table, 2), conf.level = 0, main = "Noise + Neighbourhood")


      #####################################
      ## Valence ~ Noise + Socioeconomic ##
      #####################################

      glm.4 <- glm(Valence ~ Noise + Socioeconomic,
                   data = df,
                   family = binomial(link = "logit"))
      mdl.4 <- train(Valence ~ Noise + Socioeconomic,
                     data = df,
                     method = "glm",
                     family = binomial(link = "logit"),
                     trControl = control)
                                              # Cross validation results
      ## Confusion matrix
      mdl.4.cm <- caret::confusionMatrix(mdl.4,
                                         norm = "overall",
                                         dnn = c("Prediction", "Reference"))
      print(mdl.4.cm) # this is a percentage
      ## Confusion matrix with "optimized" cutoff
      probsTrain.4 <- predict(mdl.4, type = "prob")
      rocCurve.4   <- roc(response = df$Valence,
                        predictor = probsTrain.4[,"Positive"])
      plot(rocCurve.4, print.thres = "best")

      ## highest combined (sensitivity and specificity)
      mdl.4.cutoff <- rocCurve.4$thresholds[which(
                                 rocCurve.4$sensitivities +
                                 rocCurve.4$specificities ==
                                 max(rocCurve.4$sensitivities +
                                     rocCurve.4$specificities))]
      mdl.4.pred <- factor(ifelse(probsTrain.4[, "Positive"] > mdl.4.cutoff,
                                  "Positive", "Negative") )
      mdl.4.cm.opt <- caret::confusionMatrix(mdl.4.pred, df$Valence)

                                              # Model evaluation
      ## Wald-test
      mdl.4.wald <- Anova(mdl.4$finalModel, test.statistic = c("Wald"))
      print(mdl.4.wald)
      ## Score-test
      mdl.4.score <- anova(mdl.4$finalModel, test = "Rao")
      print(mdl.4.score)
      ## likelihood ratio test
      mdl.4.lrt <- lrtest(mdl.null, mdl.4$finalModel)
      print(mdl.4.lrt)
                                              # Fit
      ## Hosmer & Lemeshow g = covariate + 1
      mdl.4.hoslem <- hoslem.test(mdl.4$finalModel$y, fitted(mdl.4), g = 4)
      print(mdl.4.hoslem)

                                              # Odd ratios
      print(exp(mdl.4$finalModel$coefficients))
                                              # Plots
      png(file = "/home/nicoluarte/Downloads/effects_1.png")
      plot(allEffects(glm.1))
      dev.off()
      png(file = "/home/nicoluarte/Downloads/effects_2.png")
      plot(allEffects(glm.2))
      dev.off()
      png(file = "/home/nicoluarte/Downloads/effects_3.png")
      plot(allEffects(glm.3))
      dev.off()
      png(file = "/home/nicoluarte/Downloads/effects_4.png")
      plot(allEffects(glm.4))
      dev.off()
      mdl.4.cm.plt <- fourfoldplot(round(mdl.4.cm$table, 2), conf.level = 0, main = "Noise + Socioeconomic")

      ###################
      ## Merge results ##
      ###################

      acc <- data.frame(mdl.1.cm.opt$overall, mdl.2.cm.opt$overall, mdl.3.cm.opt$overall, mdl.4.cm.opt$overall)
      acc <- tibble::rownames_to_column(acc, "Statistic")
      acc[1:7, 2:5] <- round(acc[1:7, 2:5], 3)
      as.pdf(acc)

      png(file = "/home/nicoluarte/Downloads/confusion_matrices.png")
      par(mfrow = c(2,2))
      fourfoldplot(round(mdl.1.cm$table, 2), conf.level = 0, main = "Noise")
      fourfoldplot(round(mdl.2.cm$table, 2), conf.level = 0, main = "Noise + Cars + Pedestrians")
      fourfoldplot(round(mdl.3.cm$table, 2), conf.level = 0, main = "Noise + Neighbourhood")
      fourfoldplot(round(mdl.4.cm$table, 2), conf.level = 0, main = "Noise + Socioeconomic")
      dev.off()


      a <- boxplot.stats(df$Noise)
      b <- boxplot.stats(df$Cars)
      c <- boxplot.stats(df$Pedestrians)
      d <- which(a$out %in% df$Noise)
      e <- which(b$out %in% df$Cars)
      f <- which(c$out %in% df$Pedestrians)

      test.df <- df[-c(d, e, f), ]
      test.df[, c("Noise", "Pedestrians", "Cars")] <- scale(test.df[, c("Noise", "Pedestrians", "Cars")])
      glm.n <- glm(Valence ~ Noise + Cars + Pedestrians,
                   family = binomial(link = "logit"),
                   data = test.df)
      mdl.n <- train(Valence ~ Noise + Cars + Pedestrians,
                     data = test.df,
                     method = "glm",
                     family = binomial(link = "logit"),
                     trControl = control)
      mdl.n.1 <- train(Valence ~ Noise + Cars:Pedestrians,
                     data = test.df,
                     method = "glm",
                     family = binomial(link = "logit"),
                     trControl = control)
      probsTrain.n <- predict(mdl.n, type = "prob")
      rocCurve.n   <- roc(response = test.df$Valence,
                          predictor = probsTrain.n[,"Positive"])
      mdl.n.cutoff <- rocCurve.n$thresholds[which(
                                   rocCurve.n$sensitivities +
                                   rocCurve.n$specificities ==
                                   max(rocCurve.n$sensitivities +
                                       rocCurve.n$specificities))]
      mdl.n.pred <- factor(ifelse(probsTrain.n[, "Positive"] > mdl.n.cutoff,
                                  "Positive", "Negative") )
      mdl.n.cm.opt <- caret::confusionMatrix(mdl.n.pred, test.df$Valence)

      all.mdl.1 <- list(mdl.1.lrt, mdl.1.score, mdl.1.wald,
                        mdl.2.lrt, mdl.2.score, mdl.2.wald,
                        mdl.3.lrt, mdl.3.score, mdl.3.wald,
                        mdl.4.lrt, mdl.4.score, mdl.4.wald )
      mdl.name <- c("mdl_1_lrt", "mdl_1_score", "mdl_1_wald",
                    "mdl_2_lrt", "mdl_2_score", "mdl_2_wald",
                    "mdl_3_lrt", "mdl_3_score", "mdl_3_wald",
                    "mdl_4_lrt", "mdl_4_score", "mdl_4_wald")
      mdl.tbl <- function(mdl, filen)
      {
        path <- paste("/home/nicoluarte/Downloads/", filen, sep = "")
        mdl.t <- mdl
        table2doc(x = mdl.t, file = path)
      }
      mapply(table2doc, all.mdl.1, mdl.name)

      ggroc(list("Noise" = rocCurve,
                 "Noise + Cars + Pedestrians" = rocCurve.2,
                 "Noise + Neighbourhood" = rocCurve.3,
                 "Noise + Socioeconomic" = rocCurve.4),
            legacy.axes = TRUE) +
        geom_segment(aes(x = 0,
                         xend = 1,
                         y = 0,
                         yend = 1),
                     color = "darkgrey",
                     linetype = "dashed") +
        ggtitle("Models ROC curve")
      ggsave("roc.png")

      ## concordance
      Concordance(df$Valence_num, glm.1$fitted.values)
      Concordance(df$Valence_num, glm.2$fitted.values)
      Concordance(df$Valence_num, glm.3$fitted.values)
      Concordance(df$Valence_num, glm.4$fitted.values)

      ## opt thresholds
      coords(rocCurve, "best", transpose = TRUE)
      coords(rocCurve.2, "best", transpose = TRUE)
      coords(rocCurve.3, "best", transpose = TRUE)
      coords(rocCurve.4, "best", transpose = TRUE)

      ## miss class
      1-sum(diag(mdl.1.cm.opt$table))/sum(mdl.1.cm.opt$table)
      1-sum(diag(mdl.2.cm.opt$table))/sum(mdl.2.cm.opt$table)
      1-sum(diag(mdl.3.cm.opt$table))/sum(mdl.3.cm.opt$table)
      1-sum(diag(mdl.4.cm.opt$table))/sum(mdl.4.cm.opt$table)



      glm.x <- glm(Valence ~ Noise + Socioeconomic,
                   data = df,
                   family = binomial(link = "logit"))

  ###############################
  ## Step-wise model selection ##
  ###############################

  glm.all <- glm(Valence ~ (Noise + Cars + Pedestrians + Neighbourhood)^2,
                 data = test.df,
                 family = binomial(link = "logit"),
                 na.action = na.pass)

  models <- stepAIC(glm.all, direction = c("both"))

  mdl.opt <- train(Valence ~ Noise + Cars + Pedestrians + Neighbourhood + Noise:Pedestrians +
                  Noise:Neighbourhood + Cars:Pedestrians + Cars:Neighbourhood,
                 data = test.df,
                 method = "glm",
                 family = binomial(link = "logit"),
                 trControl = control)
  caret::confusionMatrix(mdl.opt)

  mdl.tree <- train(Valence ~ Noise + Cars + Pedestrians + Neighbourhood + Noise:Pedestrians +
                      Noise:Neighbourhood + Cars:Pedestrians + Cars:Neighbourhood,
                   data = test.df,
                   method = "gbm",
                   trControl = control)
  caret::confusionMatrix(mdl.tree)

  library(fastDummies)
  library(FactoMineR)
  df.test <- test.df
  df.test$Neighbourhood <- as.numeric(as.factor(test.df$Neighbourhood))
  df.test$Socioeconomic <- as.numeric(as.factor(test.df$Socioeconomic))
  df.test <- subset(df.test, select = -c(Subjects, Valence_num, Neighbourhood_num,
                                         Socioeconomic_num, Pupil, Sex_num))
  pca.df <- prcomp(df.test[, c(2:6)], center = TRUE, scale. = TRUE)
  df.test$PCA1 <- pca.df$x[, 1]
  df.test$PCA2 <- pca.df$x[, 2]
  df.test$PCA3 <- pca.df$x[, 3]

  kmm <- kmeans(df.test[, c(2:6)], centers = 2)

  knn.mdl.pca <- train(Valence ~ (PCA1 + PCA2 + PCA3)^2,
                       data = df.test,
                       method = "knn",
                       trControl = control,
                       tuneGrid = expand.grid(k = 1:100))

  knn.mdl.pca <- train(Valence ~ (PCA1 + PCA2 + PCA3)^2,
                       data = df.test,
                       method = "glm",
                       trControl = control,
                       family = binomial(link = "logit"))

  knn.mdl.val <- train(Valence ~ Noise + Cars + Pedestrians + Neighbourhood + Noise:Pedestrians + Noise:Neighbourhood + Cars:Pedestrians + Cars:Neighbourhood,
                       data = df,
                       method = "knn",
                       trControl = control,
                       tuneGrid = expand.grid(k = 1:30))
#+END_SRC

#+RESULTS:
#+begin_example

Attaching package: ‘dplyr’

The following objects are masked from ‘package:stats’:

    filter, lag

The following objects are masked from ‘package:base’:

    intersect, setdiff, setequal, union

── [1mAttaching packages[22m ─────────────────────────────────────── tidyverse 1.2.1 ──
[32m✔[39m [34mtibble [39m 2.1.3     [32m✔[39m [34mpurrr  [39m 0.3.2
[32m✔[39m [34mtidyr  [39m 0.8.3     [32m✔[39m [34mstringr[39m 1.4.0
[32m✔[39m [34mreadr  [39m 1.3.1     [32m✔[39m [34mforcats[39m 0.4.0
── [1mConflicts[22m ────────────────────────────────────────── tidyverse_conflicts() ──
[31m✖[39m [34mdplyr[39m::[32mfilter()[39m masks [34mstats[39m::filter()
[31m✖[39m [34mdplyr[39m::[32mlag()[39m    masks [34mstats[39m::lag()
corrplot 0.84 loaded
Loading required package: lattice
Loading required package: survival
Loading required package: Formula

Attaching package: ‘Hmisc’

The following objects are masked from ‘package:dplyr’:

    src, summarize

The following objects are masked from ‘package:base’:

    format.pval, units

Registered S3 method overwritten by 'MuMIn':
  method         from
  predict.merMod lme4
Registered S3 methods overwritten by 'huge':
  method    from   
  plot.sim  BDgraph
  print.sim BDgraph
Registered S3 method overwritten by 'xts':
  method     from
  as.zoo.xts zoo 
message: psycho's `analyze()` is deprecated in favour of the report package. Check it out at https://github.com/easystats/report
Registered S3 methods overwritten by 'pROC':
  method    from
  print.roc huge
  plot.roc  huge
Type 'citation("pROC")' for a citation.

Attaching package: ‘pROC’

The following objects are masked from ‘package:stats’:

    cov, smooth, var

Registered S3 methods overwritten by 'lava':
  method    from
  plot.sim  huge
  print.sim huge

Attaching package: ‘caret’

The following object is masked from ‘package:survival’:

    cluster

The following object is masked from ‘package:purrr’:

    lift


Attaching package: ‘MASS’

The following object is masked from ‘package:dplyr’:

    select

Loading required package: carData
Use the command
    lattice::trellis.par.set(effectsTheme())
  to customize lattice options for effects plots.
See ?effectsTheme for details.
Registered S3 methods overwritten by 'car':
  method                          from
  influence.merMod                lme4
  cooks.distance.influence.merMod lme4
  dfbeta.influence.merMod         lme4
  dfbetas.influence.merMod        lme4

Attaching package: ‘car’

The following object is masked from ‘package:purrr’:

    some

The following object is masked from ‘package:dplyr’:

    recode

ResourceSelection 0.3-5 	 2019-07-22

Attaching package: ‘neuralnet’

The following object is masked from ‘package:dplyr’:

    compute

Loading required package: zoo

Attaching package: ‘zoo’

The following objects are masked from ‘package:base’:

    as.Date, as.Date.numeric

  Subjects Valence_num  Valence       Noise Pedestrians      Cars       Pupil
1        1           0 Negative 0.010767789  0.00000000 0.0000000         2.3
2        1           0 Negative 0.012513303  0.46391753 0.1546392          []
3        1           1 Positive 0.010372872  0.41237113 0.0000000  2.52325703
4        1           1 Positive 0.009794006  0.09278351 0.0000000 2.079852955
5        1           1 Positive 0.013058803  0.48453608 0.0000000 2.505264791
6        1           1 Positive 0.024941132  0.16494845 0.0000000 2.590279936
  Neighbourhood_num Neighbourhood Socioeconomic_num Socioeconomic Sex_num
1                 1          Cumm                 1        Middle       0
2                 1          Cumm                 1        Middle       0
3                 1          Cumm                 1        Middle       0
4                 1          Cumm                 1        Middle       0
5                 1          Cumm                 1        Middle       0
6                 1          Cumm                 1        Middle       0

Warning message:
In model.frame.default(Terms, newdata, na.action = na.action, xlev = object$lvls) :
  variable 'Valence' is not a factor

  Subjects Valence_num       Noise Pedestrians      Cars Pupil
1        1           0 0.010767789  0.00000000 0.0000000   138
2        1           0 0.012513303  0.46391753 0.1546392     1
3        1           1 0.010372872  0.41237113 0.0000000   254
4        1           1 0.009794006  0.09278351 0.0000000    45
5        1           1 0.013058803  0.48453608 0.0000000   243
6        1           1 0.024941132  0.16494845 0.0000000   288
  Neighbourhood_num Neighbourhood.Bilbao Neighbourhood.Colon Neighbourhood.Cumm
1                 1                    0                   0                  1
2                 1                    0                   0                  1
3                 1                    0                   0                  1
4                 1                    0                   0                  1
5                 1                    0                   0                  1
6                 1                    0                   0                  1
  Neighbourhood.Ecuador Neighbourhood.Hurtado Neighbourhood.StaAna
1                     0                     0                    0
2                     0                     0                    0
3                     0                     0                    0
4                     0                     0                    0
5                     0                     0                    0
6                     0                     0                    0
  Socioeconomic_num Socioeconomic.High Socioeconomic.Low Socioeconomic.Middle
1                 1                  0                 0                    1
2                 1                  0                 0                    1
3                 1                  0                 0                    1
4                 1                  0                 0                    1
5                 1                  0                 0                    1
6                 1                  0                 0                    1
  Sex_num
1       0
2       0
3       0
4       0
5       0
6       0

Cross-Validated (10 fold, repeated 10 times) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction Negative Positive
  Negative     51.8     32.5
  Positive      6.3      9.4
                            
 Accuracy (average) : 0.6121

Setting levels: control = Negative, case = Positive
Setting direction: controls < cases

Analysis of Deviance Table (Type II tests)

Response: .outcome
      Df  Chisq Pr(>Chisq)    
Noise  1 21.921  2.841e-06 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Analysis of Deviance Table

Model: binomial, link: logit

Response: .outcome

Terms added sequentially (first to last)


      Df Deviance Resid. Df Resid. Dev    Rao  Pr(>Chi)    
NULL                    753     1025.4                     
Noise  1   24.577       752     1000.9 23.071 1.562e-06 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Likelihood ratio test

Model 1: Valence ~ 1
Model 2: .outcome ~ Noise
  #Df  LogLik Df  Chisq Pr(>Chisq)    
1   1 -512.72                         
2   2 -500.43  1 24.577   7.14e-07 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

	Hosmer and Lemeshow goodness of fit (GOF) test

data:  mdl.1$finalModel$y, fitted(mdl.1)
X-squared = 7.1985, df = 1, p-value = 0.007296
#+end_example

*** Permutations model
#+BEGIN_SRC R :results output :session perm_mdl
                                          # load packages
  pkg <- c("dplyr", "ggplot2", "tidyverse", "corrplot", "Hmisc", "psycho",
           "broom", "gvlma", "pROC", "caret", "MASS", "effects", "car",
           "ResourceSelection", "neuralnet", "lmtest")
  invisible(lapply(pkg, library, character.only = TRUE))

                                          # load database
  setwd("/home/nicoluarte/Downloads")
  df <- data.frame(read.csv("data_fondecyt.csv"))
  head(df)

  ## metricas con el valence ~ socioeconimic + socioeconimic:noise
  ## permutacion valence ~ Socioeconomic
  perm_test <- function(df, label, nperm)
  {
    c_vec <- c()
    nrows <- dim(df)[1]
    y <- df[[label]]
    x <- df[, setdiff(colnames(df), y), drop = FALSE]
    for (i in seq_len(nperm))
    {
      idx <- sample.int(nrows, size = nrows, replace = TRUE)
      mdl <- glm(y ~ Socioeconomic + 0,
                 data = cbind(y = y[idx], x),
                 family = binomial(link = "logit"))
      c_vec <- rbind(c_vec, mdl$coefficients)
    }
    return(c_vec)
  }

  logit2prob <- function(logit){
    odds <- exp(logit)
    prob <- odds / (1 + odds)
    return(prob)
  }


  t.mdl <- glm(Valence_num ~ Socioeconomic + 0,
               data = df,
               family = binomial(link = "logit"))
  t.mdl.2 <- glm(Valence_num ~ Socioeconomic,
               data = df,
               family = binomial(link = "logit"))
  summary(t.mdl)
  summary(t.mdl.2)

  t.ci <- logit2prob(confint.default(t.mdl))
  t.coef <- data.frame(variable = c("High",
                                    "Low",
                                    "Middle"),
                       value = logit2prob(t.mdl$coefficients))

  library(doParallel)
  library(reshape2)
  library(ggplot2)
  cl = makeCluster(8)
  registerDoParallel(cl)
  p <- perm_test(df, "Valence_num", 10000)
  p <- data.frame(p)
  p_melt <- melt(p)
  p_melt$value <- logit2prob(p_melt$value)
  levels(p_melt$variable) <- c("High", "Low", "Middle")

  ggplot(p_melt, aes(x=variable, y=value, col = variable)) +
    geom_jitter(alpha = 0.2) +
    geom_point(data = t.coef, col = "black") +
    geom_errorbar(data = t.coef,
                  aes(ymin = t.ci[,1],
                      ymax = t.ci[,2]),
                  col = c("red", "green", "blue"))

  library(Rmisc)

  ci_perm <- CI(p_melt$value, ci = 0.95)

  ## socioeconomic model
  mdl_se <- glm(Valence_num ~ Socioeconomic + Socioeconomic:Noise,
                data = df,
                family = binomial(link = "logit"))
  mdl_se2 <- glm(as.factor(Valence_num) ~ Socioeconomic + Socioeconomic:Noise + 0,
                data = df,
                family = binomial(link = "logit"))
  mdl_se3 <- glm(Valence_num ~ Socioeconomic:Noise + 0,
                 data = df,
                 family = binomial(link = "logit"))
  mdl_null <- glm(Valence_num ~ Noise,
                data = df,
                family = binomial(link = "logit"))
  summary(mdl_se3)
  plot(allEffects(mdl_se))
  plot(allEffects(mdl_se2))
  low <- subset(df, Socioeconomic == "Low")
  mid <- subset(df, Socioeconomic == "Middle")
  high <- subset(df, Socioeconomic == "High")
  scatter.smooth(df$Noise ~ df$Valence_num)
  scatter.smooth(xx$Noise ~ xx$Valence_num)

  par(mfrow = c(1,3))
  plot(low$Noise, low$Valence_num)
  plot(mid$Noise, mid$Valence_num)
  plot(high$Noise, high$Valence_num)

  library(broom)
  tidy(mdl_se)
  Anova(mdl_se, test.statistic = c("Wald"))
  Anova(mdl_se3, test.statistic = c("Wald"))
  anova(mdl_se, mdl_null, test = "Chisq")
  TukeyHSD(bb)

  wt <- c()
  xxx <- wald.test(b = coef(mdl_se2), Sigma = vcov(mdl_se2), Terms = 6)
  wt[1] <- wald.test(b = coef(mdl_se2), Sigma = vcov(mdl_se2), Terms = 1)$result$chi2[1]
  wt[2] <- wald.test(b = coef(mdl_se2), Sigma = vcov(mdl_se2), Terms = 2)$result$chi2[1]
  wt[3] <- wald.test(b = coef(mdl_se2), Sigma = vcov(mdl_se2), Terms = 3)$result$chi2[1]
  wt[4] <- wald.test(b = coef(mdl_se2), Sigma = vcov(mdl_se2), Terms = 4)$result$chi2[1]
  wt[5] <- wald.test(b = coef(mdl_se2), Sigma = vcov(mdl_se2), Terms = 5)$result$chi2[1]
  wt[6] <- wald.test(b = coef(mdl_se2), Sigma = vcov(mdl_se2), Terms = 6)$result$chi2[1]

  mdl_se2_pred <- predict(mdl_se2, type = "response")
  mdscore::wald.test(model = mdl_se, terms = 6)


  mdl_se2_norm <- factor(ifelse(mdl_se2_pred > 0.5, "Positive", "Negative"))
  cm_se2 <- confusionMatrix(mdl_se2_norm, df$Valence)
  tidy(cm_se2)
#+END_SRC

*** 
* Defensa de Tesis
** Gráficos
#+begin_src python :results output
  import numpy as np
  import scipy
  from scipy.stats import uniform
  from scipy.stats import levy
  from scipy.stats import powerlaw
  import matplotlib.pyplot as plt

  def levy_walk(n):
      # uniform distribution for turning angles
      angle = uniform.rvs(size=(n,), loc=0, scale=2.*np.pi)

      # levy dist step length
      r = powerlaw.rvs(0.05, size=n)

      # coordinates
      x = np.cumsum(r*np.cos(angle))
      y = np.cumsum(r*np.sin(angle))

      return np.array((x, y, r, angle))

  # steps to be simulated
  n = 500

  # levy flight
  foo = levy_walk(n)
  foo2 = levy_walk(200)

  fig = plt.figure(figsize=(14, 6))

  # plot the walk
  ax1 = fig.add_subplot(2,1,2)
  ax1.plot(foo[0,:], foo[1,:], 'g--')
  ax1.set_title('Simulated foraging pattern')
  ax1.set_xticks([])
  ax1.set_yticks([])

  # plot the histogram
  ax2 = fig.add_subplot(2,2,1)
  ax2.set_yscale('log')
  ax2.hist(foo[2,:], bins=int(n/10), color = 'green')
  ax2.set_title('Foraging step-size histogram')
  ax2.set_xticks([])
  ax2.set_yticks([])

  # IRI histogram
  ax3 = fig.add_subplot(2,2,2)
  ax3.set_yscale('log')
  ax3.hist(foo2[2,:], bins=int(n/10), color = 'gray')
  ax3.set_title('Inter-response interval histogram')
  ax3.set_xticks([])
  ax3.set_yticks([])

  plt.show()

  def truncated_power_law(a,m):
      x = np.arange(1, m+1, dtype='float')
      pmf = 1/x**a
      pmf /= pmf.sum()
      return scipy.stats.rv_discrete(values=(range(1, m+1), pmf))

  a, m = 2, 10
  d = truncated_power_law(a=a, m=m)

  N = 1000
  sample = d.rvs(size=N)
#+end_src
